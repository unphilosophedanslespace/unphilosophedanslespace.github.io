--- 
title: "À propos du dilemme épistémologique que pose le problème de la matière noire" 
date: 2024-10-25T16:37:38+02:00 
draft: true 
---

# Remerciements: {#remerciements .unnumbered} 

J'aimerais remercier pour leur soutien constant ma famille, mes directeurs de mémoire Catherine Allamel-Raffin et Pierre-Antoine Hervieux, et Jacqueline Jiménez Martinez. Je remercie également l'OBAS de m'avoir accueilli du 25 avril au 6 mai 2022 pour répondre à mes (nombreuses) questions.

# La matière noire est-elle un problème d'épistémologie ?

Il est une chanson française qui parle d'un futur dystopique, dans lequel un personnage se questionne sur des évènements devenus inaccessibles. Le personnage questionne la véracité de ce qu'on lui propose sur les traces de ces phénomènes anciens: Vous êtes sûrs que la photo n'est pas truquée ? Vous pouvez m'assurer que cela a vraiment existé ? Dites-moi, allons ne me racontez plus d'histoires, J'ai besoin de toucher et de voir pour y croire [^1]. La chanson évoque tout particulièrement une méthode contemporaine et populaire d'établir la vérité d'un évènement: la vue et le toucher, l'interférence et l'enregistrement. Mais il est certaines sciences où l'on doit se contenter de voir et d'enregistrer des vues, sans pouvoir toucher ni interférer avec l'expérience.

C'est le cas de l'astrophysique, une science jeune née au XIXème siècle, dont les récents développements montrent des progrès spectaculaires: il est possible aujourd'hui de déceler des structures cosmiques qui s'étendent sur toute l'échelle de l'univers. L'astrophysique en tant que discipline tire son origine de deux domaines différents: la physique expérimentale et l'astronomie. L'astronomie est la matière la plus ancienne: c'est une discipline cultivée durant l'antiquité grecque, et assurément même bien avant. Durant l'antiquité grecque l'observation du ciel s'associe à des théorisations géométriques, que l'on retrouve dans un sens mythologique chez Platon et dans un sens réaliste chez Aristote.  La tradition de géométriser l'observation astronomique est toujours conservée aujourd'hui et elle est complétée par une algébraïsation de la géométrie (les formes géométriques sont associées à des formules d'algèbre, dans lesquelles rentrent autant l'optique d'observation que les descriptions de mouvements d'astres). Durant le XIXe siècle, la physique se développe simultanément à la chimie et à la photographie, ce qui crée des moyens poussés d'enregistrement, de rétention, de création, d'observation d'atomes et de molécules très diversifiés. La spectroscopie fait définitivement entrer l'astronomie dans le cadre de la physique expérimentale en permettant d'analyser les raies lumineuses des astres avec une grille chimique d'éléments. L'astronomie devient une astrophysique lorsqu'elle permet de déterminer les composants atomiques et chimiques d'objets lointains. Toute une classification d'astres devient possible et les étoiles rentrent dans le paradigme de la physique atomique. Aujourd'hui il est fait usage du spectre électromagnétique entier[^2] afin d'observer le ciel, ce qui permet de classifier de très nombreux types de corps.

Grâce aux télescopes radio et à la découverte du fond diffus cosmologique, les premiers instants de notre monde sont connus depuis les années 1960. La cosmologie contemporaine est une science encore plus jeune que l'astrophysique. Si cette dernière a bien 150 ans, la cosmologie en tant que telle ne fête pas même son centenaire. Elle apparaît principalement grâce à la loi de Hubble proposée à la fin des années 1920 qui permet de déterminer l'éloignement des objets en fonction du rougissement ou du bleuissement de leur spectre lumineux.  Leur éloignement spatial est toujours un signe d'éloignement temporel car la vitesse de la lumière n'est pas infinie: les ondes les plus éloignées mettent donc toujours plus de temps à parvenir à l'observateur. Simultanément, les théories sur l'origine de l'univers, qui pour longtemps n'ont été que de la pure mythologie et une spéculation peu confirmable, acquièrent un caractère de réalité grâce à la possiblité de déterminer expérimentalement l'âge des astres. En entrant donc dans le système de vérité réaliste et instrumental de l'astrophysique, la cosmologie est devenue la science que l'on connaît aujourd'hui. Elle se distingue définitivement de l'astrophysique en tant que telle par l'envergure que chacune de ces deux sciences donne à son objet d'étude: l'astrophysique peut tout à fait se concentrer sur l'étude d'objets petits et précis, mais le but avoué de la cosmologie est d'embrasser l'univers dans son ensemble et d'englober tous ses objets particuliers pour y découvrir des lois régulières. Toute astrophysique n'est donc pas une cosmologie, mais toute cosmologie observable doit être une astrophysique. Aujourd'hui ces deux sciences partagent la pratique commune de faire un usage intense de moyens informatiques de modélisation à des fins prédictives. D'ailleurs, les astrophysiciens et cosmologues travaillent dans les mêmes bureaux.

Pourtant ce sont ces mêmes sciences arrivées à maturité qui clament ignorer quasiment tout de l'univers: une grande partie de la masse et de l'énergie existantes, approximativement $95\%$, manquent systématiquement de pouvoir être connues. On se sait ne pas les posséder, n'y avoir pas accès. Voilà une chose bien étrange qui a mis la puce à mon oreille de philosophe: il est su que l'on ignore quelque chose, même s'il n'est pas possible de l'apercevoir. Cette étrange contradiction, de se savoir *a posteriori* ne pas voir ce qui n'a pas été vu *a priori*, est ce qui m'a mené en premier à m'intéresser à la notion de matière noire. En effet la matière noire constitue environ $25\%$ de cet inconnu. J'abordais le problème comme une question d'ontologie: par quel moyen est-il possible de supputer l'existence d'un objet dont manifestement, il n'y aurait aucune preuve observable pour l'hypothétiser ? Pour l'astrophysique, un objet du ciel existe définitivement s'il peut être observé. Soulevant cette question d'ontologie, je me suis intéressé à l'hypothèse de la matière noire.

L'hypothèse de la matière noire est intéressante pour plusieurs raisons: premièrement, elle contient dans son propre nom le paradoxe qu'elle soulève et le problème épistémologique que l'on peut y déceler; deuxièmement c'est une hypothèse actuelle, un problème irrésolu et elle possède une histoire courte et dans une certaine mesure, accessible à un non-spécialiste, enfin elle est fameuse et connue du public, bien que j'ai rencontré plus d'un *quidam* qui la confondit avec les trous noirs.  Au cours de cet essai, je m'efforcerai de rendre à ma lectrice et à mon lecteur la notion aussi claire qu'elle l'est pour moi: c'est-à-dire que je lui en donnerai une connaissance de surface qui ne plonge pas jusque dans l'analyse des équations, mais qui permet tout de même d'en dire quelque chose de philosophiquement intéressant.

Il est possible de penser que comme la matière noire est un problème d'astrophysique, voire de cosmologie, le jeune philosophe et l'historien des sciences que je suis ne devrait rien avoir à en dire. Je me serais trompé de domaine pour en parler, et je devrais sérieusement réenvisager mon avenir si j'aimerais en dire quelque chose. Puisque l'histoire s'occupe de faits passés, il est évident qu'il n'est pas intéressant d'en parler du moment que le paradoxe n'est pas résolu. À quoi bon critiquer un problème auquel on n'a même pas laissé aux chercheurs une chance de trouver une solution et de plus le faire à partir de la voix d'un étudiant à peine sorti de son cocon d'études de sciences humaines ?  Je me serais trompé de sujet sur tous les points.

Tout d'abord, je répondrais que je ne suis pas le premier à m'intéresser aux paradoxes scientifiques en science humaines. Thomas S. Kuhn (1922 - 1996) a écrit un livre prolifique et fameux sur le sujet, *La structure des révolutions scientifiques*[^3], dont je reprendrai d'ailleurs l'essentiel pour ma propre analyse. Ensuite, la philosophie est une activité libre et radicale qui consiste à poser des questions et à en chercher les réponses. Elle n'a pas de sujet de prédilection et c'est pourquoi on peut la considérer comme une méthode de doute. C'est la fameuse maïeutique de Platon, c'est questionner dialectiquement et à force de réponses progressives et partielles, accoucher d'une idée. Rien n'empêche non plus la philosophie de choisir ses propres théories sur les objets de science pour les questionner, ce qui lui rajoute un degré de connaissance déterminable: je m'efforcerai donc de reconnaître d'où viennent mes points de vue et mes sources de réflexion, afin de convaincre celle ou celui qui me lit que mes opinions sur un sujet brûlant ne sont pas créées *ex nihilo* par pure gratuité et en toute ignorance du problème concerné. Ensuite, il est faux de penser que les chercheurs n'ont eu aucune chance de résoudre ce problème. Du fait que le problème n'est pas résolu, il serait également faux de penser que je veux, par ma critique, en quelque sorte arrêter dans son instant l'activité scientifique et lui donner l'apparence de l'ineptie, dire que ce n'est qu'une construction sociale ou quelque autre sorte d'évènement scientifique à caractère absurde. En effet, le problème de la matière noire n'est pas récent, il est né au début du XXe siècle, et sa version contemporaine est établie depuis au moins les années 1980, ce qui fait bien 40 ans. Quant à la critique que j'aimerais promulguer, elle consiste plutôt à proposer une analyse historique du problème, afin de déterminer quel est son contexte passé et présent et comment se comportent ses acteurs, couplée à une analyse philosophique qui s'intéresse à quels types de propos sont portés sur ce qui doit être vrai et ce qui doit être réel . De surcroît il est illusoire de penser qu'une simple critique puisse attaquer une théorie scientifique: les théories sont bien plus résistantes que cela, quand même elles subiraient une seule attaque sur leur fondement; enfin je n'ai aucune prétention d'apporter la moindre idée sur le contenu des hypothèses: je n'en ai pas les moyens. Hors de toute volonté et toute capacité, je ne veux pas et je ne peux pas apporter une critique sur le contenu de ces hypothèses scientifiques. Je ne fais pas partie de la communauté concernée. En revanche, je peux apporter des idées sur la manière dont ce contenu est donné. Pour éclaircir le problème que j'ai entraperçu, il faut parler de ce qui est recouvert par le mot de matière noire .

Deux niveaux préliminaires de l'analyse de cette notion peuvent être donnés. Le premier niveau d'analyse concerne son étymologie, le second niveau le sens historique qu'elle recouvre aujourd'hui. Par son sens historique, j'entends les faits, les façons d'agir et de penser qui lui sont liés et qu'il est possible de recueillir à son propos. Donc au sens étymologique, la matière noire met en avant sa propre contradiction: c'est une matière qui renvoie peu de lumière et pour l'astronome, elle est difficile à observer. Il y a à peine accès, mais sûrement, que dans tout le noir du ciel, son propre noir signifie une présence, celle d'une matière . L'analyse historique permet de compléter cette analyse de surface. En effet, historiquement, la matière noire n'a pas toujours disposé de ce nom: elle a aussi été appelée matière invisible ou encore matière sombre , dans tous les cas, c'est la difficulté de l'observer qui a été en mise en lumière, mais aussi le fait qu'elle *devrait* être observable. Cela veut dire qu'il y a un *signe* qui indique que cette matière devrait être présente, cependant, elle est difficile ou impossible à voir, au contraire de son signe. L'idée d'une matière fait référence à la physique contemporaine, notamment à l'idée d'un corpuscule qui pourrait s'inscrire dans les lois déjà connues ou prédictibles du paradigme employé. Elle devrait donc posséder un noyau et une masse atomique, et elle pourrait être de nature baryonique ou exotique. La matière baryonique correspond à la matière dite normale que l'on trouve en physique des particules: c'est celle qui possède des nucléons, eux-mêmes distinctibles en particules plus légères. C'est en somme l'ensemble des atomes connus et leurs compositions respectives. A la matière baryonique s'oppose en astrophysique la matière dite exotique . C'est une matière virtuelle et heuristique, que des cas d'anomalie forcent de supposer car ils ne sont pas solubles par l'interprétation de la physique standard. La matière noire est un tel cas d'anomalie. Les solutions exotiques à la matière noire en astrophysique sont communément appelées WIMP (*weakly interactive massive particles*. Ces solutions sont souvent créées *ad hoc*, c'est-à-dire dans la vue de réinterpréter ce qui est anormal comme une forme de normalité en conservant un maximum d'hypothèses habituelles du paradigme. Il faut aujourd'hui pour la matière noire que ces particules émettent très peu de radiations (car on ne les observe vraiment pas) en même temps qu'elles soient massives (car il y a une gravitation plus forte que les radiations observées ne le laissent prédire) et qu'elles aient une interaction uniquement gravitationnelle avec le reste des particules baryoniques. Les particules *ad hoc* sont des solutions construites sur mesure qui ne trouvent pas d'autre raison d'être que la justification salvatrice de la plupart du paradigme de recherche de l'astrophysique, qui été jusqu'à l'apparition de l'anomalie, fructueux d'un point de vue prédictif. C'est en quelque sorte un énième effort de prédiction donné afin de conserver la voie normale de la recherche. Ces hypothèses ont ceci de perturbant, d'étrange et de presque pernicieux qu'elles n'existent que sous la forme de fantômes ou de mythes, qu'elles ne sont conçues que pour une semi-existence qui consiste à conserver une certaine vision des choses.  C'est l'ectoplasme du conservatisme[^4]. Avec les particules exotiques, la physique eisteinienne de la gravité est entièrement conservée et un nouveau champ de recherche, si elles existaient, pourrait être découvert: le paradigme normal serait donc étendu plutôt que remplacé.  Au premier chapitre, nous explorerons les raisons pour lesquelles elle pourrait n'être pas vue et pourquoi elle devrait cependant être prédite.  Pour le moment, il me suffit de dire que la notion de matière noire recouvre une collection d'anomalies de nature similaire et que cette collection est interprétée comme le signe de la présence d'une matière.  La nature de ces anomalies est de présenter une plus grande masse visible que celle qui est prédite. L'anomalie entière se résume à un signe quand elle est prise comme une matière noire .

A vrai dire, si le problème s'arrêtait là, ce ne serait qu'un problème classique d'histoire et de philosophie du progrès scientifique. Il ne serait pas bien original. Il aurait pu reposer sur une analyse du contexte et de la téléologie du progrès scientifique contemporain, et analyser pourquoi et de quelle manière l'on pense que cela doit être une matière qui se cache. Cependant le problème va plus loin. Le nom de matière noire ne met pas immédiatement à jour un second type d'hypothèses qu'il recouvre pourtant, le type d'hypothèses selon lequel les anomalies de masse de certains astres ne sont pas les signes d'une particule difficile à apercevoir mais bien au contraire des signes que les lois de l'astrophysique ne sont plus efficaces à certains niveaux.  Il y aurait une collection de phénomènes que les lois actuelles du mouvement des astres décrivent mal, mais tout autant que pour l'hypothèse matérielle, la communauté concernée ne dispose pas de solution définitive au problème mis en lumière. La matière comme ces lois sont encore cherchées. Ce type d'hypothèse est le revers de la pratique de la recherche par prédiction d'atomes exotiques, il consiste à transformer le paradigme afin de nullifier l'anomalie en expliquant ce qui est anormal par une nouvelle loi descriptive et prédictive de la gravité qui rend l'anormalité normale et régulière. En fait, c'est un autre type d'*ad hoc* qui conserve en grande partie le paradigme dans le but d'expliquer l'anomalie. Mais il n'opère pas par extension ou prédiction d'un nouvel objet: plutôt il propose une correction qui remplace certaines lois. C'est la voie appelée MOND (*MOdified Newtonian Dynamics*). Ce qu'elle a de particulier c'est qu'elle retire la nécessité de penser qu'un nouvel atome puisse exister. En plus de matière noire, il y a donc des lois noires. En mettant face à face les prétentions de ces deux types d'hypothèse, celui d'une nouvelle matière, et celui de mauvaises lois, on atteint alors un paradoxe épistémologique très intéressant. La question suivante devient légitime: comment est-il possible que l'existence d'une particule soit totalement mise en jeu, car ses chances d'être comme de ne pas être sont concurrentes ? C'est la raison même de supposer que cette particule doit exister qui est mise en jeu par une théorie alternative. Les deux voies, la voie exotique et la voie corrective, sont explorées simultanément. Que nous assure que l'une est plus juste que l'autre et que chacune ne pourrait pas réussir simultanément ? Si cette double réussite venait à être, ne rendrait-elle pas tout but ontologique absurde et ne laisserait-elle pas pantois les métaphysiciens de l'empyrée ? Si les deux voies étaient vraies, alors tout l'atomisme exotique se résorberait dans un nouveau réalisme et un nouveau paradigme de recherche sur la base d'une équité avec la formule représentant les phénomènes. C'est l'équation loi-particule qui devrait être découverte et constituerait l'anomalie subsumante des deux voies fructueuses. N'y a-t-il pas dans ce paradoxe de deux voies heuristiques opposées la mise à jour claire et distincte d'un paradigme entier de réalisation ?

Je retrouve là un problème d'ontologie plus profond que celui que je supposais avoir à partir de mes connaissances superficielles de l'objet d'étude que j'avais choisi au départ. D'abord je me demandais comment cette matière pouvait exister et ne pas exister en même temps, comment *a priori* inobservable on se savait pourtant ne pas la voir, mais c'était un biais imputé à la raison que je n'avais pas encore connaissance du fait que la notion décrit un ensemble d'anomalies que les hypothèses prennent comme des signes et prémisses de leur contenu.  Maintenant que je sais également que la matière noire recouvre différents types d'hypothèses, qui ne statuent pas de la même manière relativement à l'existence des choses, j'atteins un problème d'histoire et de philosophie au centre duquel je peux mettre en question la manière dont la réalité est faite. C'est alors proprement plus qu'un simple problème d'ontologie, c'est un problème d'épistémologie qui peut se formuler comme suit: par quels moyens, théoriques et observationnels, la science astrophysique permet-elle de proposer des hypothèses relatives à une même anomalie mais contenant des propos ontologiques contradictoires ? Cette fabrique de la réalité au centre de l'anomalie de la matière noire peut être prise comme le paradigme de la fabrique de la réalité en astrophysique et *in fine*, des sciences d'observation et du système de connaissances qu'elles lèguent dans nos manuels et dans nos cours. Or cette fabrique peut être mise en question et soumise à l'examen critique afin d'en posséder une connaissance profonde plutôt que de se contenter de l'histoire des vainqueurs (et des vaincus que l'on oublie facilement\...) qui est d'habitude popularisée. Sous l'oeil de la critique et de la contextualisation historique et philosophique, il est possible d'éclaircir cette situation peu commune où la création d'une réalité se fait.

La majeure partie de l'activité scientifique se déroule dans un paradigme qui ne prend pas nécessairement en compte l'entièreté de ses présuppositions épistémologiques et qui laisse la plupart du temps une partie de ses règles au statut de connaissance tacite. Ces règles ne sont alors connues que des acteurs qui participent au paradigme. En général, pour comprendre un paradigme, il faut faire partie de la communauté qui le partage et interagir avec ses participants. Le but de mon travail est alors double. D'un côté, j'ai besoin d'éclaircir le paradigme dans lequel le problème de la matière noire prend place, afin de mieux comprendre le problème en lui-même. C'est un but qui se subdivise lui-même en deux parties. Pour ce faire je dois me renseigner d'abord sur les conditions de réalité du paradigme, c'est-à-dire sur deux choses: sur l'histoire sur laquelle le paradigme est fondé et qui est conservée actuellement, ce qui me permet de comprendre le paradigme d'une manière *générale* et de me faire une certaine idée de la manière dont la relation entre les phénomènes et la vérité est conçue; puis sur son histoire locale, qui me renseigne sur la manière dont il est employé et développé de manière pratique et individuelle. Si son histoire générale peut être trouvée dans les livres, en revanche sa pratique individuelle est difficile d'accès: les articles publiés possèdent un langage et une méthodologie hermétiques qui participent déjà du paradigme sur lequel je veux me renseigner et qui m'en exclut alors nécessairement, alors que les livres d'études des astrophysiciens se trouvent à un niveau de science que je ne possède tout simplement pas - je n'ai pas une formation scientifique suffisante pour y accéder de manière fructueuse pour mon analyse. Un moyen de me renseigner sur ce qui se passe de manière locale consiste alors à m'impliquer dans une situation de dialogue et de médiation avec des astrophysiciens qui travaillent sur la matière noire. C'est ce que j'ai eu l'occasion de faire durant un stage à l'Observatoire Astronomique de Strasbourg (OBAS) durant deux semaines, l'année 2022 du 25 avril au 6 mai. J'en tire six entretiens enregistrés, de nombreuses notes supplémentaires à partir d'autres entretiens et des notes d'observations que j'ai pu réaliser sur les manières de travailler des astrophysiciens et astronomes. Ces sources historiques rétrospectives et contemporaines qui me renseignent sur la manière dont les astrophysiciens comprennent le problème et travaillent dessus me permettent ensuite d'avoir une matière solide sur laquelle proposer une analyse philosophique.

D'un autre côté, mon analyse prend l'angle de l'ontologie et cherche à reconnaître les conditions d'(in)existence de l'objet mis en question.  Pour cela, il faut également une théorie sur le fonctionnement de la théorie astrophysique, auquel cas j'emploierai des analyses duhemiennes et bachelardiennes (habituellement appliquées à la physique) et je proposerai une analyse du concept de modèle qui est si central dans le travail des astrophysiciens. Ce que je veux mettre en lumière, et ce qui m'a intrigué de prime abord, c'est la manière dont la réalité se crée.  Cependant je ferai également usage d'une analyse kuhnienne, l'auteur ayant parlé à la fois des paradigmes de recherche (les larges théories et faits sur lesquels reposent la recherche) et des anomalies , qui sont des énigmes difficiles à résoudre dans le cadre théorique duquel elles ont émergé. Les anomalies sont le parallèle de ce qu'on a l'habitude de qualifier de paradoxe scientifique et je conserverai le terme d'anomalie, qui fait référence à *La structure des révolutions scientifiques*[^5]. A partir des hypothèses de Kuhn je pourrai proposer quelques conclusions supplémentaires. En effet, l'analyse que propose Kuhn possède une certaine portée généralisatrice et inductive sur la science puisqu'elle décèle une structure de fonctionnement dont les preuves se retrouvent dans de nombreux cas historiques (sans prétendre très explicitement à l'exhaustivité). J'envisage alors de déterminer si la matière noire peut se plier à l'analyse probabiliste de cette structure, et de voir s'il est possible d'appliquer cette visée prédictive afin de produire un effet rétroactif à l'instruction des astrophysiciens. Il est en effet dans l'idée de Kuhn que plusieurs signes indiquent l'arrivée d'un changement de paradigme, l'un deux étant la démultiplication des hypothèses aliénées vis-à-vis du paradigme standard qui a pour tâche de résoudre l'anomalie. La question d'à quelle étape de déréliction du paradigme l'état actuel de la science astrophysique se situe vis-à-vis de la matière noire peut alors être posée. S'il est confirmé que le paradigme est sur le point de changer, alors les raisons de tenter de le conserver sont amoindries. Cette hypothèse supplémentaire repose cependant sur la validité de la théorie de Kuhn que j'emploie pour interpréter le problème.

## La méthode 

### Qu'est-ce que l'épistémologie ?

La méthode de recherche que je compte employer pour étudier le problème que je vois est un mélange organisé d'histoire, d'anthropologie et de philosophie des sciences. On peut qualifier cette méthode d'*épistémologique*. L'épistémologie englobe un ensemble de disciplines utiles pour étudier le fonctionnement d'un système propice à produire des connaissances. Il existe une épistémologie de la physique, de la biologie, de la sociologie, de l'histoire, de la théologie\... Le but de l'épistémologie est de rendre clair le système de règles de pensées internalisées ou inscrites par un groupe scientifique[^6]. De ce fait, on trouve différentes disciplines qui ont développé leurs propres hypothèses à l'usage de l'épistémologie. Pour accomplir mon but, j'utilise l'histoire des sciences, l'anthropologie de laboratoire, la philosophie des sciences et plus largement la philosophie de la connaissance.

#### Qu'est-ce que l'histoire des sciences, l'anthropologie de laboratoire et la philosophie des sciences ?

L'histoire des sciences a pour but de restituer la diversité qui se trouve derrière un fait scientifique. Elle s'intéresse donc aux institutions, aux personnages, aux documents, aux instruments et aux objets mis en jeu pour créer un fait scientifique. Elle doit également prendre en compte l'histoire des idées qui est à l'origine du-dit fait.

En donnant une telle importance scientifique à l'histoire je restreins de ce fait la philosophie à une méthode critique de questionnement. Je l'utilise pour définir l'ontologie qui se trouve historiquement en astrophysique de la matière noire. Trouver l'ontologie de la matière noire qui a été constituée durant l'histoire de cette notion et qui est en usage aujourd'hui est mon but ultime. Cependant, ce but ne peut être réalisé sans l'histoire qui m'informe sur ce que la science astrophysique a fait et pensé.

Un moyen de faire l'histoire au temps présent consiste à se rendre sur les lieux où le fait en question est produit. Aujourd'hui ces lieux sont très nombreux : plusieurs laboratoires travaillent sur la matière noire à travers le monde. Il n'est donc pas possible de les visiter exhaustivement. Cependant, j'ai pu passer deux semaines à l'Observatoire astronomique de Strasbourg (OBAS), qui fait des recherches sur la matière noire. Pour faire de l'histoire au présent, l'anthropologie de laboratoire est alors impliquée : d'abord développée et popularisée par Bruno Latour[^7], elle consiste à observer les chercheurs dans leur milieu naturel et à organiser des entretiens avec eux pour les questionner sur ce qu'ils font. Puis il est possible de comparer ce qu'ils font en les observant à ce qu'ils disent qu'ils font en les questionnant. L'observateur doit faire l'effort de ne pas partager le milieu des observés. Cela signifie qu'il doit se mettre mentalement à distance de son environnement pour ne pas s'y habituer et donc mieux pouvoir remarquer tout ce qui joue dans la mise en action des personnages qu'il étudie. A vrai dire, il m'a été impossible de ne pas m'habituer à un milieu auquel je me suis rendu tous les jours, même sur un  court laps de temps de deux semaines. Passer environ huit heures chaque jour durant dix jours dans le même lieu a suffit à totalement m'habituer à ce que je voyais et entendais, même contre mon plein gré.  C'est pour cela que j'ai pris des notes dès les premiers jours, surtout des notes de ce que je ne comprenais pas et qui me surprenait, au moment où j'étais encore béat et ignorant. Car après quelques jours j'entendais si souvent le même langage que j'avais l'illusion de le comprendre, et venais à l'utiliser naturellement[^8]. J'ai également enregistré mes entretiens pour les étudier par après, avec l'espoir que l'effort d'abstraction serait plus facile lorsque je serai sorti du laboratoire.  Enregistrer un document audio est aussi nécessaire pour avoir une source plus fiable : la prise de notes  en direct est sujette à tout un tas d'omissions et de transformations de la parole pour des raisons parfois difficiles à contrôler (locuteur rapide et loquace, temps de compréhension nécessaire avant de pouvoir inscrire ce qui a été compris\...).

L'utilité générale de l'anthropologie de laboratoire est d'avoir un outil pour se renseigner sur ce qui se fait actuellement en astrophysique à propos de la matière noire. Je compte combiner une source historique actuelle à d'autres sources historiques et philosophiques plus générales (moins immédiates) pour entamer la critique philosophique de ce qui se passe actuellement en astrophysique quant à la matière noire.

### Formation personnelle 

Tel qu'il est commun de le faire en STS[^9], la situation de son propre point de vue dans un contexte culturel déterminant est nécessaire pour le neutraliser: non pas que le point devienne *ineffectif*, mais plutôt qu'il gagne en neutralité car son origine est plus claire. Je dois en quelque sorte reconnaître ce qui peut jouer dans mon raisonnement afin qu'il paraisse moins contingent. C'est particulièrement utile pour une étude anthropologique, mais j'estime que cela peut être appliqué à toute entreprise.

J'ai eu depuis longtemps un intérêt pour les sciences dures, et l'astronomie et l'astrophysique m'ont intéressées depuis le lycée. J'ai découvert la philosophie en terminale et je l'ai immédiatement appréciée pour la manière dont elle permettait de questionner les évidences et tout type de connaissance afin de découvrir quelle est leur origine.  J'ai reçu une formation de philosophie en faisant une licence à l'Université de Strasbourg, laquelle dispose d'un laboratoire (le CREPHAC) axé sur la philosophie ancienne (principalement la philosophie grecque) et sur la philosophie allemande (surtout celle des Lumières et du début du XXe siècle). On y enseigne également de la philosophie française, plus classique, avec des auteurs tels que Descartes et Bergson. J'ai l'habitude de penser avec ces auteurs, et c'est pourquoi je m'intéresse à des questions d'épistémologie, de conditions de vérité, de réalisme, de raisonnement: ces questionnements ont toujours été au coeur des discours de Platon (La République, Le Phèdre), des écrits de Kant (La critique de la raison pure, La critique du Jugement), de l'esprit franco-allemand du début de siècle (Husserl, Bergson, Duhem).  Je n'étais cependant pas tout à fait satisfait de cette formation à mon goût trop abstraite pour questionner les sciences: il me fallait une méthode pour accéder à des informations d'ordre plus historique, plus concret pour philosopher dessus. Aimant la connaissance en elle-même (et c'est exactement ce que signifie *philosophe*) j'ai en effet à coeur de comprendre comment la connaissance est constituée et c'est un questionnement que je possède depuis le début de mes études. Les sciences sont souvent déclarées comme étant le sytème à la pointe de la connaissance, je devais donc m'y intéresser. Je me suis alors tourné vers la formation du master *Science et Société* avec le projet de m'intéresser à la constitution des connaissances en cosmologie.

Ce master promulguant exactement ce que je cherchais: des méthodes permettant d'inclure l'histoire dans un processus de réflexion sociologique ou philosophique. Je vois des professeur qui font des études ultra-précises du progrès de certaines connaissances à partir de collection d'archives étudiées laborieusement et minutieusement, et ce sur des époques tant anciennes que contemporaines. J'y découvre surtout l'anthropologie (ou ethnologie) de laboratoire et je désire en faire usage pour finalement étudier les hypothèses sur la matière noire. Le caractère prédictif probabiliste de l'histoire des sciences telle que Thomas Kuhn la perçoit devient également un outil essentiel pour moi, puisqu'il introduit en quelque sorte le sacré Graal si souvent cherché et perdu de la sociologie, la loi. Dans le domaine des études philosophiques de sciences et de leur culture aidées par une forte proximité aux sources, j'apprécie particulièrement les travaux de Ian Hacking, mais des philosophes-historiens comme Helge Kragh m'inspirent dans une même mesure. A partir de ma formation, j'en viens donc à considérer la philosophie des sciences comme un activité de réflexion devant faire usage de sources également abstraites et concrètes sur les théories de la connaissance, ce qui se réalise en prenant en compte conjointement l'histoire de la philosophie et des idées philosophiques et l'histoire des concepts et pratiques scientifiques.

La philosophie m'a habitué aux concepts d'universel et de transcendant mais le tournant pratique des STS m'a permis de découvrir une structure de travail et d'élaboration de la connaissance partagée par un groupe.  C'est dans cette continuité d'intérêt, qui consiste à révéler une structure d'élaboration de la vérité relativement à une histoire des pratiques qui peut être contemporaine et qui est surtout située, tout en maintenant une méthode de doute et de questionnement acerbe telle que la conçoit la philosophie, que je réalise le présent mémoire.

### Revue de la littérature 

La littérature de la matière noire est extrêmement restreinte: remplis de descriptions techniques et mathématiques, faisant usage d'hypothèses et de théorèmes uniquement connus dans des cercles de diffusion particuliers, les articles de science sont difficiles d'accès et illisibles pour le chercheur en science humaines. Il en va de même des manuels d'astrophysique et de cosmologie. En sciences humaines, les études sur la matière noire sont rares également. L'une des motivations de mon travail et de compléter ces études, dont aucune ne propose d'approche anthropologique. J'ai tout de même pu lire David Elbaz, *À la recherche de l'univers invisible: matière noire, énergie noire, trous noirs.*, Paris: Odile Jacob, 2016, et Françoise Combes, *La matière noire dans l'univers: leçon inaugurale prononcée le jeudi 18 écembre 2014*, Paris: Collège de France Fayard, 2015, pour avoir un accès direct à une histoire générale du problème. Cependant, ces livres font assez peu cas des théories de la gravité modifiée: on lui accorde rarement plus de quelques pages car ils se concentrent plutôt sur les théories concernant des corps standards ou des particules exotiques. J'ai remarqué une histoire tout à fait similaire chez les deux auteurs, qui remonte en général jusqu'au XIXe siècle. Le croisement de ces sources qui s'entreconfirment me fait penser que l'histoire doit être plutôt correcte, même si elle demeure générale. Ce qu'il est intéressant de savoir, c'est qu'Elbaz et Combes sont tous deux astrophysiciens. En fait, il ne semble pas que le monde des sciences humaines se soit intéressé à ce paradoxe de sa propre impulsion. La seule exception française que j'ai trouvé se situe dans le livre Vincent Bontemps, Roland Lehoucq et Scott Pennor's, *Les idées noiresde la physique*, Paris: Les Belles Lettres, 2016, qui propose dans l'un de ses chapitres une analyse de l'idée de matière noire, notamment en référence aux conceptions passées et alchimiques de la noirceur . Mais cette psychanalyse bachelardienne ne me permet que très peu d'avancer dans le problème que j'ai choisi, principalement car le sens de cette noirceur est déjà éclairé par l'histoire de l'astrophysique. Je n'en fait donc pas usage. Afin de compléter mes connaissances historiques rétrospectives, j'ai voulu m'instruire sur l'astronomie et les techniques d'observations, ce que j'ai fait avec Yaël Nazé, *Histoire du télescope: la contemplation de l'univers des premiers instruments aux actuelles machines célestes*, Paris: Vuibert, 2009 et Michael A. Hoskin, *The cambridge concise history of astronomy*, Cambridge; New York: Cambridge University Press, 1999 et Franck Varenne et Marc Silberstein, éd., *Modéliser & simuler: épistémologies et pratiques de la modélisation et de la simulation. Tome 1.*, Paris: Éditions Matériologiques, 2021. J'ai également eu l'occasion de lire Vera Buin, "Dark Matter in Spiral Galaxies" in *Scientific American* 248.6 (1983), pp. 96 - 109, un rare article d'astrophysique qui m'a semblé accessible. Je l'ai choisi car son autrice a fait date dans l'histoire du problème.

Remarquant que l'historiographie du paradoxe était en même temps très récente et rare, j'ai pensé qu'il y avait deux autres moyens de m'informer. L'un consistait à employer une méthode d'anthropologie, à savoir principalement l'entretien et l'observation, et à me rendre sur place pour comprendre ce que les astrophysiciens font vis-à-vis de ce paradoxe. Cela m'intéresse d'autant plus que le paradoxe n'est pas mort, mais bien au contraire tout à fait vivant aujourd'hui. Il est donc plus que pertinent de s'informer sur ce qui se fait actuellement. J'en tire onze entretiens semi-directifs anonymes dont six ont été enregistrés, cinq étant dotés d'une transcription correspondante[^10], et trois carnets de notes[^11],[^12],[^13]. L'autre moyen consistait à employer le chemin de la philosophie de la théorie et de la technique scientifique, afin de pouvoir mieux comprendre ce que ces deux sources historiques pouvaient vouloir dire. J'ai ainsi largement employé les philosophies de Gaston Bachelard (Gaston Bachelard, *Le nouvel esprit scientifique*, 2020), Pierre Duhem (Pierre Duhem, *La théorie physique: son objet, sa structure*, Paris: J. Vrin, 2007), Thomas Kuhn (Thomas samuel Kuhn, *La structure des révolutions scientifiques*, Paris: Flammarion, 2018) et Ian Hacking (Ian Hacking, *Anthropologie philosophique et raison scientifique*, Paris: J. Vrin, 2023). L'astrophysique étant en partie une physique, et chacune de ces philosophies n'étant pas un système très clos, j'ai pu les combiner pour analyser l'histoire à laquelle j'ai eu accès.

Enfin, pour avoir une connaissance plus rapprochée des théories au centre du paradoxe de la matière noire, j'ai essayé de m'informer sur la relativité générale. Ce n'est pas tâche facile. J'ai d'abord lu Paul Couderc, *La relativité*, Paris, 1962, mais j'ai été déçu par des explications bizarres et peu compréhensibles. Je me suis ensuite tourné vers Albert Einstein, *La relativité: théorie de la relativité restreinte et générale, la relativité et le problème de l'espace*, Paris: Payot & Rivages, 2001, et l'ouvrage a expliqué les mêmes principes et la même histoire, mais d'une manière bien plus claire. Je pense donc que Couderc n'a fait qu'édulcorer Einstein. Je ne donne pas une histoire de la théorie einsteinienne à proprement parler, c'est cette fois le contenu et l'origine géométrique de la théorie qui m'a davantage intéressé. Je sais qu'il n'est pas habituel de donner confiance à un scientifique qui fait sa propre autobiographie, mais ses explications sont retrouvées comme un cas particulier dans Bachelard, *Le nouvel esprit scientifique*, donc je pense qu'elles sont suffisamment instructives.

### Plan de travail 

J'ai construit mon travail en trois étapes: les deux premières vont du général au particulier en matière d'histoire, et la dernière se propose une analyse philosophique des deux premiers chapitres. En premier lieu, je construis une histoire générale de l'astrophysique et de certains mouvements que je pense être d'actualité. Ensuite, dans le même moment, je donne une histoire plus concentrée sur la matière noire. C'est une histoire jeune, qui naît en tant que telle dans les années 1930.

Cette histoire générale ne suffit pas à comprendre le dilemme en lui-même, car il est toujours présent aujourd'hui, or je suppose que les techniques ne sont pas les mêmes entre le siècle précédent et le nouveau millénaire. Il est possible de s'informer davantage à l'aide de l'anthropologie. Je dispose alors l'ensemble de ce que j'ai appris sur place en un résumé succinct qui analyse certaines phrases, certaines explications qui m'ont marqué et m'ont semblé centrales dans la pratique du problème.

Cet ensemble de données historiques constitue pour moi une matière à analyser philosophiquement, qui me permet de poser des questions d'épistémologie et d'ontologie sur une anomalie d'astrophysique. Je réalise alors une analyse de cette histoire de la science grâce aux outils conceptuels proches de la pratique scientifique de plusieurs philosophes de la science, parmi lesquels Bachelard, Kuhn et Hacking. Je propose finalement une synthèse de tout ce que j'ai appris à partir de mon questionnement sur l'ontologie de la matière noire.

# Histoire de la matière noire.

Proposer une histoire de la matière noire me permet d'acquérir une source de connaissance sur laquelle philosopher. Cette source peut être constituée de deux manières. D'un côté, je peux rappeler les larges conditions de pensée qui se trouvent au coeur et à l'origine de l'histoire de l'astrophysique en général. Dans ce cadre, je peux alors situer l'émergence de la question de la matière noire. D'un autre côté, j'aimerais proposer une histoire de la matière noire contemporaine.  Cette manière de faire me permet d'être à jour vis-à-vis de la pratique scientifique mais aussi de garder la trace d'une méthodologie qui est d'habitude implicite et connue uniquement d'un cercle fermé d'habitués.

Les travaux de Michael Polanyi mettent en avant cet aspect de la science qui se trouve dans le laboratoire - et qui est difficile à trouver autre part. Catherine Allamel-Raffin raconte à ce propos l'histoire de la technicienne-observatrice d'un microscope électronique, qui elle seule savant assez bien utiliser un échantillon pour en tirer quelque chose, doit cependant aller au travers d'interminables discussions sur l'interprétation à des donner des images produites par l'instrument[^14],[^15]. Ce genre de savoirs, qui ne sont pas montrés à l'oeil du public, qui ne sont pas communs à tous et qui ne sont en général pas même conscientisés par les pratiquants - ils se savent certes compétents, mais il n'y a pas nécessairement un moyen d'expliquer entièrement comment ils le sont devenus à moins de mener une enquête biographique - ce genre de savoirs est qualifié de tacites . Le qualificatif de la tacité révèle en même temps la présence et le caractère caché de ce type de savoirs. Cependant, ce n'est pas parce qu'ils sont cachés qu'ils ne peuvent pas être découverts.

Sur ce point, c'est l'éthnométhodologie qui m'a frappé et m'a donné envie de me retrouver dans un laboratoire. Ceci de deux points de vues.  Celui assez descriptif de Michael Lynch, dont j'ai une connaissance éloignée mais dont l'exemple d'une expérience entièrement reproductible à partir de la méthode scientifique analysée par les notes minutieuses d'un éthnométhodologiste acharné m'a beaucoup frappé. Il n'est pas forcément nécessaire d'être scientifique pour faire de la science, c'est-à-dire que les connaissances employées et apprises dans des cercles fermés accessibles selon des conditions très restrictives peuvent tout de même être découvertes et reproduites par un observateur extérieur. D'autre part, c'est l'éthnologie de Latour qui m'inspire. Je ne me reconnais pas latourien - l'idée de réseau est trop complexe, trop large et presque fourre-tout - mais son enquête dans *La vie de laboratoire* a ceci de passionnant qu'elle montre que tous les aspects de la science de laboratoire peuvent être soumis au regard des sciences humaines: à celui de la philosophie, de la sociologie, de l'histoire.  Tout autant que la philosophie qui est une méthode de questionnement, les autres sciences humaines n'ont pas d'objet privilégié et c'est là une capacité forte d'extension du savoir. La science n'a de fait pas de position particulière, elle n'est pas hermétique, ininterprétrable et inattaquable par rapport aux sciences humaines, qui sont libres du choix de leur sujet d'étude. L'histoire et l'anthropologie permettent de transgresser le mythe habituel de la science dure reine de toutes les sciences, et définitrice apparemment ultime de la vérité, protégée dans sa tour d'ivoire.

L'histoire large que je veux raconter remonte jusqu'à Platon. Je trouve chez l'auteur une manière de penser qui a été ravivée en occident à la renaissance et qui a définitivement impacté la science telle que Newton puis les astronomes l'envisagent au XIXe siècle. Je mentionne l'apport de la relativité générale à cette histoire, puisque c'est la théorie qui est employée ou discutée par les astrophyisiciens de la matière noire, mais aussi car je pense qu'elle s'inscrit dans une même continuité. Puis à partir de ce large contexte je propose une histoire quelque peu plus précise: une histoire des idées et des techniques sur la matière noire.

Je disais que je ne me reconnaissais pas latourien, et en effet je me reconnais bien plutôt kuhnien. Pour moi, cette histoire permet de retracer l'origine de règles de pensées au centre du paradigme de l'astrophysique actuelle à propos de la matière noire. Selon Kuhn, toutes les règles de pensées ne sont pas théorisées. C'est pour cela que je parle de cas analogues à la matière noire - il me semble qu'il y a une manière de proposer des hypothèses commune à ces anomalies similaires situées dans un paradigme ressemblant mais qui a crû au cours du temps, et que cette manière n'est pas nécessairement explicite en quelques mots. Mais elle est effectuée par quelques personnages.

J'introduis enfin une analyse anthropologique de la manière actuelle de proposer des hypothèses, que j'ai effectué en me rendant durant deux semaines à l'Observatoire Astronomique de Strasbourg, du 25 avril au 6 mai 2022. Cette anthropologie me permet de lier à la fois le contexte passé et le contexte présent, et de m'informer mieux sur les pratiques de la science que par un regard rétrospectif qui raterait peut-être la méthode de quelques décennies.

## Une histoire de la philosophie de l'espace-temps.  

Nous sommes habitués à faire l'expérience du temps et de l'espace. Ce sont pour nous un ensemble de phénomènes aisément qualifiables : l'espace, c'est *ici*, c'est *là*, c'est *là-bas*, c'est *par là*, c'est *au loin* (voir la figure ci-dessous)... L'espace est une position perceptible située par rapport à notre corps. Quant au temps, c'est une expérience psychologique quasi-continue qui ne cesse qu'au moment du sommeil (bien qu'il puisse se présenter à l'intérieur du rêve). Le temps est une expérience du devenir des choses et de soi-même. C'est la sensation de l'écoulement vers le passé, c'est l'activité de la mémoire dans le présent. Le futur est une projection du mouvement possible, c'est un autre élément pragmatique de l'esprit. L'espace et le temps sont des expériences psychologiques et perceptives totalement subjectives. Ils sont différents pour chaque individu. L'espace dépend de l'intention de positionnement du sujet dans un environnement, et le temps dépend de son activité mémorielle et prospective[^16].

![image](/img/TEKA0071693EXTR.jpg)

Dans les sciences de la nature, les notions d'espace et de temps ont un sens objectif, uniforme et maîtrisé. L'espace et le temps sont des formes mathématiques abstraites qui permettent de décrire la situation concrète d'un évènement à l'aide d'une métrique, d'un corps de référence et d'instruments de mesure. Mais alors comment s'assurer que la forme abstraite de l'espace-temps corresponde à la forme concrète de l'espace-temps: que nous dit que le devenir perçu par les formes subjectives de l'espace-temps est reproduit par l'espace-temps abstrait ? Il y a-t-il un temps et un espace universels ?

De toute évidence l'opération d'abstraction et de mesure ne peut pas conférer à l'espace-temps le même statut que celui qui appartient au positionnement d'un corps dans un environnement et à l'activité de la mémoire d'un sujet: précisément car cette opération de mesure et d'abstraction est une transformation du substrat original de l'expérience. S'il n'est pas possible de le saisir intuitivement, il convient donc à tout le moins, si nous voulons comprendre en quoi consiste l'espace-temps de la cosmologie actuelle, d'en faire une histoire qui montre ses racines et sa justification à partir de l'histoire de l'espace-temps en sciences physiques et dans l'occident.  Il convient de chercher ce qui est transformé et quelle est l'histoire de cette transformation. Il faut chercher comment l'espace et le temps sont représentés pour comprendre les problèmes qui se situent dans leur cadre.

Pour ce faire je me tournerai tout d'abord vers l'antiquité grecque, où se trouvent les racines occidentales de la conception de l'espace et du temps. Ces premières idées, les efforts des astronomes grecs et la révolution de l'humanisme me mèneront vers le concept de représentation qui est au centre de l'épistémologie physique à la fin du XIXe siècle et au début du XXe siècle, en France et en Allemagne. Enfin je discuterai de l'origine et de l'utilisation de la relativité générale en cosmologie. C'est la relativité générale qui promulgue aujourd'hui la représentation majeure de l'espace-temps mais elle est remise en cause par l'anomalie de la matière noire: pour comprendre cette remise en cause il faut comprendre la forme d'espace-temps qui est concernée.

### Le temps et l'espace : une approche historique.  

Pour trouver d'où provient le concept d'espace-temps qui est actuellement en usage en astrophysique à partir de la relativité générale, il faut se tourner vers l'antiquité grecque. Je pense que c'est en effet là que commence une tradition qui continue d'être perpétrée aujourd'hui.

Platon et Aristote ont une conception particulière de l'espace et du temps où le devenir est central. Je montrerai que cependant chacun d'eux ne propose pas une conception entièrement identique, et que leurs concepts respectifs ont également influencé les concepts subséquents de l'espace et du temps dans l'histoire occidentale de l'espace-temps.  Notamment, je retrouve aujourd'hui l'idée que les apparences et la réalité sont deux concepts séparés.

Selon Platon, il faut distinguer les apparences de l'essence, et ainsi le devenir du monde des idées. Le devenir désigne le changement opéré par les objets de la nature. La séparation n'est pas d'ordre physique, elle est d'ordre dialectique: le monde des apparences est ce à quoi l'on accède par les sens, alors que le monde des idées est accessible par le raisonnement ou l'intelligence (le *nous* en grec) en partant des apparences. Il faut selon la métaphore du boucher qui est donnée dans le *Phèdre*: détailler par espèces suivant les articulations naturelles, en tâchant de ne briser aucune partie comme le ferait un mauvais découpeur de viande[^17], c'est-à-dire qu'il faut retrouver l'ossature ou la structure du réel à partir de celui-ci en l'analysant avec le couteau de la pensée. C'est une opération d'observation, de distinction et d'abstraction. Le devenir (l'ensemble des apparences dans leur mouvement) n'est donc pas scindé du monde des idées qui constitue l'ensemble des essences ou ossatures supportant les apparences.  Cependant ce sont deux mondes séparés d'une manière intellectuelle: il faut partir des apparences pour accéder aux idées, ce qui crée deux niveaux épistémiques différents selon Platon. D'autre part, dans le *Phèdre*, Platon développe le mythe que le ciel des idées est opéré par les dieux selon une marche cyclique et géométrique, en cercles qui s'élèvent. On trouve davantage de géométrisation du monde des idées et des apparences dans le complexe mythe créateur du *Timée*. Les géométrisations du ciel que Platon propose sont avant tout d'ordre mythologique[^18].

Platon introduit donc deux choses dans ses discours: d'une part une géométrisation de l'espace, laquelle est divine (intellectuelle[^19] et mythique) et essentielle (elle ne fait pas partie du monde des apparences, mais du monde de l'intelligence). Il propose donc une différence dans l'ordre de la connaissance entre les apparences et les essences. Les apparences ne sont pas fausses, mais elles ne sont pas connaissables en elles-mêmes: il y a besoin d'une explication intellectuelle pour en posséder une connaissance. Avec Platon, les apparences n'appartiennent pas à une logique de la vérité contenue dans la sensation ou située dans les objets d'une manière extérieure au sujet pensant. Je vais montrer que le système de vérité des apparences change déjà avec Aristote, puis avec l'instrumentation et la métrique après la Renaissance.

Si chez Platon l'essence est une idée divine , au sens où le *nous* est l'élément humain qui se rapproche le plus du divin, le caractère supérieur de cette connaissance la sépare du monde corruptible du devenir. Dans le monde du devenir rien ne se maintient comme tel. Rien n'est stable, éternel et à l'épreuve du mouvement, mais le mouvement est un progrès ontique: il n'est pas abstrait, il est sensible et apparent.

Aristote réussit à rapprocher les apparences et les essences en considérant que les idées ne sont pas seulement des structures, mais des éléments moteurs du réel: ce sont des causes[^20]. Ainsi, la *physis* grecque (qui désigne le devenir de la nature au sens où elle est mouvement et croissance) peut être expliquée selon une théorie des causes motrices de chaque élément naturel. Les espèces du réel sont donc expliquées de manière intellectuelle par un ensemble de causes dont le lieu est placé dans le devenir et semble appartenir à l'espèce. En naturalisant les idées, Aristote propose une théorie différente de la vérité par rapport aux apparences. Si certes les causes sont d'origine rationnelle (on en trouve la même théorie dans l'ensemble de textes *Métaphysique* et *Physique*[^21]) leur application devient plus évidente avec Aristote: elles sont diversifiées et plus précises que la structure générale d'un objet réel que propose Platon. On peut dire qu'Aristote a développé l'ossature du réel que Platon avait aperçu dans le *Phèdre*. Les causes prennent en compte l'idée de croissance, de direction du vivant, et tentent même d'expliquer l'origine d'accidents[^22] et de la diversité au sein d'une même espèce. Aristote diversifie la théorie platonicienne de la connaissance des apparences et ce faisant, il la sécularise au sens où il promulgue une explication plus étoffée du fonctionnement des apparences, ce qui lui donne un caractère quasi-réaliste l'éloignant du monde des idées platonicien qui revendiquait clairement son origine dialogique et l'éloignement d'avec le devenir.

Cependant les apparences en philosophie grecque ne concernent pas seulement les êtres sur Terre. Le ciel aussi se meut, il devrait ainsi faire partie du monde des phénomènes. Mais les mouvements du ciel sont plus constants que les apparences de la nature: ils ne sont pas corruptibles. Aristote distingue de ce fait entre le monde sublunaire (au-dessous de la Lune) et le monde supralunaire (au-dessus de la Lune).  Le monde dessous la Lune est soumis au dépérissement et à la naissance à mesure qu'il se meut, mais celui d'au-dessus la Lune possède un mouvement éternel et identique. La constance du mouvements des astres distinctibles dans le ciel permet à Aristote de les géométriser, déjà à la manière de Platon qui les symbolisait dans le Phèdre par la ronde des dieux dans le ciel. Les astres se meuvent en cercle parfaits, et chaque astre est lui-même une sphère. Dans le modèle cosmologique qu'il propose, les astres parfaits mettent en mouvement le monde corruptible: ce qui introduit une théorie des causes à plus large échelle. Mais cette théorie générale des causes implique que le mouvement du ciel fût lui-même l'effet d'une cause précédente. Afin de ne pas remonter indéfiniment dans l'ordre des causes, Aristote invente par nécessité logique une première cause qui a mis les astres en mouvement, une cause qu'il appelle divine. L'invention de cette cause permet d'éviter une régression infinie dans l'ordre des causes à l'échelle cosmologique.

Aristote et Platon proposent une géométrisation sommaire du réel qui commence par l'astronomie. En cherchant une structure sous-jacente au réel, ils entament également une distinction épistémique entre l'apparence sentie et l'idée de vérité. Ces deux manières de penser créent une tradition de recherche fortement perpétuée à partir de la Renaissance et des efforts de Descartes (qui propose une forme d'atomisme géométrique universel à partir de la forme du tourbillon[^23]), de Copernic (qui représente géométriquement le mouvement des planètes dans un système solaire héliocentrique à partir d'observations, notamment avec l'aide de Rheticus) et de Galilée, qui exprime l'idée dans *L'Essayeur*, 1623[^24], que: La philosophie est écrite dans cet immense livre qui se tient toujours ouvert devant nos yeux, je veux dire l'Univers, mais on ne peut le comprendre si l'on ne s'applique d'abord à en comprendre la langue et à connaître les caractères avec lesquels il est écrit. Il est écrit dans la langue mathématique et ses caractères sont des triangles, des cercles et autres figures géométriques, sans le moyen desquels il est humainement impossible d'en comprendre un mot. Sans eux, c'est une errance vaine dans un labyrinthe obscur [^25]. Ces formes géométriques que Galilée mentionne font référence aux formes platoniciennes qui sont très populaires dans la culture de l'humanisme, et tirées du *Timée* de Platon. Les trois lois de Képler et les lois de Newton mathématisent également le réel. L'humanisme de la Renaissance a généralement introduit l'idée d'une nature géométrisable. Cette fois la mathématisation et les développements de l'algèbre touchent autant la *physis* que l'empyrée, la géométrie se naturalise.

Ce qu'ont entamé Aristote et Platon de manière essentielle, c'est donc l'idée que le devenir ne puisse être compris sans *explication*. Le devenir n'est vrai qu'à condition d'être rationalisé. Cette idée est réitérée durant l'ère de l'humanisme[^26]. L'oeil, la main et tous les sens, ne suffisent pas à saisir le réel: il faut encore le comprendre par les mots, les généralisations, voire les causes et les lois. L'une des manières de rationaliser le réel consiste à le représenter par des formes géométriques et des nombres permettant des calculs algébriques.  Une origine potentielle de ces nombres et formes peut être instrumentale (lunette, télescope) et cette origine empirique se développe surtout à partir du XVIIe siècle et des Lumières.

Cependant une telle conception du réel n'est pas nécessaire. La connaissance de l'espace-temps n'a pas à être une mathématique: je veux faire remarquer que c'est un choix.

### Phénoménologie de l'espace-temps.  

J'aimerais montrer, grâce à la description de l'expérience humaine, à partir de points de vue phénoménologique et anthropologiques que la géométrisation de l'espace-temps n'est pas une nécessité comme si c'était une règle naturelle de l'esprit humain qui doit se produire en tout lieu et en tout temps. Au contraire, c'est une invention située dans une certaine tradition. Il est en effet possible de considérer que l'espace et le temps n'ont rien de mathématique, ni même d'abstrait et de conceptuel, mais qu'ils correspondent à une manière transcendantale (c'est-à-dire inhérente à la nature humaine) de faire l'expérience du monde.

#### La durée chez Henri Bergson.  

En effet, selon l'*Essai sur les données immédiates de la conscience*, 1889[^27] de Bergson (1859 - 1941), l'espace et le temps sont des formes conjointes dans l'esprit sous le nom de la durée . Ces formes sont étendues dans la mémoire de sa surface présente dans l'action à ses profondeurs éloignées dans le souvenir. On distingue ainsi entre mémoire présente et mémoire profonde, bien que pour Bergson ces deux mémoires ne forment qu'un seul puits. L'espace-temps prend place de la prospection à la réminiscence en passant par la rêverie. La durée bergsonienne se situe volontairement tout à l'opposée de la conception kantienne des formes transcendantales de l'espace et du temps puisque cette dernière est très mathématisante selon l'auteur. Bergson se concentre au contraire sur une définition qualitative qui vise à reproduire la durée telle qu'elle est immédiatement vécue: c'est une suite de phénomènes *successifs*, *contrepénétrés*[^28] et *hétérogènes*. La succession n'est pas linéaire car il n'y a pas dans la durée de flèche stricte du temps du fait de le contrepénétration des souvenirs: les connaissances passées ont une influence sur les connaissances présentes à l'échelle de la mémoire individuelle. La perception du présent est alors constituée d'une succession d'états de conscience dans chacun desquels les jonctions de la sensation et du souvenir sont réalisées. L'hétérogénéité de chaque nouvel état de conscience est nourrie par la contrepénétration des souvenirs et par la situation dans un environnement du corps à l'origine de la perception acquise. Chez Bergson, l'espace-temps est une activité de sensation et de réflexion unique et individuelle. Les conceptions instrumentales et mathématisées du temps présentent des phénomènes *discontinus*, *distincts* et *homogénéisés*, qui n'ont *a priori* aucun rapport avec l'expérience subjective de la durée.

Bergson s'exprime surtout par images et métaphores, et il est possible d'aller plus loin dans l'analyse qu'il aborde. Le soucis de rappeler l'expérience immédiate du monde et la forme qu'elle prend avant toute intellection est celui de la phénoménologie, une discipline qui s'est largement développée au début du XXe siècle avec notamment Husserl et à sa suite Heidegger, Merleau-Ponty, Sartre\... Son but est également de rendre compte de l'intelligence et du raisonnement, après l'importance du corps et de la conformation de la psychologie humaine dans l'ordre de la connaissance. La phénoménologie a une portée fondamentale. Dans *Physique de l'espace et phénoménologie de l'espace*, Gunnar Declerck, 2011[^29], une analyse de l'espace-temps du point de vue phénoménologique similaire au bergsonisme mais qui se concentre sur son aspect pragmatique et le développe est donnée. Il est fait généralement référence à un type *conatus*[^30], qui motive l'individu à spatialiser le monde: l'espace et le temps devenant ainsi la métrique de l'effort à réaliser afin de se mouvoir en vue d'aboutir à une fin envisagée par l'acteur. Ce qui primerait dans la spatialisation du monde, ce serait donc un désir d'agir, une finalité employée. Que l'on soit bergsonien ou plus pragmatiste encore, l'espace-temps peut être saisit d'une manière radicale sans concepts intellectuels. Il peut être vécu radicalement en opérant une réduction phénoménologique faisant abstraction de toute connaissance mathématique élaborée intellectuellement et en se concentrant sur l'aspect pragmatique du monde.

De fait, l'espace-temps se montre comme un substrat original profondément différent de la manière dont il est *conçu*, qui possède un aspect purement nerveux et pratique. L'espace-temps mathématisé n'est donc pas anthropologique dans un sens ontologique : ce n'est pas en nombre et formes géométriques que le devenir du monde est d'abord perçu.  Cet espace-temps est une construction abstraite et arbitraire qui remanie la connaissance humaine fondamentale du réel, celle qui apparaît lorsque l'on fait abstraction de toute culture. Si cette analyse phénoménologique et transcendantale de l'espace-temps est possible en philosophie grâce à une attention portée à la phénoménologie de l'expérience humaine du devenir, elle n'est en revanche pas aussi commune dans les sciences de la nature. L'espace-temps des sciences physiques, selon Declerck, est considéré d'un point de vue purement représentatif. Ce point de vue chercherait à expliquer sa propre émergence, sans sortir pour autant de la logique mathématique qui est la sienne. Il semble alors que l'explication transcendantale en physique pour un physicien soit impossible car elle demeure au niveau descriptif des mathématiques qui est déjà une transformation du substrat original de la perception humaine, ce qui constitue une pétition de principe. Une véritable physique transcendantale intégrerait le facteur de perception humain à son analyse de l'espace-temps [^31].

Selon une analyse fondamentale de la perception humaine de l'espace-temps, le lien entre le point de vue anthropologique et le point de vue intellectualisé devrait alors être opéré par un certain moyen de traduction. De fait l'espace-temps mathématisé est un espace-temps facultatif, qui traduit un état de conscience fondamental dans une langue choisie dont la logique est contingente et léguée par une culture précise.

#### L'espace-temps performatif des *indiens du cosmos*.  

En effet une toute autre langue que celle des mathématiques et de la géométrie progressant au cours de l'histoire de l'occident (à partir de la Renaissance en Angleterre, en France, en Allemagne, en Italie et aux Pays-Bas) aurait pu être empruntée. Dans M. Young, Pity the Indians of Outer Space , 1987[^32] (Plaints soient les indiens du cosmos ), l'anthropologue présente une partie de la cosmologie Zuni, qui se trouve aussi dans des versions similaires chez plusieurs peuples amérindiens natifs. Les points majeurs de cette cosmologie avec lesquels le legs de la Grèce et de l'humanisme contrastent sont: l'unité de la Terre et du Ciel, ces deux milieux obéissant à la même logique, au contraire de l'explication d'Aristote; une conception organique et mythique de l'univers, qui fait usage de figures à respecter plutôt que d'essences intellectuelles scindées des apparences et trouvées dans un monde virtuel par une activité de pensée individuelle et dialectique ; un temps cyclique dont l'activité passée influence l'activité présente, et où la réincarnation est une possibilité ouverte ; une situation sans nombre du passé, l'origine du temps n'étant pas définie (elle est tout simplement *avant*) ; enfin la possibilité de déplacer un lieu par un mouvement rituel, par exemple lors de la rencontre de la Lune dans la *kiva*. Ces concepts montrent l'utilisation d'une logique différente vis-à-vis de la spatialisation et de la temporalisation du monde. Il est donc possible de posséder un corps et des facultés intellectuelles en commun tout en produisant un ensemble conceptuel par rapport à l'espace et au temps tout à fait différent. La phénoménologie de l'expérience humaine n'implique pas une rationalisation déterminée du réel.

#### La contingence des représentations: le cas des épicycles de Hipparque.  

La culture grecque possédait une science astronomique, et des auteurs tels que Platon et Aristote ont proposé des cosmologies sommaires (et principalement mythiques chez Platon) ainsi que des moyens d'accès à la connaissance par rapport aux phénomènes du ciel et de la nature. Ces moyens sont fondés principalement sur la géométrie et sur l'idée que les apparences ne sont pas satisfaisantes pour être dites vraies, mais qu'elles nécessitent une explication qui aille au-delà des apparences.  Ces idées ont grandement influencé la manière dont l'espace-temps a été conçu en occident et dans les sciences européennes qui ont émergées de cette tradition grecque. Dans cette tradition grecque, l'on trouve un mot d'ordre qui détermina pour longtemps l'activité des astronomes: *sozein tai phenomena* , il faut sauver les apparences . J'aimerais en parler car ce mot d'ordre se rapproche de l'idée de représentation employée en physique au XXème siècle, et car il fut longtemps une limite du savoir possible sur les apparences du ciel.

La tradition de sauver les apparences en astronomie fait référence à l'injonction grecque de représenter adéquatement le mouvement des astres sans pour autant tenter d'en saisir la nature[^33]. Les mouvements des astres devaient être représentés par des formes géométriques.

Cette injonction mène à un paradoxe flagrant durant l'antiquité. Vers le II^e^ siècle av. J.-C., l'astronome Hipparque proposait deux moyens équivalents de représenter le mouvement des astres dans le ciel. L'un était appelé l'épicycle et l'autre, l'excentrique. Ces moyens étaient mathématiquement (logiquement) corrects, mais les trajectoires n'étaient pas identiques. Le paradoxe pose la question de comment il est possible que ces deux modèles en apparence équivalents puissent exister et décrire une réalité apparemment une.

Le paradoxe introduit le problème de la prédictivité d'un modèle mathématique et de sa valeur ontologique , c'est-à-dire de sa justesse vis-à-vis de ce que *sont* les phénomènes. A partir du moment où deux modèles *a minima* ne décrivent pas les phénomènes de la même manière, la question de la réalité sous-jacente aux phénomènes se pose, afin de déterminer lequel est le plus correct. Ce problème est saillant dans le cadre de l'astronomie, qui est une science à part et entièrement descriptive dans sa conception aristotélicienne, fondée sur l'observation et la géométrie.

La tradition de sauver les apparences est brisée avec les effort explicatifs de Copernic (1473 - 1573) et Képler (1571 - 1630) qui cherchent une loi mathématique plus simple en concordance avec les apparences, et qui cherchent donc à restreindre les moyens de description par l'observation et donnent en quelque sorte une première idée de preuve qui n'est pas contenue dans la maxime grecque. Elle est de surcroît abolie par Newton: il réussit le premier à établir une théorie générale de la gravité faisant le lien entre la physique des cieux (qui se contentait habituellement de n'être que descriptive) et la physique de la nature (qui se permettait d'être explicative et cherchait à saisir la nature du mouvement).

En publiant les *Principes mathématiques de la philosophie naturelle*, 1759[^34] Newton détruit la scission établie par Aristote entre le monde sublunaire et le monde supralunaire du point de vue de l'explication géométrique. Ces deux mondes sont de manière identique soumis à la loi de la gravité[^35]. L'analyse du mouvement devient alors générale, lorsqu'elle était scindée entre la physique et l'astronomie auparavant[^36]. D'autre part, Newton possède des instruments pour observer le ciel: il invente notamment le télescope newtonien, bien qu'il ait peu utilisé l'instrument pour observer le ciel.

L'ère de l'astronomie qu'engendre Newton et ses prédécesseurs (Descartes, Képler, Galilée) peut donc se caractériser par: une mathématisation et géométrisation des forces qui expliquent le mouvement; une unification du monde sublunaire et supralunaire; un premier développement de l'instrumentation dans la fin d'observer le ciel: une méthode empirique qui pourrait restreindre la logique de description des apparences par rapport à la manière grecque de faire science[^37]. Avec Newton, l'espace et le temps deviennent des outils géométriques universels et restreints par la physique générale et les mathématiques. Ces outils de pensée possèdent un domaine d'applicabilité grâce aux instruments d'observation. A ce titre, la géométrie commence à représenter les phénomènes: les phénomènes sont le critère de véracité de la théorie, alors que le cas était inverse chez les grecs: la géométrie était le critère de véracité des apparences. Si la théorie newtonienne n'est pas exempte de métaphysique (du fait de ses propositions théologiques et de l'action à distance), c'est en revanche cette voie mathématisante, universalisante, géométriquement abstraite et empiriste (instrumentaliste)[^38] qui mène au développement de la physique et de l'astronomie jusqu'à l'astrophysique. En renversant le critère de véracité entre géométrie et apparence par rapport à la Grèce antique, on en vient *in fine* à expliquer la possibilité la création de la relativité restreinte puis de la relativité générale par Albert Einstein.

Selon l'histoire que j'ai fait des sciences de la nature englobant la physique de la Terre comme la physique Céleste, il semble que l'esprit mathématisant et la cogniscibilité[^39] soient nécessairement corrélés.  Mais la connaissance n'est pas par nature mathématique. je le montrais déjà en rappelant la conception amérindienne du cosmos, je peux le rappeler encore en montrant que même dans la tradition de l'instrumentalisme et de la représentation mathématique, la recherche ne progresse pas nécessairement mathématiquement, en prenant l'exemple de l'histoire du phénomène de décharge électrique dans un gaz[^40].

![image](/img/geissler_tube.png)

Si aujourd'hui le concept est subsumé dans le notion de *plasma*, il était auparavant vu comme une relation difficile à saisir entre l'électricité, le gaz, la lumière, la force magnétique et la chaleur. Il questionnait la nature de chacun de ces concepts physiques et leurs relations mutuelles. Le phénomène était d'une grande complexité et il était très mal compris d'un point de vue théorique: une longue partie de la recherche opérée dessus (entamée au XIXe siècle) demeura longtemps entièrement empirique avant d'être mathématisée de manière prédictive.  Si l'on regarde dans les carnets de recherche de Michael Faraday (1791 - 1867) et principalement de Julius Plücker (1801 - 1868), qui sont connus pour être des pionniers de l'effet, l'on s'aperçoit que les auteurs font deux choses: ils tentent de faire varier les causes hypothétiques de production des phénomènes, ou d'interférer avec elles, et ils sont subjugés et motivés par la beauté de l'effet visuel des rayons cathodiques. On trouve ainsi dans [@hiebert_en_electric_1995] plusieurs descriptions des carnets de recherche des chercheurs. A propos de Faraday dans son *Diary* de Juin 1836, l'auteur écrit[^41] The ramifications, seemingly endless, produced appearances \"of great beauty\" ^5^ et: one of the Friday Evening Lectures at The Royal Institution in January 1859: \"Few subjects of physical investigation possess greater interest than the electric charge; its brillant effects and mysterious characteristics offer powerful stimuli to curiosity and enquiry\" [^42]. La découverte de nouveaux phénomènes est une réelle motivation pour Plücker: le chercheur est fasciné: Conspicuous in the account is Plücker's poignant and running commentary about the salient, delicately varied, and \"beautiful\" visual phenomena occurring along the full length of the tube: *schöner Effekt, besonders schönes Licht, schön geschichtetes Licht, schöne Ringe, schönes Spektrum, schöne Streifen* [^43]. Il décrit dans ses carnets son plaisir à la contemplation de ces effets (*schön* signifie beau en allemand), un plaisir qui est similaire au concept kantien de sublime [^44]. Le sublime de Kant implique simultanément un sentiment de terreur face à un phénomène, et le plaisir intense de le contempler: c'est l'écrasement ressenti face à ce qui surpasse le sujet percevant (illustration de ce sentiment ci-dessous).


![image](/img/3EXTR.png)

Dans un cadre où les causes du phénomène cathodique étaient trop complexes pour être contrôlées, leurs effets étaient souvent imprévisibles. Il se mêlerait alors chez Plücker la peur de ne pas comprendre ce qu'il aperçoit, de ne pas savoir d'où cela provient, jointe avec le plaisir de contempler le phénomène lumineux et coloré.  C'est le plaisir de l'incompréhensible, de l'incommensurable à l'échelle d'un esprit rationnel, de l'imprédictible. C'est un plaisir romantique [^45]. Si Plücker était à l'origine mathématicien, ce n'est pas nécessairement avec un esprit mathématique qu'il abordait toujours le problème.

A partir des multiples histoires que j'ai raconté, je veux montrer qu'une représentation mathématique et géométrique de l'espace-temps n'est pas la condition *sine qua non* de la connaissance des phénomènes.  Plutôt, c'est le produit d'une partie de la tradition occidentale en matière d'épistémologie de la nature. Cette manière de concevoir le monde est donc située: elle n'est pas universelle. Elle introduit une scission entre ce qui existe en apparence et ce qui existe abstraitement, parallèlement entre ce qui est faux ou n'est pas susceptible de vérité (l'aperçu) et ce qui est vrai (le concept abstrait expliquant la perception). Le relativité générale se situe dans cette tradition représentative d'abstraction. Elle peut être considérée comme le point culminant de la symbolisation de l'espace-temps par un moyen mathématique en astrophysique et elle est au centre du paradigme actuel de la cosmologie. L'anomalie de la matière noire la remet en question.

### L'espace représentatif du XXe siècle: Duhem, Mach, Poincaré et Albert Einstein.

Au début du XXe siècle, il se développe en France et en Allemagne ce que l'on peut appeler une épistémologie de la représentation en physique. Le physicien et historien des sciences Pierre Duhem (1861 - 1916) la met au fondement de la pratique de la théorie physique dans son ouvrage *La théorie physique: son objet, sa structure* qu'il publie en 1906 [^46]. Duhem possède une relation épistolaire avec le physicien allemand Ernst Mach (1838 - 1916) et le cite dans son livre[^47], ce dernier partageant ses idées: la physique est une activité économique, commode et esthétique de classification et composition de lois qui porte sur la représentation mathématique. Henri Poincaré (1854 - 1912) dispose d'un point de vue similaire à Duhem mais il est plus radical[^48].

Cette philosophie que ces physiciens et philosophes français et allemands partagent résulte du développement de la physique expérimentale, de l'instrumentation, de recherches fondamentales[^49] et de la logification et systématisation des lois physiques. Elle peut se résumer ainsi: la théorie physique est un ensemble de lois classées et librement choisies dont l'élaboration n'est restreinte que par la logique des mathématiques envisagée; la vérité des propositions du système ne reposant que sur le respect des règles logiques employées et sur les valeurs tirées de systèmes instrumentaux. Les systèmes instrumentaux constituent un ensemble de techniques empiriques dont les productions qualitatives sont symbolisées par des signes appartenant aux mathématiques, une idée que je résumerai en étendant le terme bachelardien de *phénoménotechnique*. Le physicien expérimental s'occupe de combiner les signes de l'évènement qu'il étudie, mais le physicien théoricien est d'autant plus libre dans le choix de ses combinaisons par le fait qu'il est averti non pas des symboles des découvertes expérimentales de cas particuliers, mais des généralisations de moindre échelles qui en sont tirées par les physiciens expérimentaux et qui se présentent sous la forme d'équations. Il est en toute liberté de lier les lois de son choix entre elles[^50]. Cette activité générale et abstraite lui permet de déduire de nouveaux évènements possibles des lois qu'il a créé. *In fine*, ce n'est cependant que l'observation instrumentale de l'évènement prédit qui confirme la vérité de la loi inventée. Duhem soulève à propos de cela un problème intéressant d'ontologie et d'épistémologie. Cette pratique libre de la théorie physique pousse le théoricien à considérer que sa prédiction réalisée grâce à un moyen purement abstrait doit signifier quelque chose à propos du fondement de la réalité, lorsqu'elle est confirmée par des mesures expérimentales. En effet la confirmation par l'observation d'une loi abstraite donne l'impression que la nature ne se laisse pas seulement décrire en langage mathématique, mais qu'elle a toujours été composée en cette langue, à cause de l'adéquation entre la production intellectuelle et l'observation qualitative remarquée. Duhem exprime ce fait surprenant[^51] pour le chercheur dans la mesure où l'évènement de la preuve force la conviction de la découverte d'un *existant fondamental et absolu* alors que l'auteur considère que la physique doit être une activité *antimétaphysique*, qui ne se soucie que de représenter des mesures instrumentales symbolisées selon un système choisi possédant certaines règles d'opérations (les mathématiques et l'ensemble des lois impliquées dans l'observation). Or l'efficacité de la prédiction ferait nécessairement choir le chercheur de son piédestal antimétaphysique pour tomber vers la métaphysique du réalisme mathématique, du moins du moment que la fascination de la prédiction théorique persiste.

Cependant, si l'on considère l'histoire de la physique telle qu'elle s'est développée après Newton et telle que Duhem la décrit à sa propre époque, la pétition de principe qui cause la surprise du chercheur est évidente. Les théories physiques censées sanctionner la vérité du mouvement des phénomènes font partie d'un système logique général comportant des nombres et des points, que l'on appelle mathématique et géométrie. Or ce même système est employé afin de symboliser les mesures qualitatives que les instruments opèrent, c'est-à-dire que les interactions phénoménales entre l'instrument et l'objet mesuré sont représentées elles-mêmes par des nombres et des points ou trajectoires.  La surprise que décrit Duhem provient de l'oubli que tout instrument de mesure employé est une phénoménotechnique et qu'alors, le cercle du système de représentation de son plus bas à son plus haut niveau est fermé. La phénoménotechnique est une notion que Bachelard présente dans le *Le nouvel esprit scientifique*[^52] en 1934 et qui désigne un système d'objets qui crée radicalement un phénomène: ce phénomène n'aurait jamais pu naître autrement que par l'interaction de l'instrument avec un autre objet. Il en va de même du nombre associé à la mesure: il ne serait pas né sans la situation[^53] de l'instrument et le phénomène qu'il crée. De plus, la mathématisation s'ajoute à la phénoménotechnique des instruments lorsque l'on touche à la physique à partir du XIXe siècle[^54]. La phénoménotechnique se révèle d'ailleurs très facilement dans le concept de l'*artefact*, qui est un phénomène généré indépendamment et à l'insu de la volonté de la personne expérimentatrice par l'instrument et son interaction avec une cause parfois inconnue. Cette cause sera sujette à des recherches ultérieures afin d'être éliminée. Il est possible de considérer dans cette optique que tout phénomène perçu au travers d'un instrument est à l'image d'un artefact désiré.

Cette épistémologie que propose Duhem et qui est celle de la théorie physique fait une claire distinction entre la libre activité logique de la théorie grâce aux mathématiques et les questions de véracité qui sont confinées à l'expérience. L'expérience est le critère de vérité des lois et des théories. Les lois ne sont que des représentations de la réalité instrumentale tirées du système symbolique élaboré par les mathématiques et appliqué à la phénoménotechnique de l'instrument. Les théories ne sont pas strictement réelles mais elles sont restreintes par les normes de la logique empruntée, qui est impliquée jusque dans les instruments.

En général, cette épistémologie s'accompagne d'une attitude antimétaphysique. Duhem ne préconise l'antimétaphysique que durant l'activité de la physique. Il stipule que tout ce que l'on peut connaître du monde en physique n'est que représentations et interactions instrumentales: il n'y a rien en-dessous en physique. Il n'y a rien à connaître au-delà de cette nature créée par l'instrumentation physique.  Poincaré dispose d'un point de vue plus radical: rien ne peut être connu, sinon ce système, en tout temps et dans toute discipline. La position de l'épistémologie de la représentation est également partagée par Mach, et nous verrons à l'analyse philosophique que la théorie de la relativité générale proposée par Albert Einstein possède une épistémologie de même parenté.

#### Pourquoi la théorie de la relativité générale ? La relativité de l'information.  

Source fondamentale de représentation du mouvement et des masses en astrophysique aujourd'hui et élaborée en 1915, la relativité générale est un système mathématique prédictif et représentatif. En tant que ce système de propositions est prédictif, il est attendu par la communauté physique et son épistémologie que les prédictions soient prouvées par un moyen phénoménotechnique. A l'origine de la relativité générale se trouve un ensemble de recherches mathématiques et instrumentales dont Einstein fait le détail dans un livre de sa main[^55]. L'ouvrage est publié et traduit en français vers la fin de sa vie, en 1956. Il a pour but de retracer l'histoire des idées à l'origine de la relativité générale, mais ce n'est pas un livre d'histoire a strictement parler: c'est un livre de vulgarisation qui ne fait pas cas d'une chronologie datée. L'axe principal de l'ouvrage est de montrer les fondements logiques de la relativité générale, quels emprunts elle a réalisé, et les prédictions et quelques expériences qui ont été réalisés et qui peuvent la vérifier. C'est un livre écrit pour convaincre de la justesse de la théorie en l'expliquant. On y trouve peu de calculs à l'exception de quelques équations simples accessibles au *quidam* dont je fais partie. L'autre livre que j'ai lu et auquel je voulais me référer, de Paul Couderc[^56], n'apporte en grande partie pas plus d'explications et même édulcore certains exemples identiques qui deviennent incompréhensibles sans la vue d'ensemble que proposait Einstein dans sa propre vulgarisation. L'intérêt principal de l'ouvrage est de pouvoir comprendre comment la géométrie spéciale de la relativité générale a été conçue du point de vue des idées qui lui sont propres et qu'elle emprunte: ce qu'il est important pour moi d'introduire, afin de comprendre le contexte actuel de l'espace-temps en cosmologie[^57].

Le principe de la relativité générale décrit, je ferai ensuite l'effort de l'expliquer, avant de l'analyser philosophiquement vis-à-vis du contexte épistémologique franco-allemand en physique qui était présent durant la première publication de la théorie (1915) et que l'on retrouve dans les vues de Duhem, Mach, Poincaré que j'ai introduites. Le principe de la relativité générale selon Einstein s'énonce comme suit: Tous les systèmes de coordonnées de Gauss sont en principe équivalents pour la formulation des lois de la nature [^58]. Afin de saisir le sens de cette phrase, j'expliquerai tout d'abord pourquoi selon Einstein la théorie de la relativité qui précédait la sienne, celle fondée sur un espace galiléen, était insuffisante afin de décrire l'ensemble des évènements réels mesurables. Puis, je décrirai les quelques concepts géométriques qu'Einstein a emprunté afin d'achever le but de sa théorie que je pourrais définir comme étant de mathématiser de manière universelle les règles de la nature. Einstein théorise sans respecter la tradition newtonienne, c'est-à-dire en montrant les limites de *l'idée classique de simultanéité*, de l'inertie galiléenne et du concept d'espace euclidien.

#### Coordonnées galiléennes, mouvement uniforme et vitesse infinie de la lumière: l'ère de la simultanéité.  

Au centre des théories physiques et de celle d'Einstein l'on trouve le concept de référentiel . Un référentiel, c'est une métrique construite par rapport à un corps choisi. Une métrique, c'est l'adjonction indéfiniment rapprochée de bâtonnets de même forme, c'est-à-dire de parties égales et identiques. On appelle référentiel galiléen un référentiel construit autour d'un corps en mouvement d'inertie (mouvement conçu par Galilée et Descartes, et repris par Newton également). Un mouvement d'inertie est uniforme et sans influence extérieure: le corps dans un référentiel inertiel conserve sa vitesse (la vitesse du corps ne réduit pas ni ne grandit).

Dans la physique classique newtonienne, la lumière possède une vitesse infinie. Cela signifie qu'elle se propage de manière instantanée, en tout lieu et en tout temps. Une simultanéité absolue pour deux observateurs d'un même évènement est alors possible. La physique dont le but est de découvrir par l'intellect les lois du devenir de la nature, et ce en partie par des concepts mathématiques, peut donc établir que tous les référentiels galiléens disposent des mêmes lois et qu'un évènement a vraiment eu lieu s'il est observé de manière simultanée, c'est-à-dire au même temps et au même endroit par deux observateurs.  C'est en cela que consiste la relativité de Newton-Galilée.

L'idée de relativité implique la comparaison des lois de la nature telles qu'observées par différents observateurs. Elle est liée à la comparabilité de différentes instrumentations et à des modèles représentatifs de la réalité qui sont également comparables. Cela signifie que ces outils doivent posséder un langage qui leur est propre et qui est normé *a minima* afin d'être comparable. Dans ce cas, ce langage est celui que je décrivais comme appartenant à la théorie et à l'expérience physique: les symboles et outils abstraits de la géométrie et des mathématiques. Les lois de la relativité galiléenne sont facilement universalisables, puisque le temps et l'espace prennent un caractère universel au sens où chaque évènement observé simultanément dans un mouvement d'inertie appartient au même concept géométrique de l'espace-temps et au même concept de mesurabilité (information instantanée de la lumière)[^59]. La manière dont la réception de l'information est conçue, et dont la géométrie des référentiels est identique, permet de comparer les situations d'observation (d'effectuer une relativité) et de les rejoindre dans une même loi. Cependant, la découverte de la vitesse limitée de la lumière change la donne: les observations d'un seul évènement sont différées dans deux corps de référence différents.

Là où Einstein intervient avec la relativité restreinte en 1905, c'est en prenant en compte la vitesse limitée de la lumière. Ses recherches se situent à la fin du XIXe siècle, alors que la théorie de l'éther est encore discutée et que les expériences de Michelson (1852 - 1931) ont raté pour mettre en évidence l'effet de l'éther, cependant que la mesure de la vitesse de la lumière par Fizeau (1819 - 1896) a déjà été effectuée[^60].

La vitesse limitée isotrope[^61] de la lumière est négligeable à la plupart des échelles où le temps séparant deux observateurs de la mesure d'un évènement est très court. Mais dans les temps longs[^62], la relativité de Newton-Galilée devient invalide. En effet, la vitesse limitée de la lumière, que l'on indique communément par *c*[^63], implique que deux évènements ne sont comparables qu'à condition de prendre en compte la variabilité de perception de l'évènement en fonction des positions relatives des observateurs entre eux et par rapport à la source observée. Cela signifie qu'il faut prendre en compte la différence de réception d'information entre deux référentiels à partir desquels le même évènement est observé. On appelle transformations de Lorentz les équations permettant de calculer le facteur spatio-temporel en jeu entre deux corps de référence. Aux calcul des valeurs *x*, *y*, *z* et *t* au coeur de la métrique construite autour du corps choisi comme référentiel galiléen (métrique qui est habituellement tridimensionnelle pour l'espace et unidimensionnelle pour le temps), pour chaque observation est ajoutée *c* lorsque c'est nécessaire. La position et la vitesse d'un objet dans un référentiel de la relativité restreinte est donc conditionnée par la vitesse finie de la lumière comme source d'information pour l'observateur d'un évènement.

![](/img/strasbourg1.png)
*Sur la photographie de gauche, les référentiels sont galiléens. Sur celle de droite, ils ne le sont plus.* Crédit photographique: Négatifs sur Agfa APX 100, 2022, Benjamin ULRICH (gauche), Jacqueline JIMÉNEZ (droite).

Prenons un exemple pour rendre la relativité des référentiels par rapport à un évènement plus claire. Situons-nous au marché de Noël de l'hiver 2022 à Strasbourg et regardons les deux photographies ci-dessus.

L'image de gauche et de droite représentent le même endroit: des guirlandes pendant des murs de la rue lors du marché de Noël et la lumière d'une porte de magasin dans le coin gauche bas. Prenons l'appareil photographique qui a réalisé l'image et la source lumineuse des guirlandes et de la porte comme deux référentiels d'observation différents. La proximité des référentiels rend la vitesse de la lumière négligeable. Si nous nous situons dans la première image, les deux référentiels sont galiléens: il n'effectuent aucune trajectoire, ne subissent pas d'accélération et l'ancienne mécanique suffit à décrire l'évènement de l'observation de la source lumineuse pour en rendre compte dans les faits (faits qui sont représentés par la trace instrumentale obtenue grâce à l'appareil photographique).

Mais dans l'image de droite, la source lumineuse semble se mouvoir. Si nous nous situons du point de vue de l'appareil, celui-ci a réalisé une trajectoire irrégulière en arc de cercle sur l'axe de la hauteur (y) en décrivant des accélérations en plusieurs points, sans se déplacer sur les autres axes. Or la représentation galiléenne ne s'occupe que des mouvements uniformes sans accélération et n'est donc pas en mesure de représenter ce qui se passe. Pour adéquatement saisir l'image produite, il faut comparer les référentiels dans lesquels se situent les deux évènements impliqués: le mouvement de l'appareil et le mouvement de la guirlande. Celui de la guirlande demeure uniforme lors de la prise de vue, mais celui de l'appareil est accéléré: d'où le résultat obtenu.  L'image de la source lumineuse n'est donc pas la même en fonction du référentiel à partir duquel elle est observée, et c'est pourquoi il est important de pouvoir représenter tout type de mouvement quand il faut décrire une situation d'observation.

Pour deux observateurs, un évènement n'est simultané que si l'on considère le facteur d'équation des valeurs respectives des deux référentiels en jeu: c'est la seule manière d'universaliser les lois de la nature découvertes dans plusieurs systèmes de référence où l'on accepte la prémisse que la réception de l'information est limitée par une vitesse finie. La vitesse de la lumière n'étant pas infinie, il n'y a plus de référentiel universel, absolu et évident du temps et de l'espace. Le temps et l'espace respectifs d'une observation sont devenus *propres* au référentiel dans lequel l'observateur est situé.

La relativité restreinte n'est cependant pas en mesure de représenter et de comparer géométriquement des observations pour tout référentiel non-galiléen (non-uniforme). La solution à ce problème est l'apport supplémentaire à l'édification d'une géométrisation universelle et relative de la nature qu'offre la relativité générale, publiée en 1915.

La relativité générale permet de représenter un évènement selon différents référentiels de manière apparemment universelle, c'est-à-dire indépendamment du type de référentiel choisi. Elle prend en compte les accélérations du référentiel et les champs de gravitation.

Einstein propose l'exemple suivant: soit un référentiel galiléen située sur un corps échauffé. La métrique à partir du point d'échauffement va être modifiée à mesure que la surface sur laquelle le référentiel est fondé s'étend. De ce fait, la géométrie euclidienne n'est plus en mesure de décrire le système de référence choisi. Il faut disposer d'une géométrie acceptant une variation de l'unité en son sein.

Ce qu'Einstein appelle des mollusques de référence [^64] existe en

![image](/img/Octopus3.jpg) 

astronomie en vertu de l'égalité entre la masse maupertuisienne et la masse pesante. Une masse maupertuisienne est une masse créée par un choc: le corps est accéléré et sa masse grandit. Une masse inerte (celle dont dispose un corps dans un référentiel inertiel) soumise à un champ gravitationnel, par exemple celui de la Terre, va également connaître un accroissement de masse. Ces deux masses sont de fait identiques et la représentation suivante peut être donnée: tout champ gravitationnel engendre une accélération uniforme des corps qui lui sont soumis.  Cependant à l'image de la source thermique, un champ de gravitation n'est pas constant mais décroît à mesure que la distance par rapport à son centre grandit, ce qui implique une variation de l'accélération par rapport à la situation du corps dans le champ de gravitation. L'unité de représentation des corps dans le champ doit donc pouvoir être variable[^65].

Afin de pouvoir comparer adéquatement tout point d'observation à un autre pour en tirer une loi représentant le mouvement des phénomènes faisant cas des différences d'observation, il faut pouvoir se situer non pas seulement dans des référentiels galiléens, mais également dans des référentiels soumis à des accélérations, par exemple de type gravitationnelles. Einstein emprunte pour ce faire à Hermann Minkowski (1864 - 1909) l'idée d'un continuum à quatre dimensions galiléen déjà pour concevoir la théorie de la relativité restreinte [^66], ensuite à Bernhard Riemann (1826 - 1866) une géométrie déjà explorée ne supposant pas la validité de la géométrie euclidienne pour les corps rigides [^67] et à Gauss une méthode pour le traitement mathématique de continua quelconques [^68] pour concevoir la relativité générale. La méthode de Gauss permet de représenter un continuum[^69] d'espace (x, y, z) et de temps (t) de n'importe quelle forme.

Ce que propose la relativité générale c'est donc une représentation de l'espace-temps faisant usage des développements les plus récents de son propre temps en mathématiques et géométrie. Elle propose une représentation en accord avec le comportement de la lumière relativement à sa vitesse et aux champs d'accélération qui ne sont pas représentés par la relativité galiléenne. La relativité générale est une élaboration qui repose sur des recherches antérieures et qui trouve sa confirmation à la fois dans des expériences antérieures à sa conception (telles que l'expérience de Fizeau, la vitesse finie de la lumière étant un des principes de la théorie, et l'expérience négative de Morley et Michelson) et des expériences prédites, comme la solution à la périhélie de Mercure.

L'une des conclusions les plus importantes pour notre étude et les plus fameuses de la relativité générale est qu'E = mc².  Elle est importante puisqu'elle est impliquée dans les problèmes que pose la matière noire. E = mc² signifie que la masse d'un objet est mise en relation avec la célérité de la lumière. Cette formule met en jeu des principes fondamentaux de la relativité générale: celui selon lequel la masse d'un corps puisse être décrite de manière identique indépendanemment du référentiel choisi; l'autre selon lequel la vitesse de la lumière est une limite et source d'information en physique. Or pour les astrophysiciens la matière noire remet en cause ces idées, puisque la masse supposée est supérieure à la masse lumineuse dans de nombreuses galaxies et nombreux amas de galaxies: il y a alors un problème d'information lumineuse ou de calcul de masse vis-à-vis du référentiel d'observation dont les chercheurs disposent.

En remettant en cause les principes fondamentaux d'un paradigme de science, la matière noire est ce que l'on appelle proprement une anomalie. Dans la *Structure des révolutions scientifiques* [^70], Thomas S. Kuhn propose une certaine représentation de l'histoire et du fonctionnement de la science qui serait constituée comme un paradigme: un ensemble de propositions *en principe* indémodables à partir duquel les recherches en son intérieur sont menées. Le paradigme peut grossir, il peut se corriger et s'affiner mais il ne peut pas se contredire lui-même. Ces propositions peuvent être étendues de la théorie abstraite aux instruments concrets. Par exemple, dans le paradigme de l'astrophysique contemporaine, l'on retrouve le télescope et sa phénoménotechnique (que je décrirai plus bas), et la relativité générale, une théorie permettant de représenter l'espace-temps des objets lointains. Je ne dis pas que la relativité générale est prise telle quelle depuis sa conception en 1915, ni que les idées embryonnaires proposées sur la forme générale de l'univers (est-ce une espèce de sphère tridimensionnelle ?[^71] ont été conservées. En revanche les principes de la relativité générale sont toujours admis.

Une anomalie, c'est selon Kuhn, un problème de recherche qui ne trouve pas sa solution en empruntant la voie royale des principes du paradigme.  C'est un problème qui fait se questionner la communauté partageant le paradigme sur la correction et la véracité de ses principes. Si l'anomalie engendre un nombre suffisant de dévoiements du paradigme, elle crée une phase de chaos qui entame la chute imminente d'un paradigme et précède l'apparition du suivant. Durant cette phase de chaos, de nombreuses hypothèses qui vont à l'encontre du paradigme précédemment accepté sont émises afin de pouvoir expliquer l'anomalie tout en devant prédire un certain nombre de phénomènes déjà connus. La théorie de la relativité générale est un changement de paradigme vis-à-vis de la théorie newtonienne de l'espace et du temps: elle le corrige, elle change de principes, et permet de représenter les mêmes phénomènes et d'en prédire un nombre supérieur. Ce que je me demande alors, c'est: à quel point la matière noire est-elle à même d'engendrer un changement de paradigme ? La phase de chaos prévue par Kuhn a-t-elle déjà été entamée ?

Le statut actuel de l'épistémologie employée vis-à-vis de la matière noire prend en compte la relativité générale. Cette anomalie de paradigme remet en question les fondements de la relativité. Il est *a priori* avéré que la matière noire n'est pas un artefact instrumental.  Se pourrait-il que cela soit un artefact de la théorie, c'est-à-dire, une erreur imprévue, une équation fausse ?

La relativité générale est une géométrisation de l'espace-temps qui intègre l'idée de phénoménotechnique en situant toujours l'observateur et son instrument par rapport à l'objet observé et à des observateurs relatifs. Ce qu'il faut remarquer cependant, c'est que: 1° l'instrument employé est conçu et ses données sont perçues comme faisant partie intégrante de la relativité générale; 2° qu'en tant que système soumis à une logique hypothéticodéductive[^72], elle tombe nécessairement dans une faille similaire au second théorème d'incomplétude de Gödel: ses fondements admis comme véritables, elle ne peut les contredire pour demeurer prédictive et maintenir les faits qui ont déjà établis en les pensant avec elle. Tout le problème d'un paradigme est de ne pas pouvoir s'échapper de lui-même sans se détruire.

Le but de cette section était de montrer sur quel type d'idées repose la relativité générale afin de n'être pas pris dans son paradigme. J'ai montré que c'était une théorie appartenant à une tradition située et qu'il n'y a pas de raison de ne pas concevoir l'espace-temps d'une autre manière. La seule raison du maintient d'un paradigme tient en général à la valorisation de ses découvertes par la communauté partageant la connaissance de ce paradigme. La relativité générale est donc une théorie dont l'épistémologie et les prémisses possèdent une origine historique et sociale. Il est possible de considérer que l'astrophysique dispose d'un paradigme elle aussi, dont la relativité générale fait partie. Mais la contextualisation que je propose est générale et l'astrophysique n'est pas qu'une histoire d'idées, c'est aussi une histoire de techniques. Dans la section suivante, j'aimerais montrer ces deux envergures en me concentrant sur l'hypothèse de la matière noire plus précisément.

## Histoire des techniques et des idées sur la matière noire.

Composer une histoire des techniques et des idées sur la matière noire est utile afin de saisir dans quel contexte se situe cette notion de recherche aujourd'hui. Une telle situation historique précède mon étude d'anthropologie de laboratoire, ce qui permet de comprendre à partir de quelle histoire ce laboratoire travaille: une chose nécessaire afin de saisir quels sont les prémisses à l'origine des méthodes de recherche qui sont employées.

L'histoire de l'astronomie jusqu'à la Renaissance est très vaste, mais les développements de l'astronomie amérindienne, chinoise, ou maya, ne semblent pas avoir eu autant d'influence sur la conception européenne de l'objet qui doit être celui de l'astronomie que l'avancée des instruments d'observation à partir du XVIIe siècle[^73]. Comme le montre Yaël Nazé[^74], ce n'est pas Galilée qui a inventé la lunette, ni Newton qui a inventé le télescope. Mais c'est au XVIIe et XVIIIe siècle que l'emploi de tels instruments pour l'observation du ciel commence à se développer. Les défauts primaires de ces instruments furent leurs aberrations (pour la lunette) et la difficulté de les entretenir et de les manier (pour les télescopes), ils connurent chacun leurs heures de gloire contrebalançant les défauts de l'autre. Les astronomes actuels n'utilisent plus que des télescopes.

A partir du XXe siècle, les techniques de perfection des télescopes sont de plus en plus abouties et la lunette est désuète. Les télescopes sont plus légers, sujets à moins d'aberrations et plus faciles à produire que les lourdes lunettes ; leur matière aluminisée tient mieux les aléas du temps et l'entretien est plus facile qu'au XVIIe siècle ; leur grandeur leur octroie une plus grande surface illuminée ; par le biais de l'interférométrie, ils peuvent grandir monstrueusement pour les larges fréquences ; les techniques de photographies se développent et le CCD remplaçant le film plan argentique, combiné à l'ordinateur, permettent l'amassement indéfini[^75] de données et l'amélioration de l'efficacité en terme de recueillement des photons [^76]. Ces données amassées sous forme informatique peuvent ensuite être traitées par des ordinateurs forts puissants. J'ai montré que le paradigme actuel de l'astrophysique a été constitué par une certaine continuité d'idées qui prend son origine dans la renaissance du platonisme à l'ère de l'humanisme. J'ai mentionné, d'autre part, que les instruments ont joué un rôle dans la formation de ce paradigme. Je pense de surcroît qu'il existe des anomalies passées similaires à la matière noire qui à force de se répéter, ont introduit une méthodologie de propositions d'hypothèse qui est aujourd'hui reproduite.

### Des cas paradigmatiques de la recherche en astrophysique et en astronomie.  

#### Le Verrier et Neptune.

Le paradigme actuel de la cosmologie dispose d'une méthode de recherche qui remonte pour une part au XIXe siècle, notamment par rapport à la matière noire. C'est la méthode de l'hypothéticodéduction: de certaines prémisses admises comme probables, on conclue un objet qui devrait exister. A cela s'ajoute l'expérimentalisme selon lequel c'est l'observation du-dit objet qui devrait confirmer son existence: l'astronomie n'étant plus un exercice purement théorique. Françoise Combes et David Elbaz, deux auteurs qui ont écrit sur l'histoire de la matière noire, mettent chacun en avant une  préhistoire de la matière noire [^77] ^,^ [^78] constituée par la découverte de Neptune en 1845.  Une analogie se fait entre Uranus et la matière noire. La trajectoire inhabituelle de la planète pouvait en effet signifier deux choses : soit les lois de Newton n'étaient plus valides aux abords des extrémités du système solaire (ainsi qu'en doutait John Adams (1819 - 1892)[^79]), soit un astre déviait la trajectoire de la planète. Urbain Le Verrier (1811 - 1877) prédit cet astre et il fut observé très peu de temps après par Johann Galle (1812 - 1910), un astronome allemand.

L'analogie entre ces deux cas de recherche se situe au niveau du fait qu'un évènement invisible mais prédit ait pu montrer sa trace par un effet second, dans ce cas la perturbation de la trajectoire d'une planète. De surcroît, l'invisibilité d'un évènement permet de remettre en cause la validité du paradigme: il est tout à fait possible que la prédiction n'ait pas lieu d'être. En effet, cette invisibilité pourrait tout aussi bien être réelle (en tant que défaut d'observation) comme elle pourrait être un leurre (comme défaut de la théorie : il n'y a rien à voir, ce n'est qu'un évènement mal prédit et erroné). Dans de tels cas, il est possible de faire le double travail de remettre en cause la validité d'un paradigme qui tente de se sauver par un argument paradigmatique. La matière noire se situe dans ce même problème d'hypothéticodéduction permettant également la remise en cause d'un paradigme.

Ce genre de cas d'invisibilité, qui était celui de Le Verrier, et qui fut discuté en son propre temps par rapport au paradigme newtonien, est révolutionnaire[^80] et c'est ce qu'il a en commun avec la matière noire : elle ne peut être observée, mais il est prédictible qu'elle puisse exister et qu'elle puisse être détectée. Simultanément, il est possible d'imaginer une solution alternative qui consiste à corriger un principe du paradigme qui serait faux et aurait mené à une prédiction erronée. Cette correction implique une conséquence ontologique en ce qu'elle nullifie la possibilité de prédire le corps propre à l'hypothèse paradigmatique. Le dilemme peut donc entamer à lui seul un chemin vers la révolution de paradigme puisque cette manière de douter ne rend plus nécessaire de prédire selon les mêmes principes de réalité naïfs qui sont remis en cause.

#### Les galaxies sont-elles en récession ? Fritz Zwicky et Hubble.  

Une question similaire ayant émergé dans les années 1920 a défini d'une manière essentielle le paradigme de la cosmologie adopté par après pour la compréhension de l'information lumineuse. En effet, Edmund Hubble (1889 - 1953) publie un article en 1929 qui mentionne le *redshift* des galaxies, bien qu'il ne soit pas le premier à les étudier. En 1912 Vesto Slipher (1875 - 1969) commence à cataloguer les vitesses radiales de galaxies éloignées (c'est Hubble qui les qualifie de island universes ) et Carl Wirtz propose la valeur *K* afin de soustraire au moins la vélocité de notre propre galaxie à l'étude de ces îles-nébuleuses. C'est en s'associant avec Milton Humason (1891 - 1972), qui a de bonnes capacités d'observation, que Hubble produit un ensemble de vélocités radiales associées indépendamment à une distance possible[^81]. Le *redshift* est une tendance des galaxies à rougir à mesure qu'elles sont plus loin de l'observateur. Deux interprétations de l'effet ont été données.

L'une, apportée par Hubble, interprète l'effet de rougissement comme le signe que les galaxies sont en récession, c'est-à-dire qu'elles se déplacent dans un mouvement de fuite par rapport à l'observateur. La lumière qu'elles émettent subit alors un effet similaire à l'effet Doppler, qui est une dilatation de l'onde à mesure que la source de l'onde s'éloigne. Cet effet produit le rougeoiement, puisque le rouge possède une fréquence plus lente que le bleu dans le spectre électromagnétique et que l'onde est dilatée. L'autre interprétation, donnée par Fritz Zwicky (1898 - 1974), propose que la lumière perde de son énergie en fonction de la distance qu'elle parcourt. C'est une solution qui fait une analogie avec l'effet Compton[^82]. On l'appelle hypothèse de la lumière fatiguée [^83]. La première hypothèse se situe alors dans un univers en expansion. L'autre hypothèse conserve un univers fixiste, théorie qui prévalait à cette époque. De nombreuses figures majeures de la physique et de l'astrophysique s'étaient engagées dans le débat.

L'hypothèse qui prévaut aujourd'hui est celle de Hubble. Pourtant, dans l'optique d'un univers fixiste, l'hypothèse de Zwicky était la plus prudente, au sens où elle modifiait le moins le paradigme en place. Mais elle ne pouvait pas être corroborée car elle impliquait des effets *ad hoc* improbables (un très grand âge de l'univers et une relation avec la distribution de matière), et parce qu'elle a été réfutée expérimentalement[^84]. C'est ultimement l'hypothèse de la récession des galaxies qui prend le dessus à mesure que les hypothèses de la lumière fatiguée montrent la présence de conséquences *ad hoc* improbables.  Pourtant, pour accepter l'idée de Hubble, il fallait changer de paradigme cosmologique et accepter le principe que l'univers entier était en mouvement d'expansion. À cette hypothèse se sont ralliés les astronomes après les années 1940[^85], du fait qu'elle était bien plus simple à condition d'en accepter les prémisses. L'hypothèse de Zwicky était plus prudente si l'on conservait le paradigme fixiste, mais trop improbable. Il a donc *fallu changer de paradigme pour expliquer l'effet*.

Je pourrai comparer les interprétations de l'anomalie de la matière noire à cette histoire. Elle est importante car elle a défini également une part du paradigme actuel, qui est de rendre compte du *redshift* des galaxies par une théorie de l'expansion de l'univers. L'univers a été mis en mouvement pour résoudre cette anomalie.

#### L'Éther.  

Bien que l'éther soit une hypothèse qui ne trouve pas nécessairement sa place en astronomie précisément (c'est une hypothèse qui concerne toute la physique en général), en revanche il offre une analogie intéressante par rapport à l'idée d'hypothèse *ad hoc* et superflue. Je ne vais pas m'étendre sur l'explication de son histoire qui est d'une complexité sidérante, tant il fut hypothétisé, modifié, mathématisé et conservé au travers de théories conflictuelles et successives[^86].

Je simplifie: il est considéré par Maxwell et par Faraday comme un milieu indéfini dans lequel doit se propager un champ, car sinon le champ serait sans substance de propagation. L'expérience de Michelson et de Morley finit de détruire l'hypothèse de l'éther lorsqu'Einstein explique que ce milieu est tout à fait imperceptible par rapport au référentiel de la Terre et qu'il est donc superflu d'en supposer l'existence. Un champ constitue donc son propre support de propagation, ou plutôt, il n'y a pas besoin de supposer la présence d'un support.

On en tire de là l'idée qu'une hypothèse *ad hoc* est toujours contingente et que sa nécessité ne peut pas être prouvée sans l'expérience. Les particules de matière noire WIMPs, par rapport à la simple descriptivité de MOND qui est une modification des lois du mouvement rendant compte de l'anomalie de masse, sont donc des hypothèses *ad hoc* certes heuristiques, mais non nécessaires[^87]. A l'issue de l'anomalie, il pourrait se produire la même chose qu'avec l'éther: l'hypothèse *ad hoc* qui est moins descriptive et plus explicative pourrait disparaître (à ceci près que c'est une particule qui est *ad hoc*, et non pas un milieu universel et diaphane).

### L'organisation et la conformation des télescopes au XXème siècle.  

#### La course au plus grand miroir.

Le paradigme de recherche sur la matière noire est qualifié par des idées théoriques sur l'espace et le temps et par certaines expériences exemplaires, tout autant que par un instrumentalisme fort.

La fin du XIXe siècle est caractérisée par des découvertes astronomiques permises notamment par une course au plus grand télescope: l'agrandissement du miroir permet un gain de lumière et de résolution.  Grâce à cela, les astres plus éloignés sont plus lumineux. Outre le but utile de posséder de meilleurs moyens d'observation, c'est aussi l'ambition de concurrence nationales (au cours des ans, c'est ainsi les États-Unis, un pays européen ou la Russie qui détient le plus grand télescope du monde\...). Une raison qui pouvait encore être valable en début siècle était la gloire personnelle de richissimes personnages nourrissant leur passion: c'est le cas de George Ellery Hale, qui construisit de très larges télescopes au Mont Palomar et au Mont Wilson.  En revanche, l'institutionnalisation de l'astronomie et l'internationalisation des financements changea la manière de s'organiser pour la construction et l'utilisation de télescopes, ce qui devient clair avec les très larges édifices construits à partir des années 1970-80, sur des monts spécifiques avec une météorologie peu perturbée (voir par exemple le NTT et le VLT) et à partir d'association de laboratoires internationaux.

Afin que les miroirs ne ploient pas sous leur propre poids, des supports correctifs élaborés et motorisés sont inventés permettant de légèrement déformer le miroir à l'endroit où le poids se fait sentir. De même, les aléas des vents et des différences de températures induites, qui déforment le métal, peuvent être contrecarrés par un miroir secondaire qui dévie adéquatement la trajectoire de la lumière vers le miroir principal.

Le XXe siècle connaît un perfectionnement des techniques du miroir qui permet son élargissement jusqu'à la dizaine de mètres pour l'optique, et plusieurs centaines pour l'onde radio. Des tentatives infructueuses au siècle précédent (morcellement du miroir pour alléger son poids, miroir géant, support à pieds adaptatifs) sont réalisées et répétées à travers le monde.

#### Plaques et CCD 

Outre l'agrandissement du miroir et l'abandon complet de la lunette, c'est la photographie qui révolutionne aussi l'utilisation du télescope.  Tout d'abord sur plaques, elle permet de n'avoir plus à se fier à l'oeil, à la prise de notes et au dessin lors de longues nuits d'observation. Elle permet de glaner plus de lumière que ne peut l'iris, et d'observer des détails à tête reposée. Surtout, elle permet de mettre en place des études systématiques et comparées, puisque la vue enregistrée est exacte[^88] et transmissible. Très tôt, des catalogues et cartes du ciel sont envisagées[^89] et l'élaboration de la spectroscopie est concomitante de l'utilisation des plaques photosensibles et de la naissance de l'astrophysique. En effet, la spectroscopie est une technique qui consiste à décomposer le spectre d'un rayon lumineux, et celui-ci peut être reçu sur une plaque. Il est alors possible d'observer systématiquement le spectre de certaines étoiles, notamment celui du Soleil qui est très proche et très lumineux.  Des photographies d'éclipses du soleil sont réalisées durant un siècle particulièrement favorable à leur apparition, dans le but d'étudier la chromosphère de l'astre. Avec la photographie et le développement de la spectroscopie, l'appareil du télescope fait entrer l'astronomie dans l'ère de l'astrophysique: la spectroscopie astronomique devient comparable à la spectroscopie de laboratoire qui se développe au cours du XIXe et qui permet d'identifier un élément chimique unique à une raie unique. D'abord confondant une raie du Soleil avec celle du Sodium, on s'aperçoit qu'en la décomposant suffisamment, celle-ci est composée de raies subsidiaires dont une est particulièrement orangée. Elle est constituée d'un élément chimique alors peu connu, que l'on décide d'appeler hélium (du grec Helios, le dieu du Soleil). Cette découverte est essentielle puisque le ciel est riche en hélium, et que c'est l'un des composants fondamentaux de la théorie du Big Bang.

Le CCD (Charge-Coupled Device) est un assemblage quadrillé de cellules photosensibles produisant un effet photoélectrique: à la réception d'un photon sur la matière, un électron est émis. Le signal électrique analogique reçu est ensuite converti en signal numérique, qui peut être enregistré et traité informatiquement. D'abord développé pour l'usage de la télévision (à partir de tubes photoélectriques faisant penser aux tubes cathodiques, qui d'ailleurs remplissent les télévisions et moniteurs avant l'utilisation du LCD), le CCD entre en utilisation en astronomie durant les années 80. Son efficience meilleure que celle des plaques (il enregistre plus de photons) le rend populaire et aujourd'hui la plaque photographique est abandonnée au profit des capteurs électroniques.

#### Financement et organisations internationales.  

Avec l'avènement de la photographie et la multiplication des observatoires, au XIXe siècle commencent à émerger des organisations internationales de la recherche afin de combler des projets ambitieux tels que des cartes du ciel. Ce genre d'organisation permet également l'étude combinée d'un seul phénomène depuis plusieurs observatoires.

L'organisation de la recherche contribue grandement à faire de l'astronomie une science empirique systématisée en permettant la publication partagée et lue de résultats, et en incitant les relations épistolaires et internationales. En revanche, elle joue en astrophysique un rôle particulier au sens où elle devint, vers les années 1960, un outil de financement majeur. Le coût d'un télescope est énorme mais l'astronomie repose également sur un système économique particulier à retombées technologique qui en fait un investissement intéressant: les techniques développées pour parfaire les télescopes peuvent être dérivées dans d'autres domaines. D'autre part, les alliances internationales peuvent répondre à des enjeux politiques, soit de concurrence soit de diplomatie (par exemple, l'ESO regroupe de nombreux pays européens autour de projets précis). A Strasbourg, l'OBAS (l'Observatoire Astronomique de Strasbourg) joue le rôle particulier d'héberger des données astronomiques à l'échelle mondiale au CDS (Centre de Données astronomiques de Strasbourg). *In fine*, la conversion des observations grâce au capteur CCD en un ensemble de données entreposables et partageables (via l'accès à de large serveurs), permet un total décentrement de l'activité de l'astrophysicien qui n'a pas besoin d'être sur place pour contrôler son télescope, ni d'être assis derrière l'oculaire. Le télescope devient un outil distant dont l'utilisation est partagée en temps d'exposition accordés à plusieurs laboratoires pour leurs projets individuels ou regroupés.

Comme nous le verrons dans le prochain chapitre, le laboratoire d'astrophysique que nous connaissons aujourd'hui ne ressemble en rien à la lunette et son essayeur qui a existé à partir du XVIIIe siècle et qui s'est développé en course de riches individus à partir du siècle suivant. Si le principe du télescope est resté le même, sa mise en place, son appareillage et son utilisation ont grandement changé: le lieu d'observation est optimal ; il est partagé entre plusieurs groupes et financé par plusieurs nations ; on accède à ses productions à distance et on les traite dans un lieu éloigné par ordinateur.

#### Démultiplication du spectre observé.

Le XXe siècle est celui de la démultiplication du spectre observé par les télescopes. En effet, grâce à la découverte de Penzias et Wilson du résidu radioélectrique provenant du fonds diffus cosmologique, des antennes ont commencé à être construites pour observer l'univers. On découvre alors les quasars et les pulsars. Au cours du siècle, des télescopes sont conçus pour tous les champs du spectre, des infimes rayons gammas aux grandes ondes radioélectriques. L'atmosphère terrestre bloquant naturellement une grande partie du spectre (rayons gamma, X, ultraviolet, une part de l'infrarouge), certains télescopes sont placés dans des régions privilégiées (par exemple l'antarctique) ou plutôt, dans l'espace, où aucune atmosphère n'obstrue le chemin des ondes.

L'une des prémisses les plus intéressantes de la matière noire est en général de ne posséder aucune interaction lumineuse, et donc, en apparence, de ne pas pouvoir être présente dans le spectre connu. Le problème est alors de mise: l'astrophysicien, qui est toujours dépendant d'un chemin lumineux dévié par le télescope, ne semble pas pouvoir accéder à une connaissance invisible. Il n'y aurait pas de chemin lumineux jusqu'aux outils de l'astrophysicien pour la matière noire.

### Histoire des idées concernant la matière noire.  #### La matière noire\... visible.

A l'origine de l'hypothèse de la matière noire actuelle, qui concerne une matière invisible *par principe*, se trouve un autre type de matière noire, qui lui était supposément détectable. Il concernait plus ou moins tout type de corps peu lumineux que l'on supposait être présents mais demeurer cachés (parfois en faisant obstruction au chemin de la lumière). Ainsi, les poussières d'étoiles sont des matières noires connues en début de siècle, puis les MACHOs (*massive compact halo objects*) sont des corps de matière noire qui n'émettent ou ne réfléchissent que très peu de lumière, mais ils ne doivent pas être confondus avec l'hypothèse telle qu'elle se présente aujourd'hui.

En effet l'hypothèse d'une matière noire corpusculaire (qui ne serait pas située à l'échelle d'une particule exotique) a été explorée mais sans succès. Ainsi Combes rapporte que l'on a cherché dans les années 1990 des objets compacts et peu lumineux (étoiles naines brunes, trous noirs) que l'on appelle communément MACHOs (*massive compact halo objects*). Deux équipes indépendantes s'y attelèrent mais l'hypothèse finit par montrer son insuffisance[^90]. Les hypothèses actuelles se concentrent sur la recherche de particules dites WIMP , ce qui signifie  *weakly interactive massive particles* . Ce sont des particules qui interagissent très peu avec la matière baryonique et qui ne rayonnent donc pas d'une manière perceptible mais qui possèdent une masse suffisante pour avoir une influence dans le champ gravitationnel des corps. Les nombreuses solutions possibles pour ce type de particules imposent de nombreuses hypothèses à tester. L'hypothèse peut être qualifiée d'*ad hoc*, puisqu'elle prend pour prémisses les anomalies remarquées (influence gravitationnelle, absence de rayonnement).

#### Fritz Zwicky à Caltech en 1931.  

L'histoire de l'anomalie de la matière noire telle que nous la connaissons commence plus précisément avec Fritz Zwicky aux États-Unis à Caltech en 1931 qui met en lumière des objets qui conserveront leur anomalie. Sur idée de Robert Millikan (1868 - 1953), il s'intéresse aux rayons cosmiques[^91]. Afin d'expliquer la grande énergie de ces rayons, Zwicky suppose qu'ils proviennent d'une  super-novae , une étoile mourante délivrant beaucoup d'énergie. Afin d'avoir une chance d'observer une super-novae, Zwicky choisit une région du ciel fort peuplée, l'amas de Bérénice (*Coma Berenices*). Il utilise pour ce faire un télescope de très grande ouverture : un 8-inch ouvert à *f*1,0[^92].  C'est un télescope très lumineux. Il s'aperçoit que la masse nécessaire à la vitesse de rotation des galaxies mesurée par spectroscopie dépasse la masse totale des galaxies visible par le télescope. Dans un article de 1933 Zwicky suppose alors que la présence d'une matière sombre (*dunkle Materie* , [^93]) explique cette vitesse. Il décrit à nouveau ce phénomène dans un article de 1937[^94].

Zwicky n'est pas le seul astrophysicien de son époque à remarquer des problèmes de divergence entre masse visible mesurée à partir des régions lumineuses et masse calculée par la vitesse mesurée de ces régions.  C'est aussi le cas pour Sinclair Smith (1899 - 1938) en 1936 dans l'amas de Virgo, et Horace Babcock (1912 - 2003) vis-à-vis de la galaxie d'Andromède, enfin le premier, Jan Oort (1900 - 1992) quant à  l'équilibre vertical du plan de notre galaxie, au voisinage du Soleil [^95]. Selon Elbaz, Oort avait déjà utilisé le terme de  matière noire pour  décrire les étoiles plus faibles que la limite de détection de ses images dans le proche voisinage du Soleil [^96].

A ce moment, le terme de matière noire désigne donc la présence d'une masse qu'il est difficile d'observer: c'est une matière sombre. Du moins, elle n'est pas présente dans le spectre optique. Mais l'idée d'observer l'univers dans un spectre plus large (gamma, X et radio) n'émerge que bien plus tard, grâce à la découverte du fonds micro-onde de l'univers et à  la découverte fortuite, dans les années 60, de l'émission en rayons X du gaz chaud dans les amas de galaxie proches (Virgo, Coma) [^97]. Françoise Combes présente ces découvertes fortuites (le fonds micro-onde n'aurait pas non plus été attendu) comme des erreurs d'anthropomorphisme : nous n'aurions essayé d'apercevoir l'univers qu'à notre échelle optique. Elle ajoute qu'aujourd'hui  nous devons à nouveau nous défaire de notre anthropomorphisme [^98]. Ce n'est pas tout à fait exact puisque Penzias et Wilson ont contacté un astrophysicien pour identifier le bruit qu'ils percevaient, se situant à environ 5 kelvin, et que l'astrophysicien, promoteur du Big Bang, avait théorisé et cherchait justement[^99]. De plus Penzias est diplomé en radioastronomie à Caltech avant de travailler chez Bell lors de sa découverte avec Wilson et il était déjà attendu en 1948 que l'univers rayonne autour de 5 kelvins lors de son époque primordiale (celle qui suit le Big Bang). La découverte de Penzias et Wilson se situe à 3,5 kelvins[^100].

Dans les années 1960 et surtout à partir des années 1980, la radioastronomie se développe et l'article que Vera Rubin publie en 1983[^101] présente une bonne somme des connaissances et technologies alors disponibles pour étudier des cas de  matière invisible . Cet article constitue un ensemble de méthode convaincantes qui permettent de montrer qu'une anomalie existe. Les méthodes présentées sont très similaires à celles employées aujourd'hui, notamment l'informatisation de l'astrophysique et la multi-instrumentalité[^102].

#### La somme de Vera Rubin: modélisation et multi-instrumentalité.

Vera Rubin est une astronome qui écrit en 1983 un article intitulé Dark Matter in Spiral Galaxies qui paraît dans la prestigieuse revue *Scientific American*[^103]. L'article et l'autrice ont fait date dans l'histoire de l'astrophysique pour des raisons qui se montrent à la lecture: l'article possède un aspect exhaustif et il est d'une grande clarté en faisant l'effort d'expliquer certaines notions évidentes pour les astrophysiciens, mais difficiles pour qui ne serait pas du domaine.  Il touche donc un public large en étant très précis et en douze page donne le sentiment que rien de plus n'aurait pu être dit. Cet effet d'exhaustivité que produit Rubin provient de la richesse de ses sources d'argumentations: elle compare trois types d'observation, elle mentionne l'histoire passée sur laquelle elle s'érige et qui soutient ses propos, elle produit un grand nombre de graphs et de photographies qui convainquent (les graphs car ils représentent ce qui est mesuré ou théorisé, les photographies car elles montrent les objets astrophysiques principaux dont elle parle)[^104], elle fait état de la recherche actuelle et de celle qui peut être à venir, elle rappelle régulièrement l'avancée de l'argumentation et les conclusions tirées, enfin elle montre qu'elle a à sa disposition un large réseau de communication qui peut étayer ses arguments.

Dans cet article, les deux arguments qui m'intéressent le plus sont ceux de la multi-instrumentalité et de la référence à la notion de *modèle*.  Rubin propose plusieurs manières de faire des observations qui mettent en évidence le problème de la matière noire: elle compare les vitesses radiales de plusieurs types de galaxies semblables, en décrivant systématiquement sa méthode d'observation ; elle fait également usage d'une galaxie particulière[^105], dont les indices peuvent être importants ; elle fait part de l'étude falsifiante de plusieurs spectres[^106]. Elle complète cette argumentation en mentionnant quel type de modèle théorique guide ses observations (c'est le modèle du halo de matière noire [^107]). La multi-instrumentalité dont elle fait usage est un type d'argument qui met en avant l'existence d'un objet sous plusieurs formes d'apparences. Une phénoméniste convaincu, par exemple un duhémien, nous dirait que certes le fait que nous multipliions les raisons de croire que l'objet existe, car nous multiplions les manières de le voir et que chacune de ses images se ressemble selon une certaine approximation, ne nous octroie pas le droit de croire que l'objet existe *indépendamment* de ces images. En effet l'erreur de raisonnement constituant à postuler l'existence d'une chose en soi est déjà ancienne indépendamment de son moyen d'observation ou de conception est déjà ancienne, c'est un des paralogismes de la *Critique de la pure raison* de Kant[^108]. Un bachelardien rajouterait que toute observation est conditionnée par une phénoménotechnique, ce qui implique des théories pour expliquer ce qui est vu: Rubin décrit à la fois la théorie explicative qu'elle emploie, et la théorie de son instrument d'observation principal, un télescope et un spectrographe qui enregistrent et augmentent par le moyen d'un tube imageant la lumière perçue sur des plaques Kodak IIIa-J hypersensibilisées[^109]. Quand elle mentionne des observations supplémentaires, elle réalise à nouveau des descriptions méthodologiques et si la mention est brève, le nom des auteurs de l'observation à qui se référer est donné. Elle est donc entièrement formée à montrer comment elle mobilise plusieurs spectres et comment elle a réalisé ses acquisitions pour argumenter en faveur de son hypothèse (selon laquelle la matière noire est invisible et qu'elle est un problème dans les galaxies en spirales).

A la multi-instrumentalité dont Rubin fait preuve s'ajoute l'idée de contrainte: il est possible d'épuiser les moyens de tester une hypothèse. Dans le cas présent, l'hypothèse selon laquelle la matière noire rayonnerait peut-être dans une autre fréquence que celle de l'optique est invalidée pour l'infrarouge, les ondes radios et les rayons X, du moins à partir des tentatives qui ont été réalisées jusqu'en 1983. Cette invalidation réduit les possibilités de corps que la matière noire pourrait prendre dans le paradigme de l'astrophysique: selon Rubin, cela ne laisse que l'option d'un certain type de matière noire froide. Aujourd'hui ce n'est pas tout à fait exact, puisqu'on laisse aussi de la place à de la matière noire tiède. En revanche, l'hypothèse MACHOs est également épuisée. A ce jour, il ne reste que WIMP et MOND.

D'autre part, et c'est son second argument, ces observations peuvent être décrites grâce à des *modèles*. Nous verrons dans le second chapitre que cette notion est centrale dans l'épistémologie de la matière noire. Dans le cas présent, le modèle qualifie l'idée selon laquelle un halo de matière noire recouvrerait les disques de galaxie, ce qui expliquerait que la vitesse aux extrémités de l'objet ne décroit pas en accord avec la perte de luminosité[^110].

La méthode que Rubin emploie peut être considérée comme paradigmatique.  Face à un problème qui nécessite un type de raisonnement abductif[^111] Rubin emploie une méthode qui consiste à épuiser les possibilités d'observation dont elle dispose. Cette méthode est permise grâce à la multiplication des spectres observables pour un seul et même objet dans le ciel. D'autre part, son multi-instrumentalisme est combiné à une modélisation qui permet d'interpréter ce qui est perçu. Le but de cette interprétation est de tirer une loi probable: le modèle a donc par essence un caractère abductif, c'est un outil de recherche heuristique.  Le but de cette méthode instrumentale modélisante est de restreindre les possibilités d'être qui pourraient qualifier la matière noire. Cette restriction participe d'un paradigme général que Rubin n'oublie pas de remettre en cause: elle fait mention de MOND, une théorie encore jeune que Milgrom proposait alors, mais considère la théorie comme surprenante et se concentre plutôt sur ses recherches paradigmatiques.

J'ai décrit jusqu'ici un ensemble de manières de voir le monde théoriques, instrumentales, exemplaires et méthodologiques qui dirigent l'analyse astrophysique. J'ai remarqué que cette science possède un système de représentation mathématique des phénomènes, qui présuppose que les phénomènes doivent être expliqués afin d'être compris.  Simplement les voir serait insuffisant pour leur donner un sens. Dans cette optique de représentation mathématique qui confère un sens aux observations, cette science est nécessairement instrumentaliste. Il est nécessaire de développer et d'améliorer les instruments pour affiner la théorie. Ce contexte représentationniste-instrumentaliste a mené à l'apparition du problème de la matière noire à partir de recherches entamées au début du XXe siècle sur divers objets massifs du ciel.  La méthode la plus récente dont j'ai connaissance dans cette étude rétrospective est décrite par Vera Rubin. Elle réemploie *grosso modo* les mêmes principes épistémiques que j'ai pu décrire auparavant, cependant l'avènement de l'ordinateur commence à jouer un rôle: je ne sais pas si elle informatise ses modèles (ce n'est pas précisé dans son article), en revanche elle l'utilise déjà pour tracer des graphs . Dans l'étude d'anthropologie que j'ai effectué et que je vais décrire, j'ai voulu trouver les conditions contemporaines d'étude du problème de la matière noire qui se surajoutent au paradigme que j'ai partiellement décrit. Je pense que l'informatisation par ordinateur est centrale dans cette nouvelle méthodologie de la recherche et que l'idée de modèle informatisé est devenue systématique.

# Anthropologie de laboratoire: visite, observation et entretiens à l'OBAS.

Du 25 avril 2022 au 06 mai 2022, j'ai eu l'occasion de rester deux semaines à l'Observatoire Astronomique de Strasbourg. Sur place, j'ai réalisé 6 entretiens enregistrés, j'ai eu 5 autres entretiens dont j'ai pris note, enfin j'ai pu observer comment les astrophysiciens et astronomes travaillent. Je suis revenu de cette visite avec beaucoup de notes et quelques documents audios et visuels[^112]. Il était important pour moi de venir sur place car la position du philosophe est souvent erronée et simpliste s'il reste dehors la communauté scientifique qu'il désire étudier. Je ne voulais pas être un de ces *armchair philosophers* , des philosophes de bureau auxquels l'on peut reprocher que la distance locale par rapport à leur sujet d'étude représente une extension égale de leurs chances de se tromper dans leur analyse en n'ayant pas de connaissance rapprochée de leur sujet. J'ai donc saisi l'occasion d'aller voir ce qui se passe en pratique.

Je pense qu'un paradigme de recherche inclut des connaissances tacites.  Michael Polanyi[^113] a développé complètement cette idée et Michael Lynch a constitué une méthode pour les mettre à jour, l'ethnométhodologie. Cette méthode n'est pas la mienne, mais en venant sur place j'ai pu avoir connaissance de nombreuses notions qui ne me sont tout simplement pas accessibles dans les écrits des astrophysiciens ou sur l'astrophysique, soit qu'elles ne s'y trouvent pas, soit qu'elles ne me sont pas accessibles par manque de connaissances mathématiques et physiques. Le caractère caché de ces notions, partagées dans une communauté close, en fait la tacité, et elle ne peut m'être découverte que par une étude directe avec les acteurs possédant la notion.

L'activité scientifique n'est pas que pure théorie: c'est aussi un réseau local de discussion, de partage de pratiques et d'idées. Il y a un ensemble de pratiques qui précèdent la publication et qu'il n'est pas possible de trouver sans les observer sur place. D'autre part, me placer en situation de discussion en réalisant des entretiens me permet d'accéder à des connaissances en quelque sorte déshermétisées , puisque en annonçant d'avance que je ne suis pas astrophysicien, mes interlocuteurs ont fait souvent l'effort d'éclaircir des notions qu'ils supposaient que je ne connaissais pas, en général à ma demande. Nombre d'entre ces notions ont aussi été laissées *implicites* de prime abord car elles sont si habituelles qu'elles font partie d'un vocabulaire normal et localement compris dont la définition n'a pas besoin d'être donnée à chaque référence. Par exemple tout au long de mon stage, plusieurs fois par jour a été mentionné lambda cé dé èm sans que je n'ai la moindre idée de ce que cela signifiait avant assez tardivement, même après mon stage: entrer dans une communauté qui n'est pas la sienne permet de ressentir la *normalité*, qui en tant qu'étranger, constitue un ensemble d'idées, de choses prises sur le qui-vive et le mode de l'évidence, qui sont surprenantes et que l'on ne se sent pas en mesure de remettre en question ou de demander la définition sous peine de paraître déraisonnable. Or c'est exactement ce type de raisonnement normalisé, local et implicite, en général ignoré dès que l'on sort de la communauté, qui m'intéresse, puisque ce sont des normes tacites de la recherche que tout le monde connaît *sauf moi*. L'apport de telles connaissances à l'analyse philosophique est déterminant puisque c'est le niveau le plus récent de la recherche qui entre en compte sur les hypothèses de la matière noire, c'est aussi le niveau le plus caché car il est local et universellement compris dans une communauté hermétique.  Je dois reconnaître que mon séjour à l'OBAS fût particulièrement facilité par la bienvenue que les astrophysiciens ont donné à des discussions d'ordre philosophique. J'ai pu remarquer qu'ils se savent opérer à tel haut niveau d'erreur (ou à si petit niveau de probabilité) qu'ils sont alertes par rapport à ce que leurs propres théories impliquent d'un point de vue ontologique et du point de vue d'une théorie de la connaissance. J'ai alors pu glaner beaucoup d'informations explicites et implicites sur leurs outils et buts de travail. Comme moyen d'information, j'ai choisi principalement de réaliser des entretiens semi-directifs et d'observer les pratiques.

La méthode que j'ai choisi pour réaliser mes entretiens est plutôt simple: j'ai choisi des personnes que je savais travailler sur l'hypothèse de la matière noire (plutôt que d'autres qui ne travaillaient pas dessus), et avant de les rencontrer, j'ai consulté leurs travaux précédents. Je les ai ensuite questionnées de deux manière: d'abord sur comment elles interagissaient avec les hypothèses de la matière noire dans leur travail, ensuite si j'en avais l'occasion sur la manière dont elles considéraient le paradoxe en lui-même, c'est-à-dire sur la valeur d'anomalie plus ou moins importante qu'elles lui conféraient. Je verrai que contrairement à l'idée que l'on pourrait s'en faire, un astrophysicien ne travaille pas simplement avec un modèle einsteinien de l'univers. Une telle chose n'existe pas. Les modèles astrophysiques et cosmologiques sont des ensembles hypothétiques bien plus complexes que la prédiction ou remise en question à partir de la théorie de la relativité générale. L'activité de modélisation constitue une méthodologie communautaire en elle-même.

Au cours de mes entretiens, j'ai eu le temps de découvrir tout un fil méthodologique d'acquisition des données, de transformation des données, et d'hypothétisation. *In fine*, là où la contextualisation historique et philosophique m'a permis de me faire une image globale de l'activité astrophysique et de la manière dont le problème de la matière noire a émergé, c'est seulement l'enquête interne qui m'a permis de comprendre comment toute cette anomalie est traitée par la communauté concernée.  Ces deux sources de connaissance de la pratique et théorie astrophysique se complètent alors en donnant une représentation historique concrète et contextualisée de la méthodologie employée pour traiter de l'anomalie de la matière noire. A partir de ces entretiens et de mes notes, je vais donc tenter de montrer deux choses: d'un côté, ce que constitue de manière générale l'acquisition des observations en astrophysique. Ainsi pour mieux comprendre comment l'on sait quelque chose du ciel, et pour comprendre de quelle manière le signe médian de la matière noire peut être atteint. Je montrerai que l'astronomie ne se résume pas du tout à un oeil derrière une lunette et à une prise de note compulsive. D'un autre côté, je décrirai comment la recherche hypothétique, qui se concentre sur les modèles, est réalisée au sein du laboratoire par les quelques acteurs auprès desquels j'ai pu me renseigner. Ce double travail m'a permis de trouver une méthode d'observation et de théorisation en astrophysique qui n'est pas évidente, particulièrement à propos de la matière noire. Cette masse d'information que j'aurai constituée et reliée entre ses éléments, je l'analyserai philosophiquement au chapitre suivant dans le contexte général que j'ai trouvé au chapitre précédent.

## Des entretiens avec des astrophysiciens.  

Bien que mes questionnaires disposaient d'une idée générale similaire entre chacun, ils ne furent pas tous identiques dans leurs questions particulières. Voici cependant l'ensemble des questions que je m'étais fixé au préalable et qui a été adapté ensuite pour chaque cas[^114]:\ a. What is your subject of research ?\ b. What have you studied ? (formation/education)\ c. What do you think of dark matter/ what is your approach on dark matter ?\ d. Why are you interested by dark matter ?\ e. Do you think that different models of dark matter or MOND are equal ?\ g. On which criteria is a model relevant according to you ?\ h. To which point do you think that a computer model/simulation represents reality ?\ i. Following which criteria would say that your model/theory is objective ? [^115]

Et d'autres questions moins axées sur la matière noire mais utiles tout de même:\ Comment concevez-vous votre activité en astrophysique ?\ Comment liez-vous la notion de modèle à une notion de réalité ?\ Que veut dire que \"ça marche\" ?\ Comment lieriez-vous votre travail à une notion d'objectivité ? \ Diriez-vous que vous êtes marginalisé ?\ Que pensez-vous de la place sociale de votre discipline dans le monde scientifique ?\ Pourquoi votre recherche prend-elle du temps ? (limites\...) [^116]

Cependant j'ai moins utilisé ce second questionnaire. De plus, j'ai abandonné dans les deux questionnaires la question de l'objectivité: parler de méthode était bien plus intéressant et informatif sur les conditions de connaissances dans lesquelles travaillent les chercheuses et chercheurs. Je vais présenter ce que j'ai pu apprendre. Du fait que ces questionnaires ont été adaptés individuellement et au fil de la discussion, il faut les prendre plutôt comme un point de départ ou un état d'esprit visant à révéler des pratiques et des théories de la connaissance. Je procéderai dans un ordre artificiel[^117] qui permet de comprendre l'origine des données en astrophysique, puis l'origine des idées.

### Obtenir des données en astrophysique.

Lors d'un entretien avec un chercheur[^118] de l'Observatoire le 27 avril à 10h01[^119], j'ai pu apprendre un certain nombre de choses sur le processus d'acquisition et de constitution des données . Le terme de données pourrait surprendre: il ne fait pas référence à des données sensorielles, *sense data*, au sens où ce serait l'acquisition d'une connaissance immédiate et inintelligente. Ce n'est pas non plus le sens baconien de la catégorisation systématique de *sense data*. Au contraire, les données font références au terme anglais de *data*, qui désigne une connaissance traduite en langage informatique, codée selon certaines normes. L'OBAS dispose ainsi d'un *data center*, d'un centre de données, où sont hébergées pour la communauté mondiale de nombreuses productions de missions d'observation[^120]. Mais une donnée astronomique ne se montre pas d'abord sous la forme d'une image ou d'une donnée publiée.

De ce que j'ai perçu dans les explications du chercheur, il vaut mieux parler d'un processus d'acquisition et de constitution des données, justement car les données sont obtenues par le biais de plusieurs étapes. La première et plus ancienne étape dans l'ordre de traitement consiste à obtenir un temps d'observation auprès d'un télescope international. Il n'y a plus de télescope à usage systématique à Strasbourg, même si les données sont hébergées ici au CDS: c'est que les télescopes sont des instruments sensibles aux conditions d'observation atmosphériques et qu'ils sont alors placés depuis les années 60 au moins, dans des lieux idéaux, par exemple des hautes montagnes au Chili.  De plus, ils sont coûteux et construits internationalement. Mais le nombre limité de télescopes ne correspond pas à un nombre limité d'observatoires: ce n'est pas un rapport 1:1 qui est établi entre les deux. Depuis la fin du XIXe siècle, il est devenu possible, grâce à la première démultiplication des observatoires, qui chacun pour la plupart possédaient leur propre télescope (du fait qu'ils n'étaient justement pas situés très idéalement), et grâce au développement de la photographie, d'envisager de massives cartes du ciel réalisées en groupe[^121]. On note cependant la première collaboration astronomique à grande échelle dès 1800[^122], et l'IAU est fondée en 1918[^123].

Bref, du fait que le nombre de télescopes est en rapport d'infériorité au nombre de laboratoires, et que les télescopes sont distants et ne sont pas à la portée pratique d'un laboratoire, mais que la multiplicité des laboratoires est habituée à collaborer dans des projets de grande envergure, il faut établir un système d'allouance de temps d'exposition.  Selon un autre chercheur, ce temps d'exposition n'est pas seulement coûteux au sens pécunier (il faut de toute manière un financement pour traiter les données), mais il est de plus coûteux au sens d'un facteur pression : le temps est alloué selon un système de proposition de projets, et selon le télescope, il peut y avoir un facteur de réussite de l'acceptation du projet de 1/5 voire 1/10[^124]. Une équipe n'est donc pas assurée d'obtenir le temps d'observation qu'elle demande. Si en revanche ce temps d'exposition est obtenu, le télescope va observer durant plusieurs semaines (selon le temps demandé) et constituer un ensemble de données crues qui seront envoyées au laboratoire.

Observer et posséder un temps d'exposition ont un sens précis dans le cadre d'un projet faisant usage de données créées par un télescope.  Observer ne signifie évidemment pas placer l'oeil derrière le télescope, prendre des notes, faire des dessins et placer les objets vus dans leur position dans le ciel. C'est la manière ancienne de faire de l'astronomie. Un télescope moderne est constitué d'un tube protégeant et soutenant des éléments optiques déviant le chemin de la lumière vers une surface photosensible. Cette surface, que l'on appelle aussi capteur photosensible aujourd'hui, a d'abord était faite de fines plaques recouvertes d'une solution photosensible. Une solution photosensible réagit chimiquement à l'exposition d'une source lumineuse. Ce genre de solution, si elle est exposée à l'image produite par une lentille qui fait converger le chemin de la lumière vers le plan focal où est positionné le capteur, permet de conserver une image des objets qui réfléchissent ou génèrent des sources lumineuses. De cette manière, il est possible d'enregistrer l'image des étoiles et des astres du système solaire, si la plaque est assez sensible et si la source est assez lumineuse. Si la plaque photographique est restée en usage jusque dans les années 1970, elle devient relativement désuète (bien que Rubin l'utilise encore en 1983) à mesure qu'un autre type de capteur est développé, le CCD. CCD signifie *Charge-Coupled Device*. Le principe du CCD n'est pas chimique, mais physique: un tel capteur possède une surface qui à la réception d'un photon, libère un électron (d'où la charge couplée), et l'électron est ensuite converti en signal numérique, lequel peut être enregistré, reçu et utilisé par des dispositifs informatiques. La numérisation de l'information lumineuse permet d'organiser le stockage et le traitement indéfini de données numériques.  Avec le capteur CCD commence l'ère contemporaine de l'observation astronomique, celle qui est toujours d'actualité. Quant à la partie mécanique et optique du télescope, outre la simple convergence de la lumière, elle comporte un ensemble de techniques de correction, notamment le calcul de la déviation de lumière par rapport aux mouvements atmosphériques, qui sont compensés par un miroir supplémentaire interférant avec la réception de la lumière du miroir primaire, ils disposent aussi de montures extrêmement élaborées supportant le télescope et opérées par un système de contrepoids dynamique pour que les grands miroirs ne plient pas sous leur propre masse ainsi déformant l'image qu'ils projettent[^125]. Observer par le biais d'un télescope, c'est donc faire usage d'une technique élaborée pour faire converger la lumière vers un capteur photosensible et numérique. Le temps d'exposition, lui, fait simplement référence au temps durant lequel le capteur est soumis à la source lumineuse. Ce temps est accompagné d'une traque mécanisée de la cible d'observation qui peut bouger le télescope automatiquement.

L'acquisition des données commence donc par la proposition d'un projet d'observation et l'obtention d'un temps d'observation auprès d'un télescope lointain. Elle s'ensuit de l'envoi des données numériques acquises au laboratoire, qui celui-ci dispose d'un an pour les traiter et publier un article dessus, avant qu'elles ne soient libérées publiquement ou délivrées à un autre laboratoire après ce délais[^126].  La demande d'observation n'est pas le seul moyen d'acquérir un tel type de données, il y a aussi des bases d'observations déjà réalisées qui sont disponibles à une échelle communautaire ou des bases de données totalement libres d'accès. Mais telles quelles ces données ne sont pas encore considérées comme utiles et fiables pour l'analyse.

Ainsi que je pouvais le voir le 27 mai à 10h, avec le chercheur sus-mentionné[^127], sur son écran qu'il me montrait, ces données sont soumises à un traitement informatique automatisé et adapté au type de données concerné. Il défilait en effet rapidement sur son moniteur les résultats successifs d'un code de traitement écrit en FORTRAN, Perl et C[^128], trois langages de programmation en interaction. Ce traitement se présente sous la forme d'une réduction des données. La réduction des données commence par l'élimination du bruit , un ensemble d'artefacts produits par le capteur qui sont considérés comme ne provenant pas d'une source lumineuse astronomique et qui ne sont donc pas désirables. Cette élimination doit être ajustée puisqu'elle ne doit pas non plus gommer les objets qui intéressent le chercheur. La seconde étape de réduction des données consiste à se passer des objets qui n'intéressent pas le projet en cours. Les données elles-mêmes sont vérifiées par croisement avec des catalogues préexistants. La discrimination n'est jamais parfaite et nécessite des allers-retours correctifs de l'algorithme de réduction, de surcroît, c'est surtout la distillation d'une quantité d'information phénoménale qui est en jeu. Ce caractère de données massives est important, c'est lui qui oblige à automatiser la réduction des données. Lorsqu'on observe le ciel et que l'on veut utiliser l'observation obtenue, il est nécessaire de réduire les données acquises. Le ciel n'est donc pas observé de manière empirique, universelle et systématique, à l'image d'une méthode baconienne qui épuiserait le réel par ses classifications qualitatives diversifiées et ses observations détaillées et perspicaces. Dans le cadre de nouveaux instruments, il est possible d'élaborer des modèles prédictifs sur le type d'observation que l'on devrait avoir et de comparer les résultats effectifs aux prédictions du modèle pour estimer si ce que l'on a obtenu dans les observations est correct et attendu. Cependant, il faut garder en tête que dans tous les cas la réduction des données est une analyse en mesure de les erroner. C'est pourquoi réduire les données pour les utiliser est un processus de correction itérative, d'abord à l'échelle individuelle et à celle d'une équipe, ensuite à l'échelle communautaire lorsque les données traitées sont publiées et critiquées par les pairs.  Observer, ce n'est donc pas voir, c'est analyser ce qui a été perçu par une machine et édulcorer l'observation pour la rendre visible et utilisable à l'astronome.

### Créer un modèle en astrophysique.  

Au paragraphe précédent, j'ai parlé de modèle . C'est une notion importante car je l'ai retrouvée dans tous mes entretiens portant sur la méthodologie de la recherche, et Rubin la mentionnait également. En donnant un détail progressif et chronologique de toutes les manières dont j'ai entendu parler des modèles, j'espère pouvoir caractériser l'idée générale derrière le concept pratique. Il me semble possible de découvrir une certaine structure de travail qui mette à jour la méthodologie portant sur les hypothèses de la matière noire.

Les modèles sont des notions tacites. Lors du tout premier entretien que j'ai réalisé avec trois doctorants le lundi 25 avril 2022[^129], je n'ai pas eu accès à une définition de ce qu'était un modèle . Je ne tiens pas les doctorants avec qui je me suis entretenu pour responsables, ce qui serait absurde, puisque j'étais leur interlocuteur. Plutôt, je me suis senti immédiatement pris dans le flot du vocabulaire habituel de l'astrophysique, ce qui relève de la norme du discours. J'éclaircirai cependant la notion de modèle dans des entretiens ultérieurs avec des questions explicites, n'ayant pas manqué sur le moment de mon entretien de remarquer ma propre connaissance incomplète du terme. Durant ce premier entretien j'ai eu accès à un ensemble de problèmes que celles et ceux qui ont une connaissance de MOND et Lambda-CDM[^130] rencontrent et je pense qu'y prêter attention est un bon point de départ pour l'enquête. Voici ce qui est problématique pour celles et ceux qui modélisent: un modèle ne se crée pas *ex nihilio*, il exige au contraire une revue de la littérature et il doit être raisonné d'une amélioration en terme de nouvelle hypothèse ou de précision, qui justifie son élaboration et l'utilisation de la puissance informatique de calcul qu'il nécessite; un modèle s'élabore par calcul, ce qui peut être effectué sur feuille comme par ordinateur; si un modèle analytique est trop complexe, il peut être calculé numériquement; MOND et Lambda-CDM ne sont pas également efficaces mais présentent une divergence d'efficacité selon le niveau des structures cosmologiques auxquels ils opèrent; dans le cas de l'information, il y a un problème de parasitage qui ne permet pas toujours de définir les causes à imputer à l'activité de la matière noire; la relation de l'observation à la modélisation n'est pas immédiate. Au cours de mes entretiens, tous ces thèmes reviendront selon des formes similaires et souvent exemplifiées et plus précises.

Mais remarquons déjà une structure générale du modèle : le modèle est une représentation. C'est ce que l'on comprend en faisant attention au fait que les modèles peuvent être appliqués au mêmes niveaux (niveau cosmologique, niveau galactique) tout en possédant cependant des performances contraires: MOND excelle pour les galaxies, mais pêche aux structures universelles; c'est le cas inverse pour lambda-CDM. On le comprend également si l'on considère que le chemin depuis et jusqu'aux observations est sinueux, et que l'élaboration de ce chemin nécessite une revue de la littérature: cette connaissance n'a donc pas le caractère d'une induction immédiate qui prendrait la forme d'une équation représentative parfaite par rapport aux observations. Le modèle se montre plutôt comme un moyen heuristique qui naît d'une suite d'hypothèses dont la trace est laissée dans la littérature. De plus, son statut d'outil est définitivement inscrit par le fait qu'il peut être résolu de plusieurs manières. En effet, le papier, le crayon et la réflexion sont d'aussi bons points de départ pour une solution que la *computation*[^131] informatique. D'autre part, il est possible de discrétiser l'analyse lorsqu'elle se révèle trop complexe. Un modèle est donc un outil de forme mathématique et souvent informatique, dont la contingence repose sur la multiplicité de formes calculables qu'il est possible de lui conférer. Le modèle émerge ainsi d'une littérature comparée et de nécessités pratiques de résolution. De l'observation à la modélisation, il y a tout un chemin inventif pour représenter un certain état des choses en astrophysique.

Lors d'une réunion hebdomadaire qui se tint le même jour à 14h[^132], j'eus l'occasion d'en apprendre plus sur le processus d'invention des modèles. En effet, cette réunion sert à présenter les avancées récentes de chercheurs et chercheuses voulant parler de leur projet. Le but est de recevoir des critiques de ses collègues afin de perfectionner son modèle. Dans cette optique, ils et elles présentent leurs modèles et l'objet et les règles qu'ils leurs ont conférés. J'ai ainsi entendu parler de: wall box , megaparsecs , particle density , I wrote a code , I wanted to look at resonances of super massive black holes , so this is a crazy model .

Ces termes qui ne font ni partie du langage commun ni du vernaculaire de l'histoire, de la philosophie ou de la sociologie m'ont immédiatement frappés. Ils étaient prononcés et utilisés avec facilité par les chercheurs, par exemple, pour le wall box , ils n'hésitèrent pas à discuter de quelle valeur numérique lui conférer. J'en conclus donc que c'est un ensemble de notions et de façons de parler qui sont situées dans un cercle de compréhension commun, ce que l'on appelle en général un paradigme . A partir de cette réunion, je peux commencer à enquêter sur le paradigme des modèles, en prenant surtout en compte le fait que les notions en jeu possèdent des définitions implicites et tacites. La tacité des idées ne me rend donc pas possible de donner des définitions systématiquement claires des mots si un acteur paradigmatique ne m'en a pas transmis la définition. Cependant, à force d'avoir côtoyé et le vocable et les pratiques des chercheurs (ayant moi-même pu mettre la main à la pâte ![^133]), et à force d'observation, j'en possède une connaissance incomplète mais raffinée par le même processus de transmissions *sans mots* qui caractérise les connaissances tacites. Le sens implicite de ces notions peut être acquis sans en parler explicitement aux pratiquants de la discipline en question, en essayant de les utiliser pratiquement ou en les saisissant par analogie avec l'ensemble culturel auquel elles participent. Cependant, je n'ai pas hésité non plus à poser des questions explicites quand j'en avais l'occasion.

Je compris ainsi avec l'habitude du laboratoire, que *wall box* fait référence à un certain découpage du modèle dans lequel les calculs s'effectuent, c'est alors selon mon idée soit une condition aux limites générale, soit une discrétisation de la continuité du modèle qui n'est pas soluble dans un temps acceptable sans compartimentage. La *particle density* (densité de particules) et les *megaparsecs* sont ce qu'on appelle des paramètres du modèle, c'est-à-dire des valeurs de départ de simulation, dont le sens notionnel est identique à celui de ces valeurs dans la science observationnelle de l'astrophysique. I wrote a code et I wanted to look at \[\...\] sont pour moi des éléments extrêmement informatifs. La première phrase fait référence à l'idée qu'un modèle est une activité d'écriture individuelle, et la mention du code signifie que ce qui est écrit se situe à un niveau abstrait qui doit être compilé par l'ordinateur en mesure d'être soluble[^134]. Le code est lui-même une représentation pour la machine que le pratiquant donne volontairement et qu'il doit faire l'effort d'élaborer durant son temps de travail journalier. Quant à la seconde phrase, elle peut paraître surprenante de prime abord. Mais la promotion de la volonté de *voir à partir d'un modèle* est probablement ce qu'il y a de plus essentiel dans toute l'activité astrophysique actuelle sur les objets peu visibles ou sur lesquels l'on dispose d'encore peu de prédictions généralisées. Voir ne se fait plus par un télescope, là où il faut diriger son regard, c'est vers un code représentant et diluant les connaissances astrophysiques avec des valeurs inscrites dans des codes compilables par ordinateur et ensuite vers le résultat qui sort du calcul. La dernière citation que je mentionne, le crazy model me sera expliquée dans un entretien ultérieur par l'auteur concerné par cette qualification.

Les modèles peuvent prendre des formes très diverses. Je rencontrais le même jour[^135] une personne qui serait ma collègue de bureau et qui m'expliquait comment elle modélisait des étoiles afin de produire en avance des spectres de galaxies anciennes: ses modèles d'étoiles étaient surtout des tables d'éléments atomiques qui seraient interprétées par un programme tiers. Son modèle propose donc un type d'analyse différent qu'une simulation temporelle de la dynamique des galaxies, par exemple.

Le 26 avril 2022, je réalise un entretien avec un autre chercheur, l'auteur du crazy model [^136]^,^[^137]. Il m'a parlé principalement de la particularité[^138] des modèles et des sous-halos de matière noire.  Cette particularité des modèles se trouve à plusieurs niveaux: tout d'abord, dans l'idée de testabilité, puis dans celle de l'inventivité d'une nouvelle hypothèse. Un modèle est particulier parce qu'il est élaboré à partir d'une idée particulière qui doit faire montre de nouveauté ou d'amélioration en terme de précision. Il est également particulier parce qu'il doit être testable, en ce sens, la testabilité est un terme pratique de distinction entre un modèle pertinent et un modèle peu utile. Un modèle qui n'est pas testable présente peu d'intérêt. Pour le moment, je sais que la testabilité du modèle se réfère premièrement à la possibilité de le résoudre par un moyen analytique ou numérique, deuxièmement à la possibilité de le soumettre aux pairs et à leur critique, troisièmement et sans nécessité, à la possibilité de le comparer à des observations[^139]. S'il n'est ni communicable ni calculable, il ne peut pas être testé. Le chercheur m'informe également[^140] du fait que cette méthode de proposition et de particularisation des modèles induit un front de la recherche si divers qu'il est impossible à suivre, ce qui l'en inquiète: peut-être ne saisirait-il pas la solution finale à la matière noire car le modèle qui la représenterait serait bien hors de son champ de connaissance même dans le paradigme général des modèles. Les sous-halos (je traduis de l'anglais subhalo ) de matière noire font référence à une certaine méthode de représentation de l'hypothèse plus générale que la matière noire se présente sous forme de halo autour d'une galaxie. L'hypothèse du halo permet d'imaginer plusieurs solutions qui lui donneraient une forme consistante avec la dynamique des galaxies qui est observée. L'une des possibilités explorées des halos est leur subdivision en plus petits corps de matière noire. Nous reverrons cette hypothèse du sous-halo et son contexte d'étude dans un entretien ultérieur avec un autre chercheur, ce qui me permettra de la préciser quelque peu.

![image](/img/rhombi_jupiter.png) 

En tout cas, l'entreprise de modélisation continue de montrer son caractère historique et sa contingence. Elle n'est d'abord pas qu'un simple paradigme horizontal: ce n'est pas une théorie unie correspondant à un seul problème. C'est un paradigme général qui connaît ses propres communautés de spécialisation. La matière noire prend alors l'apparence d'un métaparadigme, c'est-à-dire d'un problème bien caractérisé qui dispose de ses communautés de solutions, qui interprètent à leur tour le paradigme général selon des hypothèses particulières communes et hermétiques aux autres communautés. Au paradigme de la méthode du modèle, il y a donc des sous-paradigmes de solution. La matière noire met en avant une hyperspécialisation paradigmatique de la recherche. Il est alors possible de distinguer des modèles de modélisation, au sens où pour chaque projet de modèle, plusieurs patrons de modélisation sont disponibles. Outre ces considérations, le chercheur me fit mention d'un problème particulier aux modèles: la valeur de leur caractère descriptif opposée à celle de leur caractère prédictif. Il me semble qu'il considère ainsi qu'un modèle uniquement descriptif ait une valeur moindre car au fond il n'est qu'un modèle *ad hoc* qui ne permet pas de savoir plus que ce qui est déjà observé:

So what I find a relevant question right now is, well, can the theories we have be falsified ? Can we rule them out ? Because I think it's, I'm not saying it's easy, but I think it's possible to construct a theory which describes everything. But then how do we tell that it's the right one ? If it doesn't, if it only predicts exactly what we've observed so far and can't say anything beyond what we know so far, then it's nothing more than a fit of what we see, right ? And just fitting what we see, I think science can do a bit more. We cannot only describe what's going on. But if possible, we can also develop some understanding. Of course, always in the limits that who knows if the models we make really represent reality. But I think there is some logical structure to it that makes it more interesting than just a fit. So one question could be, for example, if dark matter is a particle, how much does it interact ? I think we already know that it must interact very little. But do we know that it just doesn't interact at all ? , [@enr_26 18min48s - 20min15s].

Le chercheur évoque le même problème caractéristique que je trouvais aux épicycles de Hipparque ou encore à ceux de Ptolémée et de leurs complications ou alternatives successives: une description mathématique peut être une théorie *ad hoc* dont les prémisses sont également la conclusion. Décrire ce n'est alors qu'interpréter et traduire ce qui peut déjà être constaté dans un autre langage. Si cette activité n'a pas de valeur prédictive qui permet de trouver des phénomènes encore inaperçus ou des généralisations nouvelles, alors elle n'a pas la valeur tant désirée de la découverte scientifique. Il faut donc considérer qu'en astrophysique le but heuristique de la plupart des modèles n'est pas de décrire, bien que décrire puisse être une activité de base permettant d'élaborer un patron de modélisation. Ce qui est recherché, c'est la prédiction de phénomènes nouveaux[^141].

La complexité apparente au centre de cette manière de chercher quelque chose de neuf est créée par l'entremêlement des concepts mathématiques descriptifs et des opérations d'observation au centre de l'idée de phénomène nouveau . Je pense que cette idée floue de phénomène nouveau peut être décomposée de la manière suivante: les connaissances observationnelles et la littérature de la modélisation ne sont pas liées par nécessité au départ d'une recherche. Le modèle peut être tiré de la littérature pure, dont le degré de représentation éloignée de l'observable est toujours contingent à la littérature choisie comme référence de pensée. A partir de cette littérature, un modèle est conçu qui ne représente pas les observations, mais qui représente bien la littérature à partir de laquelle il a été émis. Ce modèle peut être en mesure de proposer des équations qui prédisent l'apparition de certains phénomènes. Ensuite, la comparaison avec un autre modèle prédictif ou avec des analyses d'observation permet de déterminer si les prédictions sont vraies. Si elles ne le sont pas, le modèle est falsifié . Cela ne veut pas dire qu'il est entièrement faux dans l'ensemble des théories qu'il implique, cela veut seulement dire que sa prédiction est fausse.  Il est alors possible ensuite de le corriger, soit en augmentant sa précision, ce qui est caractéristique d'un modèle mathématique computable où les valeurs mathématiques sont interchangeables du moment que la puissance de calcul nécessaire est présente, ou en changeant ses prémisses. Il me semble que ce processus itératif de falsification et de correction des modèles constitue l'activité principale de publication et de progression de la recherche, ce que je tenterai de vérifier par la suite.

A 11h le même jour (26 avril 2022) j'effectue un entretien avec deux doctorantes et un doctorant[^142]. Je les interroge sur leurs méthodes et les deux doctorantes commencent par me décrire la leur. L'une s'occupe plus d'observations, mais réalise tout de même des modèles, l'autre fait des calculs forts et me dit qu'elle ne fait que du numérique, des simulations, du python[^143], du traitement de données.  Toutes deux mentionnent un lourd travail sur ordinateur, qui est leur outil principal de modélisation. Elles mentionnent également le fait qu'elles affinent leurs modèles en fonction des observations auxquelles elles ont accès. Elles ont appris leur méthode durant des cours, durant un stage de 6 mois en laboratoire et durant leur thèse. Enfin elles me parlent des limites observationnelles et du fait qu'un modèle devant être prédictif, il possède toujours une marge d'erreur vis-à-vis de la réalité: il n'est donc réaliste quand la mesure où il est restreint.  Elles appuient beaucoup sur l'importance de contraindre les paramètres .  Les modèles correspondent à une gamme de paramètres qui peut varier, qui doivent être mis en correspondance avec les observations et il y a plusieurs sets de paramètres différents pour une même observation [^144]. Le doctorant me décrit aussi sa méthode, il réalise pour sa part des simulations qui suivent le même impératif de correspondance aux observations et il fait usage d'intelligence artificielle pour combler des étapes de la réionisation de l'univers. Tous les trois me décrivent comment les techniques sont limitantes pour élaborer des connaissances et le fait que les observations qui permettraient de falsifier leurs modèles sont lointaines: les télescopes nécessaires ne seraient élaborés que d'ici une dizaine d'années (vers 2030). Comprendre la réalité est un problème, et un modèle, ça marche plus ou moins bien.

La mention de simulation est intéressante et il importe de la distinguer d'un modèle, du moins en astrophysique. Cependant nous reviendrons à cette distinction plus tard avec une personne qui m'en parlera de manière étoffée. Le traitement de données , c'est la doctorante faisant des simulations qui fait référence à la notion, je suppose donc que c'est une référence à la manière dont les données sont incorporées dans le modèle, ce qui rajoute un degré d'élaboration sur des données astronomiques déjà traitées auparavant. La manière dont les modèles sont affinés avec un rapport proche aux observations semble donc être commune entre plusieurs de mes interlocutrices et interlocuteurs, en revanche nous verrons qu'elle n'est pas systématique. Le cadre dans lequel elles ont acquis leur méthode me montre que cette méthode se trouve belle et bien dans un paradigme fermé, auquel il faut accéder par un ensemble d'étapes précises: l'apprentissage théorique en cours (qui inclut probablement des travaux pratiques dirigés), le stage de pratique durant lequel un court projet est réalisé, le doctorat qui dure plusieurs années et dans lequel l'on fréquente les membres de la communauté (collègues, directeurs et directrices de thèse) et la culture que ces membres diffusent également lors de conférences, de réunions. La prédictivité du modèle semble être liée de manière importante aux observations disponibles ou possibles; en effet elles sont falsifiantes et les progrès techniques sont donc essentiel pour établir le niveau épistémique de vérité et d'erreur auquel se situe leurs projets.  Contraindre les paramètres consiste à trouver un ensemble de conditions qui permettent de restreindre les solutions de l'hypothèse qu'il et elles modélisent, dans le cas présent les observations jouent un grand rôle. Cependant, nous verrons plus tard que cette façon de restreindre le modèle à partir de référence extérieures peut aussi être trouvée dans des cas entièrement abstraits et théoriques qui font alors référence aux équations et à la littérature de la physique astronomique pour déterminer les conditions d'opération des calculs de leurs modèles.  Enfin, dans le vernaculaire du laboratoire, ça marche et d'autres variations plus ou moins positive et négatives sont une façon de parler mystérieuse dont le sens est parfois difficile à percevoir. Il me semble que le sens du ça marche peut prendre un extrême avec deux simples opposés (ça marche, ça marche pas), auquel cas cela signifie soit que le modèle est faux vis-à-vis des règles et prédictions astrophysiques (il y a une règle qui devrait être changée) soit que l'ordinateur ne peut pas calculer (le modèle est trop complexe, ou le code est erroné et l'erreur apparaît lors de la compilation du code). Mais si l'on dit pas exemple, dans un entre-deux, que cela marche plus ou moins bien , que cela marche à peu près , ou que cela marche mais avec une moue, l'on fait alors référence à l'exactitude du modèle vis-à-vis d'une certaine norme de vérité ou de correction: peut-être le modèle est-il volontairement exagéré ou propose-t-il des prédictions proche d'un modèle plus précis en négligeant certains paramètres.

Il est possible de dégager une structure supplémentaire à la méthodologie des modèles qui consiste à faire systématiquement état du niveau de représentation et de vérité auquel le modèle se situe vis-à-vis des références comparatives disponibles, ces références pouvant être elles-mêmes des modèles ou des observations. Les astrophysiciennes et astrophysiciens, ainsi que les astronomes, ont donc un devoir épistémologique propre qui consiste à situer et faire la critique du niveau de connaissance qu'ils élaborent.

Modéliser s'effectue à un bureau. Il est possible de décrire cet environnement de travail qui conditionne l'activité de modélisation tout autant que les paradigmes hypothétiques. A 15h22 le 25 avril 2022[^145] j'ai l'occasion de visiter l'*open space* où les étudiants de licence et master effectuent des stages et projets. Accompagné de son directeur qui discute avec elle, j'ai l'occasion d'observer la manière de travailler d'une étudiante. Elle est située à un bureau accollé à un autre. Elle possède deux moniteurs avec un ordinateur fixe utilisant le système d'exploitation Ubuntu, et d'autre part un Macbook avec un éditeur de texte ouvert (voir ci-dessous). Sur ses moniteurs, elle code dans un autre éditeur de texte et visualise des images. Elle tente de modéliser des galaxies lointaines (7 milliards d'années lumières) avec des halos de matière noire. Elle effectue plusieurs retours entre les images, le code et la compilation du code. Le directeur pose une question assez intéressante vers la fin de l'observation: est-ce qu'il râle ? .  L'ordinateur ou le programme que l'ordinateur exécute est personnalisé et l'on veut savoir si le code marche .

Cette démarche de travail est typique et j'aurai l'occasion d'observer ultérieurement d'autres manières de faire très similaires: lecture ou relecture rapide d'articles et échange entre la fenêtre de lecture et l'éditeur de code; pensée sur le papier, production de graphs et code sur ordinateur, des activités en constant rapport. Même au niveau le plus bas d'élaboration, la démarche reste itérative et se complète à partir de plusieurs sources. Il faut d'ailleurs prendre en compte des fautes aussi simples qu'une erreur de frappe, qui change une valeur ou un mot du code et ainsi le résultat entier du modèle simulé. Mais sans cet espace adapté, avec ses outils particuliers, l'observatoire serait à peine un laboratoire et ne pourrait pas proposer de modèle à jour avec les pratiques mondiales.

![](/img/20220427-154251.jpg) 
*Les deux moniteurs reliés à la tour. Les différentes fenêtres de travail, et un seul clavier. Dans le coin gauche, le macbook. Sur les moniteurs en arrière-plan et autour des fenêtres l'on aperçoit le système d'exploitation Ubuntu. Crédits photographiques: Jonathan Freundlich & M. Lendrin.*

La valeur de la littérature n'est pas à sous-estimer. Un chercheur que j'ai interrogé le 28 avril 2022 à 9h du matin[^146] m'a parlé de sa manière de faire de la recherche sur l'effet de la barre dans les galaxies. Il utilise un halo de matière noire mais ne s'inquiète pas de trouver la théorie optimale[^147]: ce n'est pas son sujet principal de recherche. Il a simplement adapté le code diffusé dans un article. Pour construire leurs modèles, les chercheurs n'ont pas besoin de remettre en question toutes leurs théories: s'ils questionnaient tout, ils ne pourraient pas non plus faire d'hypothèse sur une seule recherche précise. Il y a donc un aspect pratique de la recherche qui fait de la matière noire une anomalie plutôt qu'une énigme seulement pour une communauté d'intéressés.

L'après midi du 28 j'interroge un ensemble de doctorants, dont trois que j'avais déjà connus lors du premier jour[^148]. Je les interroge sur leur processus de création d'un modèle. Ils me le décrivent ainsi: une partie provient de l'intuition, qui est paradigmatique, l'autre partie provient de la littérature et des observations. Pour créer un modèle, ils peuvent partir de plusieurs endroits: d'équations d'astrophysique ou de simulations. L'un d'eux me dit qu'il est commun de procéder en créant tout d'abord un modèle extrêmement grossier et simpliste et en l'affinant par la suite. La prise en compte des informations disponibles sur le sujet traité est essentielle et il faut choisir les données observationnelles adaptées et être à tout le moins capable de reproduire des prédictions déjà réalisées. Ils ont à leur disposition un ensemble d'outils qu'ils acquièrent durant leur formation, notamment des outils mathématique: par exemple l'on m'a dit qu'un oscillateur harmonique permet de décrit beaucoup de situations différentes. Je verrai d'ailleurs que l'un d'entre eux a d'abord travaillé en modélisation en médecine avant de venir en astrophysique. Enfin ils mentionnent une mesure du degré d'erreur d'un modèle en sigma , 5 étant un degré d'erreur infime. Créer un modèle n'est pas autant un processus rationnel qu'un processus inventif. Il y a une part de hasard qui tient à l'individualité et aux connexions d'idées qui peuvent se faire.  Cependant, ce procédé est en partie descriptible du fait que la plupart des individualités se situent dans un paradigme de recherche commun acquis dès les études ou transmis sur le lieu de travail. Créer un modèle est ainsi une méthode construite par une abductivité imaginative prenant racine dans les capacités d'invention et d'analyse d'un individu, qui cependant ne crée pas tout *ex nihilo*: la plupart de ses outils d'invention sont déterminés par la littérature, les données disponibles et la formation scientifique. Ainsi l'individualité se retrouve principalement dans la manière dont ces outils sont combinés, bien que celle-ci soit encore régulée: s'il est possible de ne pas suivre un ordre très strict dans la construction d'un modèle (tout le monde ne commence pas par faire une hypothèse à partir de simulations), en revanche il y a des éléments systématiques qu'il n'est pas possible de rater: la reproduction des prédictions et l'affinement du modèle en fonction d'une certaine norme (théories et hypothèses physiques ou observations). L'affinement est d'ailleurs limité par la puissance de calcul de l'ordinateur: si la solution prend trop de temps, il n'est pas envisageable. En plus de la limite des idées à la création de modèles, il y a donc également une limite technique à laquelle le pratiquant doit s'adapter. Il en résulte que tout modèle est un bricolage complexe de techniques et de théories. L'évaluation du degré d'erreur fait partie du devoir épistémologique que je mentionnais déjà et qui est également méthodique.

L'un de mes plus longs entretiens a été réalisé avec un chercheur le 29 avril à l'après-midi. Pourtant, à ce stade, la méthodologie générale de la recherche par modèles a déjà été éclairée. Ce qui suit n'est ni plus ni moins que des variations qui concourent à l'idée que j'ai reçu et donné ici. Ainsi l'homogénéité et l'isotropie de l'univers sont des outils de modélisation tirés de l'astrophysique et de la cosmologie. Ils permettent en quelque sorte d'organiser une réplication de l'expérience, à la manière de l'hypothèse ergodique [^149] qui permet de rapprocher le modèle de la mesure. Le modèle MOND fonctionne moins bien à l'échelle universelle qu'à l'échelle des galaxies, et inversement pour Lambda-CDM, qui possède quelques types de galaxies difficiles à modéliser mais fonctionne bien à l'échelle cosmologique. Le chercheur me donne plusieurs prémisses de lambda-cdm: l'hypothèse d'homogénéité de l'univers et d'isotropie de la lumière, les lois de la gravitation de la relativité générale, le modèle standard de la physique des particules, la matière noire et énergie noire, la topologie triviale de notre univers[^150], une influence gravitationnelle avant le découplage du plasma originel du big bang[^151]. L'une de leur limite étant que si l'on étudie les très petites échelles, elles ne sont plus valides, mais alors selon lui l'on sort de la cosmologie et des modèles cosmologiques[^152]. Il me dit que la simulation elle va rarement te sortir plus de choses que tu lui as donné [^153] et que le but ultime est de proposer un modèle pour trouver la matière noire dans un accélérateur: les simulations ne prennent souvent en compte que la gravité, elles supputent donc une quantité d'interaction gravitationnelle minimale pour pouvoir découvrir la particule réelle expérimentalement. Changer les prémisses sus-mentionnées permet d'élaborer des modèles alternatifs. Le 3 mai, lors d'un entretien avec un autre chercheur[^154], je glane des informations sur sa méthode qui consiste à élaborer un modèle sur papier, puis à en tester la possibilité par quelques graphs, à l'écrire ensuite sur ordinateur, et enfin peut-être à le comparer à des simulations. Il m'explique que les simulations sont toujours temporelles, ce sont en fait des outils numériques qui permettent de faire progresser un modèle avec un facteur temps. En revanche, ce chercheur ne se soucie pas de comparer le modèle à des données d'observation et travaille à un haut niveau d'abstraction.  Le modèle probablement le moins commun que l'on m'a décrit provient d'un entretien avec un doctorant le 4 mai[^155], il consiste à créér un réseau de neurones qui établirait de lui-même une corrélation entre plusieurs lois physiques et s'occuper alors d'analyse du langage.

### Une méthodologie paradigmatique.  

Au cours de mon stage, je réalise 11 entretiens dont 6 ont été enregistrés. J'ai eu accès à un bureau, auquel j'ai pu prendre des notes et utiliser un ordinateur. J'ai également pu observer durant des temps courts, comment quelques chercheuses et chercheurs travaillaient à leur bureau. Les trois derniers jours, j'ai eu l'occasion de tester un code libre d'accès[^156] de simulation cosmologique. J'ai du trouver les bonnes valeurs du modèle de départ afin qu'il marche . Mais à vrai dire, ne sachant pas vraiment faire de graphs mesurés, je ne sais pas non plus quel a été le résultat ni la nature de ce que j'ai simulé. J'ai à tout le moins pu visualiser les choses (figure

![](/img/xyxzyz1920.png) 
*Visualisation sur les axes xy, xz, zy de la simulation. La visualisation est elle-même paramétrable puisque ce n'est qu'un échantillonage du résultat: un pixel n'est pas nécessairement la représentation d'une seule particule ou d'une seule unité de simulation.*

J'ai pu observer et me renseigner sur un grand nombre d'étapes de constitution d'un modèle. C'était essentiel pour moi afin de saisir comment les astrophysicien travaillent sur les hypothèses de la matière noire. Je peux maintenant résumer le procédé dans l'ordre de formation et de matérialité du processus. Tout d'abord, un paradigme mathématique et probablement informatique est enseigné durant les études. Ensuite, puisque certains chercheurs et doctorants n'ont pas fait les mêmes études mais se retrouvent tout de même dans le même laboratoire, je suppose qu'un paradigme est transmis sur place, celui-ci se diffuse peut-être lors d'un stage et assurément toujours en travaillant et en communiquant avec ses collègues. Le paradigme sur le lieu du laboratoire transmet plusieurs choses. D'un côté, il transmet une *carte*. En effet l'habitude de se situer dans l'observatoire, de connaître les bureaux, d'aller à la cafétéria, de trouver son bureau propre à soi, de savoir ce qui s'y trouve, s'y peut trouver, et ne pourrait jamais y être, sont autant de matérialités formatrices qui font partie du paradigme. Kuhn n'a jamais limité la notion aux seules théories scientifiques abstraites. Cette matérialité permet à la chercheuse ou au chercheur de se former à ce qu'il est possible de faire, qui il est possible de rencontrer et qui il est possible de communiquer avec. Il faut par exemple savoir trouver la salle de réunion pour parler de son projet actuel et recevoir des critiques (elle est souterraine alors elle n'est pas si simple à trouver !) et il faut avoir accès à un ordinateur et un outil de prises de notes (éditeur de texte, cahier ou tableau blanc) pour réaliser son travail journalier qui consiste à modéliser. D'autre part, modéliser des règles ne suffit pas, il faut calculer le modèle, or il faut avoir cette fois accès à un superordinateur à distance, ce que l'équipe du service informatique s'occupe d'octroyer aux postes de travail. D'autre part, il faut être au courant de l'infrastructure administrative et de la manière dont de nouveaux projets peuvent être mis en place. Les projets qui requièrent de nouvelles informations nécessitent beaucoup d'efforts. Outre cette matérialité nécessaire de la recherche à l'OBAS, l'on trouve un ensemble foisonnant et commun d'hypothèses, d'idées, de représentations, de patrons de modélisation, de modèles hypothétiques, qui préexiste et trouve sa source dans l'élaboration des modèles. Le cercle du paradigme est donc fermé, il est à l'origine de ses propres solutions futures. Je dirais qu'un modèle de la matière noire est une manière de représenter l'hypothèse selon différentes méthodes qui ont en commun l'informatisation du modèle proposé, l'affinement itératif du modèle, la nécessité d'établir de nouvelles prédictions et l'évaluation du niveau de connaissance auquel se situe le modèle vis-à-vis de la littérature ou des données d'observation. Dans une optique particulière, les modèles sont aussi divers qu'il y a de pratiquants des modèles: certains sont purement théoriques, d'autres sont proches des observations, certains sont numériques, d'autres sont analytiques, certains font usage de *machine learning*, enfin ils se situent pour une part à l'échelle cosmologique et pour une autre, à l'échelle astrophysique. Je trouve alors qu'une hypothèse sur la matière noire est une particularisation d'une manière générale de modéliser que j'ai pu découvrir à l'OBAS.

# La philosophie de la matière noire.  

L'épistémologie des hypothèses sur la matière noire que je découvre au cours de cette analyse historique rétrospective et contemporaine ressemble à un édifice à plusieurs niveaux d'une grande complexité. Cet édifice est construit sur un système de connaissances et de théories de la connaissance dissemblables et situées historiquement.

J'ai en effet montré qu'au niveau théorique le plus fondamental, une tendance platonicienne a été conservée jusqu'à aujourd'hui: celle de la nécessité de donner une raison aux phénomènes pour les comprendre. Un phénomène ne se suffit pas à lui-même pour être vrai. La position actuelle est bien plus nuancée que celle de Platon grâce au développement de la mesure instrumentale et à son objectivation (élaboration et notation des conditions d'observation à des fins de reproductibilité): si un phénomène est observé, il est probable qu'il soit vrai, cependant il serait mieux et plus satisfaisant qu'il rentre dans le paradigme d'interprétation rationnel qui est promulgué à propos du ciel. La prédiction sanctionne ainsi l'observation. Ce paradigme d'interprétation étant principalement einsteinien, il repose alors nécessairement sur une interprétation de l'espace conditionnée par l'information lumineuse. Cette condition est également technique puisque les ondes électromagnétiques sont les seules sources d'information provenant du ciel disponibles à l'astronome.

J'ai également montré que dans la pratique contemporaine, cette collection de connaissances est manifestée par un instrumentalisme fort qui est présent autant dans l'observation qu'au niveau représentationnel et heuristique des modèles informatisés. C'est-à-dire que la technique est également essentielle et limitante autant pour l'observation que pour la recherche mathématique[^157].

Je pense que l'ensemble de ce système peut être analysé à plusieurs niveaux, en allant du plus large au plus individuel, afin de comprendre ce qu'il signifie d'un point de vue déontologique par rapport à ce qui existe et ne peut pas exister: au niveau le plus large, cet ensemble peut être analysé comme un paradigme kuhnien. Cette analyse permet de poser des questions d'ordre historique et de mettre en valeur une énigme devenue anomalie. À un niveau second, il peut être compris comme une phénoménotechnique bachelardienne extrêmement élaborée, qui ressemble en quelque sorte au système duhémien de la théorie physique. A un niveau tiers, pour la chercheuse et le chercheur qui sont à l'intérieur d'un paradigme et qui jouent avec cette phénoménotechnique, il doit être compris comme un système de vie qui donne sens au monde. L'anomalie de la matière noire située dans cette triple analyse peut alors être révélée historiquement et philosophiquement comme un propos culturel sur l'existence du monde. Cette triple analyse permet de mettre en lumière une contingence structurelle au coeur de l'anomalie en elle-même et ainsi, de porter un propos descriptif sur ce qu'est de faire de la science .

Mais cette analyse à plusieurs niveaux de la science telle qu'elle se fait au coeur de la matière noire ne se suffit pas à elle-même pour poser des questions d'ordre ontologique: tout au plus elle dévoile un système d'interprétation du réel, mais comment questionner ce système ?  Ce que je voyais tout d'abord comme problème, c'était celui d'un paradigme peut-être sur le point de tomber à cause d'une opposition déontologique entre deux hypothèses. Mais l'histoire de l'anomalie révèle que cette opposition est fondée sur l'idée de modèle. Afin de comprendre ce que cela signifie par rapport à ce qui peut être appelé le réel , je vais adapter la théorie de la théorie la physique de Duhem tout en y ajoutant le réalisme pratique de Ian Hacking. Je discuterai d'ailleurs du fait que selon Hacking, l'intervention doit être la norme de la vérité, à tout le moins en matière de microscopie. Un tel type de questionnement sur les pratiques de la science peut être complété par la philosophie de l'histoire et Hacking dispose alors d'un usage double: il propose également une méthode d'ontologie historique , qui faisant usage du concept foucaldien du lien entre pouvoir, connaissance et éthique, propose un niveau d'activité réalisant individuel par rapport à des notions plus générales et socialement partagées. Cette théorie complète les analyses de Kuhn qui négligent l'individu. Elle ressemble fortement à la philosophie de la croyance jamesienne[^158] ou à l'analyse existentielle heideggerienne, même si l'auteur se réfère surtout à Foucault. L'ontologie historique de Hacking permet de rappeler que le paradigme a une provenance et qu'il est contingent tout en étant valide car il s'autojustifie. Le problème ontologique de la question Comment sait-on que cela existe ? peut alors être répondu par une analyse philosophique de l'histoire et du raisonnement.

Pour atteindre un point hyperbolique dans l'analyse du problème du mode de vérité proposé et établi par l'anomalie en question, j'utiliserai le *Vampyroteutis Infernalis* de Vilém Flusser[^159], une fiction philosophique à visée épistémologique qui a pour but de contrer l'habitude humaine de l'histoire des sciences et de la logique du savoir en proposant de décrire une science interne, systémique et existentielle d'un habitant des abysses. La méthode de Flusser consiste à contrevenir à tout ce que l'on croit comme universellement vrai à partir de l'humain en examinant le mode de vie culturel du Vampyroteuthis. Une telle étude comparée entre deux animaux différents permet *de facto* de remettre toute culture humaine dans la perspective d'une histoire étendue, c'est un type de réflexivité supporté par une phénoménologie génétique.  L'examen est évidemment une hyperbole, puisqu'une telle connaissance, si elle est hypothétisable par l'imagination, est bien plus difficilement accessible par les faits. Cependant, c'est un moyen efficace d'inciter à la réflexion philosophique sur les manières de faire le monde. C'est sur cette note hyperbolique que je proposerai une synthèse de mon analyse rendant compte de l'ontologie qui a été mise en place pour résoudre l'anomalie de la matière noire.

## Phénoménologie de la recherche par modèles.

Je crois que le niveau de connaissance le plus radical de la recherche par modèles en astrophysique peut être exprimé avec l'aide de la phénoménologie, c'est-à-dire à partir d'une description des pratiques techniques des acteurs. Ce niveau peut être décrit comme un ensemble de réflexes fondamentaux à l'image de ceux que remarquent Marcel Mauss[^160] (1872 - 1950) ou Henri Focillon[^161] (1881 - 1943) : Mauss décrivait les manières de marcher qui sont incorporées dans différentes populations, Focillon l'activité quasi-automatique de la main qui est comme un collègue nerveux de l'esprit. De la même manière, les astrophysiciens sont habitués à la méthode du clavier, de la souris et de l'écran, il sont habitués au système d'exploitations dont ils disposent (le plus souvent Linux Ubuntu), aux librairies dont ils ont besoin, aux navigateurs pour la documentation, aux paquets à télécharger, à des communautés en ligne pour obtenir de l'aide[^162]. Ce sont des irréfléchis fondamentaux de la recherche, sans lesquels n'importe quel projet serait impossible. La main qui sait taper et cliquer, et l'esprit qui sait ce qu'il faut chercher, sont donc mis en mouvement à la manière de réflexes élémentaires afin d'élaborer des connaissances plus élaborées. Ces mouvements sont des réflexes car ils correspondent à un ensemble de signes reconnaissables qui stimulent les pratiques en question - l'habitude fait que le clavier appelle à écrire, l'écran à être allumé, la souris à être appuyée, le logiciel à être utilisé.

Ces types de réflexes automatiques intelligents sont redoublés d'un ensemble de connaissances fondamental purement théorétique que l'on pourrait appeler réflexes intellectuels . Ces réflexes décrivent pour moi des outils indémodables de la recherche et présents dans absolument chaque projet: par exemple le nombre, l'égalité, la quantité, l'équation, la donnée informatique, le spectre, ou dans des cas peut-être plus particuliers, l'oscillateur harmonique, l'algorithme, la longueur focale\... Ce sont des types d'outils conditionnels, qui s'ils ne sont pas présents ne permettent pas de penser: le type d'outils déontologiques et principaux si normaux qu'il n'est pas nécessaire de les nommer pour penser avec. J'y ai eu accès sans formation d'astrophysique ni de laboratoire précisément car j'ai questionné les acteurs qui les possèdent afin de leur demander de me les découvrir, en cherchant à savoir comment ils constituent un modèle. L'ensemble de ces connaissances se combine dans la technique radicale[^163] du modèle, celle qui en permet même la conceptualisation. Un modèle, au niveau du système de recherche individuel de l'astrophysique qui est partagé par la communauté, se montre comme un ensemble de réflexes pratiques (main à dactylographier), pseudo-intellectuels (commencer à chercher en dactylographiant) et intellectuels (un ensemble de notions nécessaires pour chercher).

Le niveau individuel le moins réfléchi de la recherche peut être décrit comme plus qu'un système de réflexes: c'est un système de croyances qui propose une manière de concevoir le monde. William James[^164] et Martin Heidegger[^165] ont en commun d'avoir mis en valeur le partage chez chaque individu d'un système *purement contingent* de connaissances sur ce qui est , sur ce qui peut potentiellement exister d'un point de vue humain. James décrit ce système avec la notion de croyance et fonde sur son sens habituel de crédo, de foi irrationnelle, le fait que tout ce que ce système propose n'a pas plus de sens qu'un autre, et qu'ainsi son niveau de réalité dépend uniquement des niveaux de foi que l'individu distribue indépendamment dans son propre système aux éléments dont il l'a consciemment ou inconsciemment composé. Cependant son argument étant tautologique (la croyance est relative car c'est une foi), il est difficile à recevoir[^166]. Martin Heidegger propose dans ce cas une conceptualisation plus intéressante de la contingence de la connaissance au niveau individuel, qui se fonde sur l'histoire et la philosophie de l'esprit. Selon l'auteur d'*Être et Temps*[^167], l'origine de l'être se trouve chez le *Dasein* (l'être-là ), une définition de l'humain qui met en valeur sa structure propice à recevoir un niveau historique de connaissance grâce à la mémoire et à une culture du legs de la tradition. Ainsi chaque système de connaissance n'est-il qu'une histoire donnée à un individu. La dimension structurelle existentielle, d'ordre neurologique (nécessité de la mémoire) et culturelle (dire de la mémoire) que l'essai permet d'atteindre est selon moi essentielle à prendre en compte afin de compenser le manque de description de l'activité individuelle au sein d'un paradigme dont la *Structure des révolutions scientifiques* de Kuhn fait preuve. En effet, en considérant que les connaissances les plus fondamentales sur ce qui est [^168] sont constituées par le biais d'un legs nécessairement historicisable, la transmission et le travail des paradigmes peuvent être mieux compris: un paradigme n'est jamais que l'ensemble de croyances techniques, réflexes et intellectuelles d'un individu qui lui fait adhérer à un groupe les partageant et qu'il a reçut en s'inscrivant dans une institution et un groupe choisis. De ce point de vue, les prérogatives épistémologiques transmises dans le paradigme sont en même temps relatives à l'histoire en général, et valables au sein du paradigme du fait qu'il n'y a pas de norme absolue de vérité en matière humaine: chaque paradigme se justifie lui-même par l'adhésion de ses participants et l'histoire qu'ils partagent.

Un autre exemple peut permettre de faire comprendre la complexe analyse du point de vue d'un individu saisi dans un système de connaissance qui le dépasse, et qui pourtant n'existe qu'à travers lui. Le baron de Uexküll, dans les années 1930, écrit un livre de phénoménologie et de biologie qui tente de combattre le mécanisme en matière de comportement animal[^169]. Il propose ainsi un exemple fameux, que l'on peut appeler la tique de Uexküll . Il ne décrit pas la tique comme un automate, qui se laisse tomber à la sensation d'une chaleur, mais plutôt comme un ensemble intelligent disposant d'une analyse environnementale réceptive à certains phénomènes signifiants - les autres phénomènes n'étant pas perçus car ils sont pour elle, absurdes. Je pense qu'il peut être utile de penser la chercheuse ou le chercheur d'astrophysique comme une tique de Uexküll complexifiée. Chaque personne de l'astrophysique possède un sens du monde qui lui a été conféré au cours d'une longue formation généralement reconnaissable, quand bien même certains éléments particuliers peuvent échapper à d'autres individus. Par le biais du système de signes dont elle ou il hérite, la chercheuse ou le chercheur reconnaît un ensemble de phénomènes, sensitifs ou intellectuels, comme signifiants, c'est-à-dire comme éveillant une réaction potentielle inscrite en elle ou en lui. Le personnage de l'astrophysique est un *Dasein* dont la signification du monde est organisée par une culture socialement partagée et héritée par la possibilité de l'histoire qui constitue le *Dasein*. Le degré complexe de cette culture individuelle est tel que l'ouverture des potentialités est relative et peut parfois tenir du libre-arbitre; pourtant elle est également contrainte, et cette ensemble de contraintes est ce qui est trouvé par ce que l'on appelle un paradigme de recherche. Le paradigme se révèle grâce à une étude historique centrée autant sur les individus que sur les histoires dans lesquelles leurs connaissances prennent source.

L'activité paradigmatique centrée sur l'individu peut être décrite à son niveau le plus bas comme une connaissance réflexe et déontologique, transmise du fait de la disposition de chaque individu à recevoir une histoire sur l'être et à créer de cette histoire une significativité du monde qui l'entoure.

Cet ensemble de connaissances pratiques internalisées qui sont au fondement de l'heuristique en astrophysique de la matière noire peut être décrit dans un système de connaissance plus large, qui prend le point de vue d'un groupe interagissant avec un ensemble d'instruments.  Cet ensemble peut être qualifié de phénoménotechnique , au sens que lui confère *Le nouvel esprit scientifique* de Gaston Bachelard, en 1934[^170]: le sens que l'instrument est une théorie réifiée[^171]. Ce mot-valise est neuf, c'est un outil de pensée qui désigne le propre de la science de laboratoire, il décrit la manière dont: La recherche \"induit\" des équations (\"noumènes\") l'existence potentielle de phénomènes qu'un dispositif expérimental (\"phénoménotechnique\") doit produire [^172]. Tout au long de l'ouvrage, Bachelard décrit ce caractère du nouvel esprit scientifique de la physique qui est d'établir une dialectique entre l'ordre du nouménal et du phénoménal par le biais d'une technique. C'est une épistémologie de la mesure contrôlée. Dans un même sens, et avant Bachelard, Pierre Duhem décrit aussi pour la première fois en 1906 ce nouveau type de rationalisme dans *La théorie physique: son objet, sa structure*[^173] et il le rattache, plus clairement que Bachelard[^174], à un système de théorisation entier qui guide la pensée scientifique. Il me semble intéressant de se demander si l'astronomie, qui a aussi employé le chemin d'une physique (depuis le début des études spectrographiques) correspond à cette caractérisation attribuée d'origine à la seule physique et alors, de décrire en quoi elle pourrait se différencier. Sur ce but particulier de qualification précise de la structure épistémologie de l'astrophysique, un critère d'analyse réaliste conforme à l'instrumentalisme peut être donné: on le trouve à l'origine dans l'article de Ian Hacking, Est-ce qu'on voit à travers un microscope ? [^175]. Dans cet article précisément, Hacking décrit comment ce qui est réel pour des biologistes ne peut exister qu'en vertu d'une capacité à: 1° comparer différentes observations d'un objet, 2° intervenir sur l'objet au moyen d'un outil qui le transforme.  Il dit ne pas vouloir étendre l'argument au-delà des microscopes, bien qu'il laisse sentir que c'est pour lui la norme générale du réalisme scientifique. Il réitère l'argument d'un point de vue cette fois clairement général dans un autre écrit (Styles pour historiens et philosophes [^176]), ainsi: La cosmologie et les sciences cognitives demeurent des sciences qui représentent; le style de laboratoire a fait naître des sciences qui interviennent [^177]. Je crois que supposer que la cosmologie ne possède pas de moyen d'intervention est complètement faux, bien que ce moyen soit limité et qu'il ne s'emploie d'une manière non-virtuelle qu'en astrophysique[^178], ce que je tenterai de montrer.

Malgré l'étrangeté du terme de phénoménotechnique , l'idée qui se présente derrière est assez simple. Un noumène est une notion issue de la philosophie kantienne qui désigne une pure production de l'esprit, lorsque le phénomène designe ce qui apparaît aux sens. Noumène tire sa racine du *nous*, qui signifie intelligence en grec, et phénomène du grec *phainô* qui signifie apparaître, spécifiquement en astronomie. Il faut donc considérer qu'il y a en jeu une technique qui permet de lier l'intelligence à la phénoménalité, en l'occurrence, chez Bachelard c'est la technique qui crée elle-même le phénomène sur lequel l'intelligence porte. Mais, point légèrement plus complexe, le vecteur de conception et d'herméneutique de l'instrument procède également des idées aux observations[^179]. C'est la caractéristique de la physique de début de siècle, notamment celle qui naît avec la microphysique (qui s'appellerait physique quantique aujourd'hui).

Je crois que le terme est bien plus simple à saisir si on le sort de la physique et qu'on le place par exemple en musique: tout instrument de musique, lorsqu'il est employé, est une phénoménotechnique. En effet, l'instrument de lui-même ne produit rien. Il nécessite une intelligence et des mains derrière, qui possèdent une certaine idée de ce qu'est une structure musicale jouable. De plus, le son produit par l'instrument ne peut pas être indépendant de l'instrument lui-même: c'est donc un phénomène créé par une technique à partir d'une idée du jeu possible.  Enfin, l'instrument a été conçu par un instrumentiste dans le but de produire un son envisageable par un instrumentaliste, ce qui implique d'une part que l'instrument est propice à permettre une forme de notation, de manipulation et d'interprétation pour l'instrumentaliste, d'autre part que sa création artisanale participe très certainement d'une culture instrumentale et musicale qui a déjà formé en ensemble de standards en matière de pratique et de conception d'instruments. De la même manière, l'instrument en physique correspond à un ensemble de théories qu'il confirme par sa conception, dont il ne peut donc jamais être dit indépendant[^180]. Simultanément, et c'est la particularité de l'esprit scientifique que Bachelard identifie, les productions de l'instrument sont une mise à l'épreuve de la théorie.

Dans les collections du Deutsches Museum se trouve un objet pour le moins étrange[^181]: un pyromètre, conçu par l'horloger Michael Moshammer à Regensburg[^182]. La particularité de ce pyromètre, c'est de mesurer l'expansion de la matière de différents bâtons (certains de métal, de bois, de verre) sans indiquer aucune unité de mesure: on ne note que la présence de trois cadrans, l'un assez grand, qui est composé d'un cadran incrusté dans un autre, et le second très petit. Un seul de ces deux cadrans réagit à la pression, le grand. Comme le double-cadran est tiré de l'atelier de Moshammer, c'est en fait un cadran d'horloge, avec des heures et des minutes, et un cadran extérieur numéroté de 1 à 500. Le point intéressant de l'objet, c'est qu'il est ambigu de savoir si l'unité de mesure était implicitement connue, ou si elle était tout simplement inutile. En tout cas, le fait que je désirais posséder une unité de mesure afin de donner un sens à l'objet, est révélateur sur l'état de la phénoménotechnique qu'ont légué les sciences physiques du début du XXe siècle dans les lycées en France, et dans mon éducation.  Cela correspond à ce que Bachelard veut faire remarquer dans la dialectique des instruments: je ne comprends un phénomène créé par la technique que s'il correspond à une mesure; la technique doit donc être conçue telle qu'elle puisse me permettre d'inscrire un nombre, un signe, qui corresponde à une certaine théorie physique, à partir de l'ensemble de phénomènes que crée l'instrument. Et de ce fait, à la fois dans l'origine et dans le but de l'instrument, je retrouve le noumène réconfortant des mathématiques de l'équation qui me rassure dans mon herméneutique des créations phénoménales.

Je crois avec Pierre Duhem que ceci n'est pas seulement le propre de la microphysique, mais des sciences instrumentales mathématisées en général. Aujourd'hui en astrophysique, il me semble que c'est tout à fait valable, et certes d'une manière que Bachelard n'avait pas pu attendre avec le développement de l'ordinateur. La phénoménotechnique de l'astrophysique a ceci de particulier que d'une part, elle n'est pas uniforme mais correspond à plusieurs méthodes de phénoménotechnologie, d'autre part, à ce que le chemin du phénomène au noumène est créé par une informatisation de l'information phénoménale. En effet, que cela soit le cas pour un télescope et sa caméras CCD, ou pour un modèle d'hypothèse, aucune de ces deux techniques n'est indépendante d'une numérisation de la formule ou de la captation analogique.

Le télescope se montre aujourd'hui comme une technique élaborée dans la fin expresse d'interpréter des données numériques. Il déverse ses données souvent hyper-massive[^183] dans un lieu de stockage qu'une personne tierce exporte afin de les traiter à partir d'un code qu'elle propose de lui appliquer. Le télescope n'affiche donc pas un nombre pertinent de lui-même, à tout le moins il ne prend un sens pour la théorie que si ses productions sont épurées par une second théorie, spécialement conçue par rapport au degré de bruit et au nombre d'artefacts remarqués dans sa conception optique, dans son capteur photosensible et dans ses données traitées pour la première fois. Le télescope est donc bel et bien une phénoménotechnique: ses phénomènes ne peuvent être compris qu'à partir de l'utilisation de plusieurs théories (il faut connaître et prévoir son chemin optique en même temps que ses défauts de captation qui sont à épurer), d'autre part, les missions d'observations et même de financement des télescopes sont souvent conçues dans le but d'éclaircir un problème théorique, par exemple de trouver une loi générale à un objet mal connu ou de s'informer sur l'interaction de la matière noire avec différents objets astronomiques.  A l'origine, dans le but et dans les résultats du télescope, il y a donc une envergure théorique qui en fait à proprement dire une phénoménotechnique.

Il en va de même, et d'une manière plus évidente, pour l'idée de modèle.  L'idée de modèle astronomique est aujourd'hui inséparable de l'idée de *code*, or le code est un langage que l'ordinateur interprète à des fins d'opérations logiques. De ce simple point de vue, il est possible de considérer que la modélisation informatique est une phénoménotechnique car les données entrées et de sorties doivent être interprétables pour une personne et sont mathématisées, en même temps que l'effectuation du calcul est entièrement électronique et matérielle.

Ce que je voulais montrer, c'est que l'utilisation d'instruments en astrophysique contemporaine implique l'utilisation de connaissances de bas niveau que j'ai décrit avant: un ensemble de réflexes de pratique et de pensée immédiatement liés à des techniques, notamment celle du codage par ordinateur. Or cette manière de faire peut être comprise grâce à la philosophie de Bachelard comme une phénoménotechnique. Cet ensemble de connaissances phénoménotechniques est également social, puisque d'une part le lieu du laboratoire crée un lieu de sociabilisation fermé dans lequel des connaissances sont partagées qui permettent de faire de la recherche, auxquelles on n'accède pas nécessairement facilement si l'on sort du laboratoire (il y a des connaissances tacites), et d'autre part car les télescopes ne sont pas des techniques créées par une seule personne pour une seule personne, mais le produit de grands groupes organisés pour d'autres groupes organisés. Il y a donc nécessairement un partage socialisé de techniques et d'idées instrumentales qui se produit en astrophysique, à la fois à un niveau local et international. Par exemple, lorsque j'ai voulu utiliser RAMSES, j'ai dû apprendre plusieurs choses: il y a des librairies de code typiques pour faire des graphs et organiser des visualisations des données résultantes; il y a des sites internet spécialisés que j'ai à connaître afin de répondre à certaines questions que d'autres gens se sont certainement posés avant moi afin de faire fonctionner le programme. Mais spécifiquement pour RAMSES et l'astrophysique, je n'aurais pas pu apprendre ces choses autrement qu'en demandant à l'un de mes directeurs de stage, qui était tout à fait habitué à manipuler ces outils. Il est donc possible de dire que l'astrophysique comporte une sociotechnique de la recherche propre à chaque laboratoire, qui complète ses phénoménotechniques.

Cette ensemble phénoménotechnique est lui-même relié à une échelle d'hypothèses qui permet d'envisager des expériences . Transposer le système de théorisation physique que Duhem remarque permet ici de comprendre comment la théorisation astrophysique est envisagée. En effet chez Duhem, à l'origine de toute théorie (ce n'est pas nécessairement l'origine chronologique, mais plutôt l'origine dans l'ordre de vérification) se trouve la mesure. Duhem possède une notion de l'instrument très proche de celle de Bachelard, il considère en effet que l'instrument est conçu afin de produire certaines valeurs symboliques en général données sous la forme de nombre. A partir du nombre, des équations locales sont données par les physiciens expérimentaux. Le théoricien reprend ces équations et les relie à des théories mathématiques de plus large niveau, à des fins d'éclaircissement, de simplification ou de prédiction. Il ne me semble pas que la scission entre la théorisation expérimentale et la théorisation abstraite soit très prononcée en astrophysique, la plupart des chercheuses et chercheurs que j'ai rencontrés maîtrisaient eux-mêmes des concepts généraux de thermodynamiques, de relativité générale ou de physique quantique qui sont au centre de leurs recherches particulières, et dans le cas précis de la matière noire, les lois du mouvement de la relativité générale peuvent être questionnées en elle-même. Mais ce que Duhem permet de concevoir, c'est le lien fort entre l'instrument et l'hypothèse abstraite. En effet, si l'on considère que tout instrument est une phénoménotechnique, à laquelle appartient donc par nécessité un niveau de théorisation, alors l'hypothèse d'astrophysique est conçue *en concordance* avec l'instrument employé, ce qui signifie qu'aucune hypothèse n'est indépendante des instruments desquels elle est tirée. Et je crois que l'on décèle là le réalisme propre à l'astrophysique, qui est de fait exactement le même[^184] que Bachelard découvre en microphysique: il appelle cela un rationalisme réalisant . La propriété du rationalisme réalisant est de donner un caractère de vérité au phénomène créé par un instrument uniquement si ce phénomène peut être incorporé dans une hypothèse d'aspect mathématique. Somme toute, n'est réel que ce dont on peut donner raison. A ce titre, il faut considérer que la matière noire, en tant qu'exploration d'hypothèses diverses et particulières ramenées sous quelques patrons généraux et conflictuels, est sur le chemin de la réalisation d'une perception. La manière dont cette perception va être réalisée **ne dépend en aucun cas des phénomènes à eux seuls car ils ne disposent pas d'un sens en eux-mêmes**. Une perception est toujours réalisée par une phénoménotechnique qui interagit avec un ensemble technicisable d'hypothèses, et ultimement avec un système de représentation de l'espace et du temps des objets astrophysiques, qui ne dispose d'aucune réalité sans opérer cette dialectique nouméno-phénoménale. Cela veut dire que les phénomènes produits par les instruments ne sont interprétables qu'en fonction d'une théorie capable d'en rendre compte et dans le système de vérité particulier qui a été élaboré en astrophysique, un phénomène sans explication donnée par une équation prédictive ne dispose pas d'une valeur de réalité suffisante. Un tel phénomène n'est pas compris et il crée un désir insatiable (ou devrais-je dire, un désir paradigmatique) de rendre raison des phénomènes qui doit être assouvi.

Je pourrais donner l'impression qu'en plaçant l'astrophysique comme un système de connaissance relativisé par son mode particulier de pensée (rationalisant) et de vérification (phénoménotechnique), je veux dire que les êtres que le système propose comme existants, comme réels, n'ont en fait aucune réalité car ce sont des créations culturelles. Ce n'est pas exactement mon point de vue, tout ce que je considère, c'est qu'aucun être n'est éternellement réel, car la valorisation et la qualification du réel est relative à un paradigme de recherche. Mais il suffit d'être convaincu par les raison d'y croire, pour admettre ces objets comme réels. Ian Hacking a parlé spécifiquement de ce type de réalisme rationalisant tel qu'il apparaît d'abord en microscopie biologique[^185]: selon lui est réel ce qui peut être comparé sous l'oeil de divers instruments et ce qui peut être intervenu dessus. Il étend cet argument en-dehors de la biologie et dit que la cosmologie n'est qu'une science d'observation, non d'intervention: La cosmologie et les sciences cognitives demeurent des sciences qui représentent ; le style de laboratoire a fait naître des sciences qui interviennent [^186]. Je crois que c'est une parole facile qui reprend des doutes surannés sur la valeur positive de la cosmologie qui étaient présents (cette fois à juste titre) en début de siècle et qui simplifie le sens de la représentation en cosmologie. Effectivement, au début des années 1900 la cosmologie n'était qu'une théorisation peut confirmée d'une manière instrumentale, puisque des objets définis comme extra-galactiques ne sont apparus qu'avec Hubble en 1920. Mais je ne crois pas que l'opinion puisse se tenir aujourd'hui. D'une part, la multi-instrumentalité est aujourd'hui tout à fait permise et même de mise en astrophysique[^187]. Il est devenu habituel de comparer différents spectres afin d'augmenter la valeur de réalité et la compréhension d'un objet, notamment car cela permet d'obtenir des informations sur sa composition, différents spectres d'émission correspondant à différents éléments chimiques. Il est donc possible de comparer des spectres qui retrouvent une même forme d'objet, et la diversité du spectre confère une valeur explicative d'un point de vue chimique, ce qui est un bon outil de prédiction ultérieur et ce qui augmente la valeur convaincante de l'argument puisque l'observation rentre dans un second paradigme, celui de la physique. Outre cette dimension comparative, Hacking met en valeur l'idée d'intervention: c'est une modification de l'objet qui est observé par le biais d'un instrument qui convainc l'observateur que cet objet est réel . Mais il me semble que la critique est trop facile si l'on ne considère pas les subtilités de l'observation télescopique et modélisée à l'oeuvre en cosmologie. En effet, je crois que dire d'une part, que c'est une science de représentation, ne dit rien sur ce qu'observer signifie (et Hacking ne développe rien au-delà), d'autre part, qu'observer en astrophysique, ce n'est pas voir comme un grec d'il y a 2500 ans, et même dans ce cas, ce n'était pas une activité si naïve, puisque plusieurs théories sur l'apparence du monde et le mouvement des astres avaient déjà été émises. Bref, l'observation en astronomie est réalisée par le biais d'une phénoménotechnique, et je pense qu'il est possible de déceler dans la technique optique une forme d'interaction très similaire à une intervention. De surcroît, je pense que la phénoménotechnique astrophysique convient parfaitement à la définition du style de laboratoire que Hacking donne avant sa parole lapidaire: le style de laboratoire. Il se caractérise par la construction d'appareillages destinés à produire des phénomènes dont la modélisation par hypothèse peut se révéler vraie ou fausse, mais qui utilise un autre échelon de modélisation, à savoir des modèles de la manière dont les appareils et les instruments eux-mêmes fonctionnent [^188]. Certes, ce qui est vu, ce n'est pas une cellule sur laquelle peut opérer un scalpel. C'est plutôt un chemin électromagnétique réfléchi et transmis par un ensemble d'optiques déviantes et convergentes. Le chemin, ensuite, crée une interaction avec un capteur. Cette interaction analogique est elle-même convertie en valeur numérique, puis cette valeur est conservée (afin d'être traitée ultérieurement). Il me semble que cet ensemble de transformations n'est pas opérée sur du vide, ni ne crée à chaque étape son propre phénomène *ex nihilo*: c'est bel et bien une transformation d'une onde électromagnétique, donc, le télescope intervient sur l'objet avec lequel ses éléments optiques interagissent, et en connaissant l'ensemble des étapes d'interventions, il est possible de considérer que l'objet est réel du fait de sa transformation. Certes cependant, la différence entre la biologie de laboratoire et l'astronomie de laboratoire, c'est que l'objet visible est second en astronomie: l'on intervient sur des ondes électromagnétiques, mais non sur l'objet les produisant. Je ne pense pas que cela retire le fait qu'il y a une manière d'intervenir sur ces êtres seconds que sont les ondes électromagnétiques émises ou réfléchies par des sources lointaines, et de plus c'est le schème-même du style de laboratoire qui a lieu dans l'utilisation du télescope. J'accorde que la modélisation informatique est une représentation car elle n'intervient pas sur des phénomènes captés, mais sur des phénomènes produits électroniquements qui prennent origine dans une technique créatrice, même s'il serait possible d'étendre la notion d'intervention à la manipulation de fictions, puisqu'un même modèle peut être transféré entre plusieurs instruments: ce qui permet de croire qu'il est réel car il est observé de plusieurs manières. La comparaison de modèles modifiés introduit d'ailleurs une idée d'intervention. Mais ce à quoi je m'oppose tout simplement, c'est en même temps à l'idée de science représentative qui ne signifie pas grand chose sans prêter attention à la technique de l'observation, et en même temps à l'idée que l'intervention *immédiate* devrait être la norme de toute épistémologie réaliste. La microphysique et l'astrophysique sont réalistes du fait du rationalisme réalisant qui est le leur et de la manière dont il est appliqué par des techniques; ce sont d'autre part des sciences d'intervention indépendamment du fait que leur objet n'est pas premier mais second[^189], ce qui peut mener à penser qu'il est représenté[^190]. Mais la cosmologie ne se limite pas à cette activité de fiction prédictive.

Jusqu'ici, la philosophie de la matière noire a été montrée comme une phénoménotechnique ayant créé des dispositifs pratiques et notionnels réflexes dans les individus. Cette phénoménotechnique ouvre la possibilité d'élaborer des hypothèses car elle matérialise des théories.  La phénoménotechnique permet donc à la chercheuse et au chercheur de s'inscrire dans un large cadre de pensée, de théorisation et d'application technique, ce que l'on peut appeler: un paradigme. Je vais développer cette notion élaborée par Thomas Kuhn, car elle comporte en son centre l'idée d'anomalie de paradigme, et je pense que cette idée est nécessaire pour interpréter le problème de la matière noire tel qu'il se montre. De plus, je trouve que Thomas Kuhn propose sa théorie philosophique de la structure de l'histoire des sciences expérimentales comme une analyse historique fondée sur un ensemble de cas similaires, dont il reconnaît pourtant la particularité respective. Il me semble alors que la structure historique qu'il découvre prend son fondement dans une forme de probabilisme historique qui, par la généralité de la structure, permet d'adopter un point de vue de régularité dans l'histoire, ce qui est une forme de prédiction. De surcroît, Bachelard, Kuhn et Hacking qui sont chacun historiens et philosophes des sciences, proposent tous une conception disruptive de l'histoire des anomalies et révolutions scientifiques d'une manière relativement indépendante[^191], en utilisant des concepts différents, et pourtant en produisant une idée tout à fait similaire, ce qui me mène à penser que l'hypothèse de Kuhn est probablement correcte et que c'est à tout le moins un concept utile.

Un paradigme kuhnien est un concept d'analyse historique qui permet de décrire une situation où il est possible de découvrir un réseau serré d'impératifs - conceptuels, théoriques, instrumentaux et méthodologiques [^192]. Kuhn présente le paradigme comme susceptible de deux mouvement différents qui se répètent[^193]: le progrès cumulatif de la science normale, et le changement disruptif de la révolution. Le paradigme de Kuhn est un outil de travail qu'il utilise pour reconnaître un système de connaissance et la manière dont il peut être mis à l'épreuve et ainsi être mené à changer. Comme il s'intéresse principalement aux sciences physiques, on retrouve l'importance de la mesure et des lois mesurées.  Selon Kuhn, plus un paradigme est élaboré, plus il est à même de remarquer ce qu'il n'est pas en mesure d'expliquer[^194]. Les paradigmes sont souvent confrontés à des énigmes , mais les énigmes ne sont pas spécifiquement disruptives. Elles sont résolues en général par les règles normales du paradigme, elles appellent parfois l'instrument et la théorie à être précisés (pensons par exemple à la découverte de Neptune par Le Verrier, qui ne nécessitait pas une nouvelle théorie, mais seulement une bonne prédiction mathématique pour expliquer une trajectoire imprévue). Cependant, les énigmes insolubles, qui résistent aux tentatives successives de solutions normales, permettent de faire émerger des solutions alternatives se détachant progressivement des règles habituelles du paradigme. On appelle ce type d'énigme une anomalie . Les anomalies entament ce qu'on appelle une crise : c'est-à-dire une aliénation du paradigme pour résoudre le problème qu'il a fait émerger. Les crises n'abolissent pas la science normale par leur seule présence mais elles multiplient les pistes de recherche dans des voies anormales à mesure que les hypothèses normales sont réfutées[^195]. Je pense que la matière noire s'inscrit dans ce type de problème: je pense que c'est une anomalie. Un bon indicateur qu'une crise a lieu est justement la démultiplication des théories alternatives afin de résoudre l'anomalie, or aujourd'hui, autant dans la voie normale que dans la voie ésotérique, les hypothèses sont très nombreuses: il y a celles qui ont été réfutées définitivement, par exemple MACHOs, il y a la voie normale avec beaucoup de cas particuliers et toujours difficiles à tester[^196], les WIMPs, et il y a la voie définitivement anormale, les différentes théories de la modification de la gravité (précisément portant sur la second loi du mouvement de Newton), MOND, et il existe très certainement des hypothèses plus exotiques, par exemple la *phantom dark matter*, ou encore des théories à mi-chemin entre la normalité et l'anormalité. Il me semble que la diversité des solutions proposées, dont une part s'éloigne consciemment du paradigme habituel, indique qu'une anomalie est présente et qu'une révolution est en cours. Kuhn propose justement trois types de phénomènes susceptibles d'entamer une révolution, dont les deux derniers types sont les plus intéressants: ceux qui appellent à rendre la théorie plus précise, et ceux qui résistent irrémédiablement aux paradigmes existants[^197]. Il me semble difficile de décider si la matière noire est du second ou troisième type, étant donné que la voie normale est encore empruntée aujourd'hui.  Mais la voie alternative étant aussi employée, il semble définitivement juste de dire qu'à tout le moins il y a un paradigme en crise en astrophysique, et que cette crise a été entamée par la matière noire. En effet, elle donne lieu à une suite d'hypothèses essayant de rendre compte de son phénomène caractéristique, mais les voies employées restent difficiles et infructueuses car imparfaites.

La valeur instructive de se savoir être dans un paradigme en crise c'est qu'il est apparemment totalement imprévisible de savoir si c'est la voie normale ou la voie aliénée qui est la bonne, mais seulement de constater que l'hypothétisation faillit. Il n'y a donc aucune raison de préférer participer au paradigme normal que d'envisager contribuer à la proposition d'un paradigme différent, sinon par conservatisme. Une telle position, cependant, fait se poser de nombreuses questions d'ordre ontologique: si une théorie alternative est employée, qu'est-ce qui indique que si une communauté avait préféré persévérer quelques années supplémentaires dans la voie normale, elle n'aurait pas trouvé une solution également, et qui plus est proposant d'autres choses existantes ?[^198]. Il me semble que la possibilité de comparer les voies de solution du paradigme grâce à l'histoire mène à une forme de relativisme qui contrevient totalement à l'idée habituelle que l'on se fait du progrès scientifique. Que l'on soit baconien, pascalien ou duhémien, le progrès a toujours semblé être un amassement de connaissances, dont certaines peut-être ont été abandonnées, mais qui gagne en précision, et de ce fait, qui avance . Pourtant, ce n'est pas ainsi que se présente l'histoire des sciences selon Kuhn. Dans la vision qu'il en donne, il n'y a pas de notion de progrès continu à grande échelle. Chaque paradigme individuel avance dans ses propres gonds, mais lorsque ces gonds succombent, le paradigme change. Il n'y a pas eu d'avancée majeure par rapport au paradigme précédent, du fait qu'ils sont difficilement comparables: Kuhn ajoute Il en résulta, une fois de plus, un nouvel ensemble de problèmes et d'exigences \[\...\] Ces déplacements caractéristiques des conceptions d'une communauté scientifique, en ce qui concerne ses problèmes et ses normes légitimes, auraient moins d'importance pour le point de vue soutenu dans cet essai si on pouvait supposer qu'ils se sont toujours produits dnas un sens ascendant, sur le plan méthodologique . La marche de la connaissance n'est organisée que par la valorisation sociale et donc entièrement relative des problèmes à résoudre. Si donc une option à la matière noire était choisie, ce ne serait pas nécessairement pour ses vertus ontologiques (il n'est pas forcément plus pertinent de savoir si c'est une particule ou un défaut de la loi plutôt que de savoir si la solution proposée est simple et élégante, ou même complexe mais à tout le moins paradigmatique) et ce n'est relatif qu'à une conviction progressive selon laquelle une certaine option serait plus fructueuse que toutes les autres pour résoudre le problème; d'autre part, l'option parallèle serait abandonnée et c'est là la tragédie nécessaire si l'on conçoit la science telle qu'il ne semble pas possible d'avancer sans règles définies, unifiées et partagées communautairement (mais cela n'est peut-être qu'un autre paradigme, qui impose à la connaissance une norme de de contraction et de rectitude partagée[^199]. L'interprétation kuhnienne de l'histoire des sciences est retrouvée quasi-identique chez Bachelard et Hacking.  Bachelard considère que la science nouvelle , celle de la microphysique, est un système phénoménotechnique organisé de réalisation rationnelle, c'est-à-dire que le système a pour but par un moyen instrumental de rendre réelles des idées mathématiques. Ces idées se trouvent dans une axiomatique qui détermine les expériences possibles[^200]. Cette axiomatique est générale et discutée de plusieurs manières par les chercheurs[^201]. Il arrive toujours un moment où elle faillit[^202]. La manière dont elle est surpassée, c'est par une révision complémentaire, qui simplifie les principes généraux plutôt que de complexifier les cas particuliers. Ainsi le passage de la géométrie euclidienne à la géométrie non-euclidienne est-il organisé par une résorption du théorème euclidien qui ne fait de lui qu'un cas particulier d'une théorie plus générale; il en serait de même pour la physique de l'espace galiléen passant à celle de l'espace einsteinien[^203]: le référentiel galiléen n'est plus qu'un cas particulier des référentiels gaussiens ou riemanniens de la relativité générale. Bachelard conçoit similairement à Kuhn qu'un paradigme est une organisation méthodologique théorético-instrumentale qui progresse normalement (malgré des discussions internes) jusqu'à faillir et nécessiter une révision. La différence principale, c'est que Bachelard voit une forme de progrès subsumant dans l'élargissement de l'axiomatique, mais Kuhn marque davantage l'incommensurabilité du paradigme ancien et du paradigme révisé. Je partage l'avis de Kuhn que les changements de principes impliquent un changement du raisonnement et donc du rationalisme réalisant, c'est-à-dire que la matière noire-particule et la matière noire-loi ne disent pas la même chose sur le monde dans le propre système de croyance qu'elles élaborent. De fait, deux paradigmes ne peuvent être égaux lorsqu'ils sont interprétés. Quant à Hacking, il propose un concept similaire à la dialectique du non de Bachelard et aux paradigmes de Kuhn: il appelle cela un style de raisonnement . Il développe les principes de cette idée dans sa Leçon inaugurale au Collège de France , dans Style pour historiens et philosophes et Ontologie historique [^204]. La Leçon de Hacking donne une idée assez générale du concept: je prétends que chaque style introduit, en matière de preuve et de démonstration, son propre type de critères, et qu'il détermine les conditions de vérité propres aux domaines auxquels on en vient à l'appliquer [^205], il rajoute que chaque style se justifie de lui-même et propose le concept d'auto-justification: elle renvoie à la théorie vérificationniste de la signification \[\...\] L'auto-justification, loin d'impliquer une espèce de subjectivisme, joue un rôle proprement fondamental pour l'objectivité et la reproductibilité scientifiques.[^206]. Il me semble que l'on commence à retrouver un concept auquel Kuhn et Bachelard nous habituent, c'est-à-dire l'idée qu'une science forme un système de connaissance clos qui comporte sa propre méthode vérification. En ce sens, le système est incommensurable à un autre, puisque la vérification n'est pas transverse à plusieurs systèmes, ce qui rejoint l'épistémologie des paradigmes kuhniens, d'autre part, la théorie vérificationniste de la signification stipule que le sens n'est conféré que par une méthode vérification, donc la phénoménotechnique qui est au centre des paradigmes rejoint également le concept de Hacking et permet de justifier du fait que le système n'est pas arbitraire mais qu'il est fondé par une méthode de vérité. Chaque style apporte une certaine structure caractérisée par un nouveau domaine d'objet , une nouvelle classe d'objet , et un débat réalisme/antiréalisme qui leur est propre[^207]. Il me semble que c'est applicable au dilemme de la matière noire qui, faisant partie du paradigme de l'astrophysique, a introduit historiquement un domaine d'objets invisibles, parmi lesquels la matière noire actuelle n'est qu'un cas particulier (poussières d'étoiles et MACHOs furent d'autres matières noires). Ce cas a également introduit une nouvelle classe d'objets en devenant l'exemple particulier et caractéristique de la matière invisible élusive et peu interactive (WIMP) voire illusoire (MOND), enfin en opposant ces classes et en défiant en même temps tous les dispositifs instrumentaux jusqu'à ce jour employés pour tenter de la détecter, elle introduit forcément plusieurs débats sur sa nature réaliste ou irréaliste: est-ce une illusion, est-elle mal prévue, est-elle probable, est-elle réelle si elle est prédite mais indétectable, l'hypothèse explicative *ad hoc* dispose-t-elle d'autant de réalité que l'hypothèse purement descriptive, décrire est-ce de l'*ad hoc*, qu'est-ce qui fait le réalisme d'un objet astrophysique ? Bref, l'étude des styles de raisonnement qualifie l'étude de l'apparition de manières de penser et d'introduire de nouveaux objets, et il me semble que cela peut également qualifier l'effort de mon présent essai et ceux réalisés par Bachelard et Kuhn. Ce projet d'histoire par style de raisonnement est davantage précisé dans Style pour historiens et philosophes , qui met en avant le caractère indisciplinaire du projet et se concentre en même temps sur le style de laboratoire. Ce style sus-mentionné est qualifié par la modélisation du fonctionnement des instruments employés à des fins de production de phénomènes, donc plus simplement, par une connaissance consciente de la phénoménotechnique employée dans la fin de critiquer l'instrument afin d'étavlir la véracité de l'évènement produit. Hacking redéfinit la liste des nouveautés apportées par l'émergence d'un style: Chaque style de raisonnement introduit un grand nombre de nouveautés qui comprend des types nouveaux:\ D'objets\ De preuves factuelles\ De phrases, de nouvelles manières susceptibles d'être vrai ou faux\ De lois, ou tous cas d'assertion modales\ De possibilités [^208]\ Il dit s'inspirer du positiviste logique Ludwig Fleck (1896 - 1961), de l'historien A.C. Crombies (1915 - 1996) et de Michel Foucault (1926 - 1984). L'on trouve dans le dernier article (Ontologie historique ) le point intéressant que ces notions au coeur du style de raisonnement (différents objets, techniques, méthodes, phrasés) ont toujours une influence dialectique et tripartite qui reprend les trois axes d'analyse de Foucault dans ses diverses archéologies du savoir et analyses de la biopolitique: trois axes de Foucault: savoir, pouvoir et éthique [^209].  Je crois que cette prise en compte permet d'appuyer sur le fait qu'un paradigme est extrêmement formateur vis-à-vis de ses participants: il influence simultanément les connaissances dont ils disposent, le pouvoir que ces connaissances leur confère (par exemple celui d'intervenir) et l'éthique qu'il leur oblige (certaines choses sont permises et d'autres non en vertu de normes épistémiques, par exemple l'observation et la modélisation sont limitées par des ressources techniques que seules certaines conditions permettent de monopoliser). Si je n'ai pas eu l'occasion de me plonger dans des considérations logiques à la manière dont Hacking propose de le faire (analyse des modalités, structure des phrases, types de classification) car j'emploie une histoire plus large (rétrospective) et informelle (anthropologique), qui s'intéresse plutôt aux pratiques de pensées et de théorisation et aux techniques qu'à la logique des mots, en revanche je suis tout à fait d'accord avec l'affirmation que les possibilités de vérité, et donc de découverte, sont elles-mêmes façonnées dans le temps [^210]. Il me semble que cela résume le principe selon lequel tout système de connaissance est autojustifiant et que cela exprime bien l'idée que la vérité est *de facto* une notion relative au cours de l'histoire, ce que je trouve se confirmer dans des cas particuliers tels que celui du système amérindien du cosmos, de la recherche romantique de Plücker, et surtout et avant tout, de la création d'un être dans le moment dilemmatique de son apparition, ce qui est le cas pour l'astrophysique de la matière noire.  Pour moi, la matière noire met en valeur le moment exceptionnel où une notion n'est pas encore devenue paradigmatique et de ce fait, où un propos potentiellement ontologique est suspendu entre l'être et le néant, ce qui permet d'explorer tout le chemin habituel employé par la science normale pour résoudre des énigmes et de le comparer à des chemins de traverse esotériques.

Les concepts philosophico-historiques de Kuhn, Bachelard et Hacking permettent d'élaborer une image de l'activité scientifique qui rend compte de la possibilité pour l'anomalie de la matière noire d'être scindée en deux types de propositions qui contiennent des propos ontologiques opposés. En effet, si je résume ce que j'ai montré depuis le début, ce problème d'astrophysique est pris dans un système uni aux caractéristiques théoriques, méthodologiques, instrumentales qui lui sont propres et qui se justifient elles-mêmes car il y a un cycle fermé de la théorie à la matérialité; il possède donc ses propres objets et ses propres moyens de vérifier son existence; dans la méthodologie, la phénoménotechnique du télescope et de l'informatisation du modèle sont essentielles. Faire l'hypothèse que la matière noire est une particule, ou n'en est pas une, c'est alors proposer un modèle informatique dont le résultat de simulation pourrait trouver une confirmation dans des interactions télescopiques prédictibles, déjà observées ou à observer.  Organiser une observation requiert un traitement des données, un accès distant à une base de données traitées ou le lancement d'un projet qui peut faillir selon le facteur pression du télescope envisagé. S'il est possible de proposer que la matière noire est une particule, c'est parce que l'astrophysique, par le biais des analyses chimiques du spectre lumineux, est liée au paradigme standard de la physique. Ce paradigme a été longtemps fructueux, ses règles de prédiction offraient encore le boson de Higgs il y a quelques années. Il y a donc des raisons d'y accorder confiance et de l'utiliser pour hypothétiser, mais ce n'est pas le seul moyen envisageable. En effet, les échecs continus à détecter une particule prédite tout autant qu'à modéliser la particule qui permet de s'accorder avec l'ensemble des phénomènes anormaux (qui s'étendent de l'échelle astrophysique à l'échelle cosmologique) font, comme permet de le prédire Kuhn, diverger une partie de la communauté vers une heuristique aliénée qui vise à réviser l'un des principes remis en cause par le phénomène interprété dans le paradigme normal: nommément l'idée que le mouvement des galaxies et d'autres objets massifs est correctement décrit par l'ensemble des instruments qui permet de les observer. Cette voie n'est pas aussi déviante qu'on peut l'imaginer, elle conserve par exemple l'isotropie de la lumière comme principe de recherche et n'invalide pas l'information fondamentale ni la méthode de recherche par modèles. Si ces deux hypothèses se retrouvent dans le même lieu (le laboratoire d'astrophysique) et dans le même temps, c'est parce qu'elles sont tirées du même paradigme. En fait, elles ne sont opposées d'un point de vue ontologique que sur une infime partie de toute l'ontologie que propose le paradigme de l'astrophysique. Elles sont également opposables précisément car elles sont liées au même ensemble de connaissance.

La création d'une particule ou la révision d'une loi et ainsi la révision de la compréhension du monde de celles et ceux concernés par l'astrophysique ne peut donc se faire et s'opposer dans ses hypothèses que sur un fond théorique et technique commun que seule l'histoire, et non pas la pure philosophie, permet d'éclairer. Il faut chercher dans le passé, dans les pratiques des hypothétisants: il faut inspecter cette activité prédictive et heuristique pour saisir le pourquoi de comment il est possible qu'une connaissance puisse être créée ou annulée en optant d'en déconstruire et reconstruire une autre.

Je trouve que cette possibilité a été ouverte par une phénoménotechnique, un style de pensée, un paradigme, en somme par une méthode de dialectique phénoménale-nouménale socialement partagée et socialement close, cyclique dans l'interaction à tous ses niveaux (théorique, pratique, interprétation des interactions instrumentales), dont seule la fermeture forte permettait de lier les principes théoriques et prédictifs en les matérialisant dans un système instrumental, ainsi laissant place à l'imagination des choses pouvant exister par la mesure; je trouve que cet ensemble de connaissances est situé historiquement, et qu'il n'aurait pu apparaître sans la valorisation égale du platonisme, de la géométrisation, de l'algébraïsation de la géométrie lors de la révolution de la géométrie euclidienne, de la création de phénomènes par techniques de laboratoire, de la possibilité de la symbolisation de ces phénomènes par des instruments de mesure, de la modélisation d'instruments pour retracer l'histoire de leurs phénomènes, de la géométrisation et mathématisation des modèles, et d'instruments de modélisation automatiques dont le calcul mais plus rarement la compréhension peut dépasser la cognition humaine; je trouve que seule cette concrétion historique parfaitement contingente et pourtant déterminée par d'anciens paradigmes (le platonisme antique est mort, mais le désir noétique demeure)[^211], a laissé la voie à une faille qui s'est ouverte en divers chemins exploratoires visant à sauver une part de ces principes. Il n'est pas possible de savoir quelle voie sera empruntée et finalement conservée comme la voie royale, celle qui serait apte à résoudre des énigmes ultérieures, je ne saurais dire non plus si de nouvelles voies ne seront pas ouvertes à mesure que les chemins actuels seront réfutés, en revanche je peux dire que toutes les hypothèses obéissent au même principe, à savoir: d'être vérifiées (ou réfutées) par un moyen technique qui engendre son propre phénomène lequel est à interpéter en langage mathématique, et d'être élaborées de la même manière par des simulations qui sont des phénomènes électroniques et virtuels prenant l'apparence de la donnée codifiée mais disposant d'une représentatibilité de lois interprétable grâce à la littérature et aux données d'observation. C'est cet ensemble de raisonnements communs qui fait que la matière noire semble ontologiquement multiple, cela permet qu'elle pose un problème épistémologique de la nature du dilemme. Cette nature d'opposition épistémologique ne se montre que part l'étude suffisante du système. En fait, elle transparaît également pour les praticiens: les chercheuses et chercheurs des hypothèses particulières du dilemme sont très conscients que lorsqu'ils font un modèle, ils emploient une heuristique particulière qui se différencie de l'autre et dont les résultats n'auraient pas la même signification, car ils mettent certains principes en jeu. Cependant, c'est pour eux l'activité normale de la recherche et ils n'en sont pas si surpris. Posséder un point de vue extérieur permet à tout le moins de s'interroger sur la manière dont le réel est créé et de saisir selon quelles conditions cela se fait, ceci d'un point de vue plus large, moins habituel, moins incorporé que pour la personne qui se situe à l'intérieur du paradigme. Il n'y a qu'à penser à comment des enfants encore ignorants de tout nous posent de temps à autres de grandes colles sur la manière dont tel ou tel savoir si normal a été trouvé. Ils seraient pour leurs parts de meilleurs investigateurs de paradigme car ils ne prennent pas ces connaissances sur le mode de l'évidence, ils sont d'ignorants mais méthodiques questionneurs.

Pour mettre en perspective la logique anthropologique et même anthopomorphique qui est en jeu, je pense qu'il est possible d'utiliser la fiction, qui permet de trouver justement ce *questionneur* hyperbolique qui ne saurait rien de la culture humaine astronomique et serait ainsi davantage neutre sur les conditions de possibilité du problème de la matière noire.


![image](/img/vamp.jpeg) Vilém Flusser est un philosophe polyvalent[^212] d'origine tchèque qui a vécu au Brésil et en France, il fait paraître son ouvrage *Vampyroteuthis infernalis*[^213] la première fois en allemand[^214] en 1987. La principale motivation de l'ouvrage consiste à se défaire des catégories trop humaines des sciences biologiques de l'évolution, qui participaient à un anthropomorphisme fort, que l'auteur voit autant chez les lamarckiens que chez les darwiniens. Il conteste le sens des espèces et de l'évolution qui vont s'éloignant d'un centre unique, l'humain. Il conteste les possibilités d'éloge que l'on se fait à nous-même en concevant un système prétendûment objectif. Pour combattre cette forme d'égocentrisme qui imprègnerait pourtant des systèmes d'analyse qui se disent objectifs , il propose de se transcender en considérant le système de connaissance d'un animal différent de nous. Ce serait alors un sain effort pour faire tomber du trône qu'ils se sont érigés les analystes du vivant, et pour former une science biologique plus légitime et qui prendrait mieux en compte les particularités de la vie, sans ne faire qu'établir un contraste négatif et appauvrissant d'avec l'humanité. C'est là le point de vue de l'auteur mais ce n'est pas nécessairement le mien, ce qui m'intéresse précisément, c'est cet effort donné à la description d'un système de connaissance d'un animal particulièrement différent de l'humain. L'essai a un défaut principal, celui d'invoquer régulièrement une forme d'évolutionnisme vitaliste qui en fait n'ajoute rien à l'argument de comparer deux épistémologies grâce à la fable biologique. En tant qu'humains, il n'est pas possible de n'être pas humains et de ne pas penser comme des humains, et d'ailleurs nature et culture ne faisant qu'un, il est difficile dans l'héritage positiviste de la science de ne pas penser par la lunette et le télescope, par la phénoménotechnique et l'épistémologie des mathématiques et géométries, même en-dehors d'un paradigme de laboratoire. Mais l'emploi de l'imagination et de l'analyse biologique permet de créer une fiction convaincante dans laquelle il est possible pour un temps de prendre au sérieux la vie intellectuelle, sensible, culturelle d'une autre espèce. L'auteur choisit cet animal pour ses grandes différences biologiques d'avec l'humain, ce qui limite les possibilités d'anthropomorphisation.

Selon Flusser, le vampyroteuthis a la particularité de vivre dans les profondeurs, de produire sa propre lumière, de pouvoir modifier l'apparence de sa peau, de posséder plusieurs bras, de respirer, faire battre son coeur et se propulser en même temps, d'avoir un système nerveux dédoublé plutôt qu'une seule colonne, de sentir la teneur en carbone et la salinité de l'eau, d'être sensible aux ondes électromagnétiques[^215], de pouvoir se laisser flotter au courant, et tout simplement, de vivre dans un milieu sans lumière, à haute pression et avec une viscosité différente. C'est un animal social, mais violent envers les autres espèces. Il dispose pour trait commun avec l'humain d'être un cœlomate, c'est-à-dire de disposer d'un ectoderme, d'un mésoderme et d'un endoderme et d'une géométrie du mouvement qui n'est pas bilatérale mais plus que cardinale (haut, bas, arrière, avant, gauche, droite). Cependant, si l'on considère sa culture, sa manière de pouvoir interpréter le monde conditionnée par la biologie, l'analogie s'arrête à son caractère de cœlomate se mouvant dans un espace tridimensionnel. Ce point précis de ressemblance permet conséquemment de mettre en perspective par une pléthore de différences comment l'humain crée ses propres connaissances. L'essai permet de reprendre ces points déjà élaborés auparavant par Mauss, selon lesquels certaines techniques sont corporelles, mais il remet le corps humain lui-même en perspective dans l'arbre phylogénétique comme condition de savoir. Par la manière dont elle est limitée par la propre corporéité de ses acteurs et la phénoménologie qui en émerge, il faut considérer que l'astrophysique est une science à deux bras, à la tension verticale, une science de ceux qui marchent, un géométrie des bipèdes qui se tiennent droit, avec le ciel au-dessus et le sol en-dessous, c'est une science de ceux qui manipulent avec des tendons, des os et des articulations, c'est une science de l'air et de l'oeil dans l'air, une science de l'atmosphère que le vampyroteuthis ne connaît absolument pas, une science de la technique des métaux, du feu, une science de technique sub- et supra-céleste, mais ce n'est pas la connaissance de l'eau profonde, qui plonge l'environnement dans l'obscurité et sous la pression. C'est une science de ceux qui se tiennent à l'air libre, debout avec des bras et des mains, et qui peuvent élever leurs outils optiques organiques et techniques vers les cieux sans autre perversion que l'atmosphère. C'est une science conditionnée par l'accès culturellement ouvert par la technique à des rayonnements électromagnétiques particuliers accessibles par une constitution biologique limitante. C'est une science absurde selon l'animal que l'on est. Je pense que cette considération permet de rentre compte de la manière extrêmement humaine de penser l'univers qui se retrouve jusque dans les actes réflexes des astrophysiciennes et astrophysiciens qui sont à la base de la recherche: manipulation du clavier et géométrie einsteinienne. Dans un autre milieu, quelle astrophysique, quelle géométrie, quelle technologie du tentacule, de la pression, du sombre, quelle chimie et quelle physique, quel raisonnement, quelle intellection, quelles cultures de pensée, quels courants, quel type d'adversité rationnelle et sociale, quelle culture de la recherche, quelle économie de la recherche, quelle organisation du temps et de l'espace, quelle justification de la téléologie de la découverte, quelles ontologies et quelles phénoménotechniques, y aurait-il eu pour un observateur abyssal, avec un corps, un mode de vie et une conception intelligente[^216] du monde si différente ? La question est de toute évidence rhétorique, elle invoque une fiction hyperbolique, mais elle met bien en lumière l'anthropomorphisme le plus radical qui est la source du problème de la matière noire, ou pour le dire autrement, la manière dont elle prend source dans un raisonnement anthropologique, qui n'a rien de naturel et extérieur à l'humain mais qui au contraire dépend entièrement de sa conformation particulière et de l'histoire qu'il s'est constitué[^217].

# Conclusion

Si je reprends l'entièreté de mon raisonnement, au départ de ma recherche, je me questionnais sur la possibilité du fait que la matière noire semblait proposer deux hypothèses ontologiquement contradictoires pour sa solution, à savoir la recherche d'une particule exotique ou la correction de la loi de la gravité de la relativité générale. Je me demandais comment était-il possible que ce paradoxe existe. J'ai alors entamé trois manières complémentaires d'aborder le problème: l'histoire rétrospective, l'histoire contemporaine et la philosophie. A partir de l'histoire rétrospective, je suis remonté jusqu'à l'antiquité grecque.  J'y ai montré comment Platon a introduit une philosophie noétique qui s'est conservée jusqu'à l'astrophysique contemporaine, alors que la séparation de la Terre et du Ciel et l'impératif grec de sauver les apparences (au contraire de les expliquer) ont été brisés par l'instrumentalisation de l'astronomie et la physique newtonienne. Les astres n'étaient alors plus inaccessibles, comme en-dehors de l'humanité. Le progrès de l'instrumentation a mené l'astronomie à devenir une astrophysique grâce à la spectrographie et à la photographie au XIXe siècle, qui ont permis des comparaisons entre les raies stellaires et les raies chimiques d'atomes précis, par exemple le sodium. Elles mènent à la découverte de l'hélium. Parallèlement, des géométries non-euclidiennes commencent à être développées en même temps que la notion de champ[^218] ce qui crée un ensemble de théories qu'Albert Einstein unifie dans sa relativité générale en rectifiant les référentiels de Newton-Galilée qui ne prennent pas en compte la vitesse de la lumière mesurée par Fizeau. La célérité de la lumière entre en compte pour la mesure des masses à cause de la relativité des référentiels d'observation, ce qui a un impact définitif et actuel en astrophysique sur la mesure des masses et vitesses. La théorie de la relativité générale permet de saisir pourquoi il est théoriquement possible d'un point de vue conceptuel et instrumental qu'il y ait une anomalie de masse vis-à-vis de l'information lumineuse, mais elle n'est pas le seul élément à prendre en compte pour comprendre le problème. Il y a d'autres motivations au paradoxe: les succès d'une généralisation par lois confirmée instrumentalement encourage le réemploi de la méthode. Je pense, par exemple à Urbain le Verrier et à sa prédiction puis découverte de la planète de Neptune, mais il y a des cas plus ambigus qui laissent plus de place à l'interprétation tels que la récession des galaxies. D'autre part, les succès de la physique standard motivent la voie de la recherche par atomes hypothétiques[^219] plutôt que la révision de lois. C'est dans une optique instrumentaliste que des anomalies de masse commencent à être remarquées par Fritz Zwicky, Jan Oort et d'autres astrophysiciens entre les années 1920 et 1930. Comme à cette époque, les corps difficiles à observer constituent un type d'énigme commune et non-surprenante, la matière noire en tant qu'anomalie n'apparaît pas encore. La matière noire des galaxies est progressivement oubliée avant de réapparaître en force dans les années 1980. Ce qui la caractérise proprement est sa résistance à toutes les voies de recherche connues (instrumentalisme optique et théorie de la relativité générale). Aucun spectre ne permet de trouver le corps lourd nécessaire, aucune hypothèse n'est confirmée à ce jour, et ce type de résistance fort permet de remettre en question la pertinence de l'hypothèse de recherche employée qui repose surtout sur la validité de la relation entre la masse et la vitesse dans les galaxies et amas de galaxies. Celle-ci est soit employée pour prédire des types de corps atomiques exotiques (l'hypothèsee des corps baryoniques lourds et sombre; MACHOs ayant été abandonnée) soit réécrite afin de décrire correctement les observations réalisées. Mais dans tous les cas, elle doit surtout pouvoir être généralisée vers une loi du mouvement universel, ce qu'aucune des deux hypothèses de recherche n'est encore en mesure de proposer car la généralisation doit passer par une adaptation progressive à un grand nombre de cas particuliers d'astres anormaux.

Dans des entretiens et observations que j'ai pu effectuer à l'OBAS en 2022, j'ai eu l'occasion de découvrir une méthodologie complète de la recherche qui se comprend comme un assemblage de phénoménotechniques et d'organisation sociale du savoir: organisation par appels à projet pour l'observation, et partage de connaissances tacites, rares et peu accessibles par laboratoires; phénoménotechnique du télescope, qui intervient sur les ondes lumineuses, et qui produit aujourd'hui des données numériques, puis phénoménotechnique de la modélisation informatique, qui permet de calculer des hypothèses codées dans un langage interprétable par la machine électronique. Ces hypothèses informatiques de modélisation représentent des approximations souvent discrètes d'hypothèses analytiques continues et de théories plus générales. Elles mènent à des approximations théoriques comparées par la suite entre elles (par soucis d'exactitude) ou comparées à des données observationelles, qui sont elles-mêmes traitées auparavant par un autre algorithme codé afin d'éliminer le bruit et les artefacts photographiés.  En tout cas, la pratique de la recherche astrophysique est caractérisée par une numérisation des instruments et des observations par instruments, en même temps que les mathématiques se retrouvent à tous ses niveaux d'hypothèse (de la plus abstraite à la plus interventionnelle).

Je trouve que ces pratiques s'inscrivent dans la continuité de plusieurs histoires: d'un côté, le développement de la photographie astronomique; d'un autre, l'histoire de l'élaboration de lois prédictives sur le ciel, dont une grande partie tire ses qualités par l'unification avec la physique standard grâce à l'analyse des raies; enfin, le développement nouveau de l'ordinateur, qui facilite les calculs de l'astronome et en permet de nouveaux qui autrement, prendraient des temps de vie entiers pour être complétés.

Cet ensemble de connaissances historiques peut être analysé grâce à des théories philosophiques de la recherche instrumentale afin de saisir ce que le dilemme signifie d'un point de vue ontologique. Je présente ainsi ces données comme étant la marque d'un paradigme kuhnien, c'est-à-dire d'un ensemble théorique, méthodologique, instrumental et social cohérent, dans lequel une énigme résistante est devenue une anomalie car elle a créé une crise , au sens que des hypothèses antiparadigmatiques ont émergées. Dans cet ensemble, la phénoménotechnique bachelardienne et l'analyse duhémienne permettent toutes deux de comprendre comment les instruments sont liés aux théories et quelle est donc l'heuristique particulière propre à cette science. En effet, selon Bachelard un instrument est un objet conçu en rapport avec certaines théories, et interprétées telles quelles par l'instrument; Duhem permet de mieux comprendre cette technologie car il explique que les valeurs symboliques attribuées aux mesures physiques organisées par un instrument sont prévues d'avance comme devant pouvoir rentrer dans les théories d'un système physique, donc prennent la forme de nombres et de symboles attendus. Le cercle clos de la théorie-instrument est appelé une phénoménotechnique . La phénoménotechnique permet la découverte car bien que l'instrument soit prévu pour donner des nombres et valeurs, rien ne permet de prédire toujours quel nombre sera reçu. A la différence de Duhem, je ne pense pas que tout est représenté en nombre[^220], en revanche je suis d'accord avec lui et Bachelard pour dire qu'un instrument doit toujours pouvoir être interprétable vis-à-vis d'un ensemble théorique et qu'il est conçu dans cette fin, ce qui seul le rend utile. Ainsi autant les résultats des télescopes que des modèles simulés sont disponibles pour un traitement ultérieur qui les rends compréhensibles à la mesure de l'intelligence humaine et dans le but d'interprétations théoriques. Enfin avec Hacking, je peux envisager une certaine position épistémologique propre à l'astrophysique de la matière noire: le réalisme de l'astrophysique provient de sa manière d'intervenir, qui est à la fois physique (transformation d'une onde et de son chemin) et fictionnelle (traitement d'informations, représentation de données et lois physiques par le calcul automatique) mais qui est en général entièrement retraçable[^221]; il provient aussi, et Catherine Allamel-Raffin met la force de cet argument en valeur, de la multi-instrumentalité et des analogies possibles entre les observations de plusieurs spectres, ou de spectres instrumentaux et d'objets simulés dans des spectres simulés; enfin il provient encore de son caractère auto-justifiant qui en fait une méthode à part, et en même temps qui relativise cette science comme *seulement* un système auto-justifiant parmi d'autres, situé par son histoire particulière.

Ultimement, les hypothèses sur la matière noires peuvent alors être présentées comme des modélisations particulières qui ne remettent pas en cause les observations, mais qui proposent différentes interprétations méthodologiques des observations en relation avec le paradigme de l'astrophysique. L'une de ces voies, celle de WIMP, consiste à vouloir emprunter la voie prédictive de la physique des particules, malgré le caractère purement *ad hoc* qui peut lui être reproché si elle est confrontée à MOND, qui préfère reconsidérer l'expression des lois aux échelles où les objets problématiques pour le paradigme normal se montrent. Comme l'anomalie est en fait située à plusieurs échelles, elle est difficile à résoudre et les deux voies avancent de la même façon par approximation et adaptation à des cas particuliers tout en essayant de trouver une lois générale qui puisse les englober tous.

L'espoir de synthèse est selon Bachelard la caractéristique du nouvel esprit scientifique qui s'est habitué à la possibilité de généralisations mathématiques résolvant des anomalies, par exemple pour le passage du newtonisme à la physique d'Einstein, ou de la géométrie euclidienne à celle de Riemann. Il faut considérer de ce point de vue que cet esprit s'est transmis d'une quelconque sorte en astrophysique, même si je crois qu'il est un peu moins abstrait que dans la microphysique des années 1940 dont traite Bachelard, car les données astronomiques d'observations revêtent une importance définitive dans l'élaboration des idées mathématiques. Par la modélisation informatique des objets astronomiques, l'universalisation des lois devient ultra-précise, donc avance extrêmement lentement. Elle ressemble de temps à autre à une méthode presque baconienne, tant elle doit s'adapter à tous les objets du ciels, dont les plus individuels créent en général le plus de résistance[^222].

Pour ma part, je considère que le paradoxe de la matière noire est capable de proposer deux voies ontologiques différentes précisément car la création d'un objet est une activité phénoménotechnique et donc en partie entièrement intellectuelle, qui n'est relative qu'aux hypothèses que tout un chacun veut bien prendre en compte; cette possibilité s'ouvre autant du fait que le paradigme d'interprétation du monde est stricte, régulé et communautaire, et ainsi il est nécessairement mathématique et même informatique, et du fait que ce paradigme est malléable et qu'une théorie, si elle permet d'interpréter un produit instrumental, n'est en revanche jamais que le choix d'une personne de voir le résultat tel qu'elle le décide. Donc un paradigme n'est réel que du moment que ses participants le justifient; la révolution commence lorsque cette justification commence à se perdre. Souvent, c'est une énigme très résistante qui pousse tout un chacun à se désespérer de principes devenus inefficaces et à en chercher d'autres, et l'on peut appeler cela une crise créée par une anomalie.

Je ne dirais pas que la matière noire est une révolution totale et finie précisément car les hypothèses proposées sont assez peu aliénées du paradigme principal. La méthode de recherche des diverses hypothèses n'a rien de différent sinon l'objet qui a été décidé d'être cherché. Les mondiens et les wimpiens travaillent dans les mêmes bureaux, avec les mêmes outils, ils se connaissent entre eux, ils discutent et si l'hyperspécialisation d'un domaine d'hypothèse empêche de comprendre tous les chemins empruntés, ils n'emploient pas en général de moyens de recherche différents. Le paradigme de l'astrophysique s'est donc subdivisé en deux champs de recherche qui ne se caractérisent cependant pas tant par leur méthode, que plutôt par la nature révisionnelle qu'ils attribuent à leurs hypothèses.

Le problème de la matière noire peut lui-même être situé dans une histoire ultra-large grâce au *Vampyroteuthis infernalis* comme étant un problème profondément humain, ayant émergé de la capacité de tourner le regard vers le ciel et de manufacturer des instruments à l'échelle de la main pour étendre la vue et enregistrer des interactions lumineuses. Le paradigme est alors caractérisé comme une phénoménotechnique incorporée dans des actes réflexes intellectuels et pratiques qui sont à la racine des manières de faire la recherche sur l'anomalie de la matière noire.  D'autre part, la phénoménologie humaine de l'espace-temps telle que permet de l'explorer la philosophie bergsonienne montre que ce paradigme entier est contingent tout en étant le legs de précédentes histoires.

Ce n'est au fond ni la première ni la dernière anomalie de l'histoire de la connaissance, mais elle est intéressante parce que l'étudier permet de mettre en lumière la manière dont sa résolution est tentée. La création de ce qui est et de ce que l'on considérera dans le futur être et pouvoir être, provient alors de la situation historique d'une culture méthodologique de l'interaction phénoménotechnique avec la lumière du ciel et les différentes voies d'aliénation empruntées.

L'ontologie de l'astrophysique de la matière noire est engendrée par un instrumentalisme fort symbolisé par des méthodes mathématiques appliquées à des connaissances virtualisées par l'électronique, cet instrumentalisme est lui-même systématisé dans une structure intellectuelle d'abstraction et de généralisation grâce à des équations mathématiques. C'est un paradigme commun qui offre des voies d'aliénations diverses à cause de l'inefficacité de la norme à résoudre l'anomalie qu'il a fait apparaître. L'aliénation des voies introduit des propos ontologiques eux-mêmes diversifiés: MOND et WIMP sont des heuristiques qui détermineront les êtres *normaux* ou les novuelles hypothèses de recherche à l'avenir.

J'ai émis l'idée que la philosophie kuhnienne permettait d'interpréter l'histoire dans un sens quasi-prédictif. Elle identifie en général les crises avec la possibilité d'une révolution qui réviserait une partie des fondements du paradigme. Mais cette proposition n'est permise que par la prise en compte *qualitative* d'un certain nombre défini et très limité de cas historiques. C'est donc une inférence probabiliste entièrement empirique.

Elle a cependant pour caractéristique de ne pas pouvoir permettre de décider clairement quelle voie serait meilleure à emprunter car la valorisation des solutions est toujours propre à un groupe ou une communauté de recherche, donc ne réside apparemment pas dans une structure régulière au cours de l'histoire à large échelle. C'est ainsi que si les deux hypothèses venaient à être couronnées de succès, elles devraient être subsumées dans une nouvelle anomalie qui consisterait à chercher l'équation mond-wimp ou loi-particule permettant de prédire tout un ensemble légal et atomique d'êtres, ce qui créerait un nouveau paradigme de recherche. Mais en fait, je peux proposer cette idée sur l'avenir justement car j'emprunte une manière régulière de faire la recherche que j'ai remarqué dans l'épistémologie du domaine, qui consiste dans le réalisme réalisant et généralisant par équations de l'astrophysique. Je pense donc qu'il est possible de remédier au manque de prédictivité de la théorie kuhnienne grâce à l'étude potentielle de la régularité de la manière de raisonner d'une ou plusieurs sciences: au cours d'une conversation avec Pierre-Antoine Hervieux et son doctorant Pierre Guichard, nous avons pensé qu'il serait intéressant de reprendre la théorie des types de phénomènes révolutionnaires de Kuhn[^223] et de tenter de la corriger ou de la confirmer grâce à des analyses bibliométriques. Si une certaine courbe régulière d'acheminement vers une révolution venait à être découverte dans des analyses bibliométriques de publication d'hypothèses à propos de plusieurs anomalies différentes (dans une même science ou de manière transversale) et selon un même type de phénomène révolutionnaire, cela pourrait avoir un impact intéressant et interactif sur la recherche scientifique.  D'autre part, car Kuhn ne permet pas de décider quelle voie révolutionnaire est la meilleure[^224], il serait intéressant de mesurer par bibliométrie et en plus par des analyses historico-philosophiques qualitatives pourquoi un choix est fait plutôt qu'un autre, et de voir s'il y a une régularité dans les types ontologiques contenus dans ces choix, à la manière de Bachelard dans le *Le nouvel esprit scientifique* qui identifie la régulière dialectique du non (par exemple: euclidien, non-euclidien, riemannien; newtonien, non-newtonien, einsteinien) de son époque. Si une régularité venait à être remarquée, la recherche de sa raison serait tout aussi intéressante et rétroactivement importante pour la recherche scientifique. Elle indiquerait en effet une structure reconnaissable et prédictive (par sa régularité) de la manière dont les anomalies voire les énigmes viennent à être résolues. L'idée serait donc de mesurer la régularité de certains styles de raisonnement.

*[La publication des entretiens transcrits est à venir.]*

# Bibliographie 

Allamel-Raffin, Catherine, "De la lunette de Galilée au microscope à effet tunnel" in *Les Génies de la science* 23 (2005), pp. 10 - 15.
- "The Meaning of a Scientific Image: Case Study in Nanoscience. A Semiotic Approach" in *Nanoethics* 5.2 (2011), pp. 165 - 173.

Allamel-Raffin, Catherine et Jean-Luc Gangloff, "Scientific images and robustness" in *Colloque international "Caractériser la robustesse des sciences après le 'tournant pratique' en philosophie des sciences"* (26 - 27 juin 2008).

Aristote, *Métaphysique*, Paris: Flammarion, 2008.
- *Physique*, Paris: Flammarion, 2008.

Bachelard, Gaston, *Le nouvel esprit scientifique*, 2020.

Balibar, Françoise, *Einstein 1905: de l'éther aux quanta*, Paris: PUF, 1992.

Bergson, Henri, *Essai sur les données immédiates de la conscience*, Paris: PUF.

Bontems, Vinct, Roland Lehoucq et Scott Pennor's, *Les idées noires de la physique*, Les Belles Lettres, 2016.

Chauviré, Christiane et Galilée, *L'essayeur de Galilée*, Paris, 1980.

Combes, Françoise, *La matière noire dans l'Univers: leçon inaugruale prononcée le jeudi 18 décembre 2014*, Paris: Collège de France Fayard, 2015.

Couderc, Paul, *La relativité*, Paris, 1962.

Declerck, Gunnar, "Physique de l'espace et phénoménologie de l'espace" in *Philosophia Scientiae* 153.3 (2011), pp. 197 - 219. [URL](https://www.cairn.info/revue-philosophia-scientiae-2011-3-page-197.htm)

Duhem, *La théorie physique: son objet, sa structure*, Paris: J. Vrin, 2007.

Einstein, Albert, *La relativité: théorie de la relativité restreinte et générale, la relativité et le problème de l'espace*, Paris: Payot & Rivages, 2001.

Elbaz, David, *À la recherche de l'univers invisible: matière noire, énergie noire, trous noirs.*, Paris: Odile Jacob, 2016.

Flusser, Vilém et Louis Bec, *Vampyroteuthis infernalis: un traité, suivi d'un rapport de l'Institut scientifique de rechche paranaturaliste.*, Bruxelles; Le Kremlin-Bicêtre: Zones sensibles; Diffusion Les Belles Lettres, 2015.

Focillon, Henri, *Vie des formes, suivi de Éloge sur la main*, Paris: PUF, 1943.

Hacking, Ian, *Anthropologe philosophique et raison scientifique*, Paris: J. Vrin, 2023.

Heidegger, Martin, *Être et temps*, Paris: Gallimard, 1995.

Hempel, Carl Gustav, *Élements d'épistémologie*, Paris: A. Colin, 2012.

Hiebert, E.N., "Electric discharge" in *No truth except in the details* (1995), pp. 95 - 134.

Hoskin, Michael A., *The Cambridge concise history of astronomy*, Cambridge; New York: Cambridge University Press, 1999.

James, William, *La psychologie de la croyance et autres essais pragmatistes*, Nantes: C. Defaut, 2010.

Kant, Immanuel, *Critique de la faculté de juger*, Paris: Flammarion, 2015.
- *Critique de la raison pure*, Paris: PUF, 2012.

Kragh, Helge, "Is The Universe Expanding? Fritz Zwicky And Early Tired-Light Hypotheses" in *Journal of Astronomical History and Heritage* 20.01 (mars 2017), pp. 2 - 12. [URL](https://engine.scichina.com/doi/10.3724/SP.J.1440-2807.2017.01.01)

Kuhn, Thomas Samuel, *La structure des révolution scientifiques*, Paris: Flammarion, 2018.

Labro, Philippe, *Poème sur la 7ème*, 1970.

Latour, Bruno et Steve Woolgar, *La vie de laboratoire: la production des faits scientifiques*, Paris: La Découverte, 2013.

Mauss, Marcel, "Les techniques du corps" in *Journal de Psychologie* XXXII.3-4 (1934).

Nazé, Yaël, *Histoire du télescope: la contemplation de l'univers des premiers instruments aux actuelles machines célestes*, Paris: Vuibert, 2009.

Newton, Isaac, *Principes mathématiques de la philosophie naturelle [traduit du latin] par feue madame la marquise Du Chastellet [Avec une préface de Roger Cotes et une préface de Voltaire].* T.1, 1759. [URL](https://gallica.bnf.fr/ark:/12148/bpt6k1040149v)

Pire, Bernard, *ROBERT WOODROW WILSON (1936- )*. [URL](https://www.universalis.fr/encyclopedie/robert-woodrow-wilson/)

Platon, *La République*, Paris: Flammarion, 2016.
- *Phèdre*, Paris: Les Belles Lettres, 2016.

Polanyi, Michael, *The tacit dimension*, Londres: Routledge & K. Paul, 1967.

Proust, Marcel, *Du côté de chez Swann*, Paris: Gallimard, 1992.

RAMSES. [URL](http://www.astro.iag.usp.br/~ruggiero/cosmotutorial/index.html)

Rubin, Vera, "Dark Matter in Spiral Galaxies" in *Scientific American* 248.6 (1983), pp. 96 - 109.

*Stabpyrometer mit Quecksilberthermometer*. [URL](https://digital.detusches-museum.de/projekte/gruendungssammlung/detail/849/)

Thomasette, David, "Histoire et philosophie des théories physiques", Cours professé l'université de Strasbourg, 2020.

Uexküll, Jakob von, *Mondes animaux et monde humain: suivi de Théorie de la signification.*, Paris: Pocket, 2004.

Ulrich, Benjamin, *Carnet A*, 2022.
- *Carnet B*, 2022.
- *Carnet C*, 2022.
- *Enregistrement du 2 mai à 15h08 (14min56s)*, 2022.
- *Enregistrement du 26 avril à 10h04 (49min59s)*, 2022.
- *Enregistrement du 26 avril à 17h01 (1h04min17s)*, 2022.
- *Enregistrement du 3 mai à 14h03 (46min14s)*, 2022.
- *Enregistrement du 4 mai à 14h23 (29min52s)*, 2022.

Varenne, Franck et Marc Silberstein (éd.), *Modéliser & simuler: épistémologies et pratiques de la modélisation et de la simulation. Tome 1.*, Paris: Éditions Matériologiques, 2021.

Young, M. Jane, "Pity the Indians of Outer Space" in *Western Folklore* 46.4 (oct. 1987), pp. 269 - 279. [URL](https://www.jstor.org/stable/1499889).

## Iconographie {#iconographie .unnumbered}

Barker, George, <https://www.loc.gov/pictures/item/2006689603/>, *People on snow-covered ice at the base of the frozen American Falls, Niagara falls, New York*, 1883, oeuvre de domaine public (visité le 11/09/2023).

Kok, Albert, <https://commons.wikimedia.org/wiki/File:Octopus3.jpg>, *Octopus vulgaris*, 4 août 2007, Licence CC-ASA-03 (visité le 11/09/2023).

Lumière, Auguste et Louis, <https://digitalmuseum.se/021016300041/lumiere-autokrom-kontorsinterior>, autochrome, sans date, licence de domaine public (visité le 11/09/2023).

OBAS et CDS, <https://astro.unistra.fr/fr/tout-public/construisez-vos-globes-de-papier/>, *Rhombicuboctaèdre de Jupiter*, attribution non commerciale (visité le 11/09/2023).

[^1]: Philippe Labro, *Poème sur la 7ème*, 1970.  

[^2]: De nombreux objets astronomiques émettent divers types d'ondes électromagnétiques qui sont propagées dans le temps et l'espace (ce que l'on appelle un rayonnement électromagnétique) qui peuvent être perçus sur Terre. Cependant, l'atmosphère terrestre bloque une partie de ces rayonnements, ce pourquoi il est également utile d'envoyer des télescopes dans l'espace. Des rayons cosmiques ultra-énergétiques aux gaz dans les galaxies dans les larges bandes radios, il y a une très grande diversité de spectres observables.

[^3]: Thomas Samuel Kuhn, *La structure des révolutions scientifiques*, Paris: Flammarion, 2018.  

[^4]: Je n'implique pas que ce n'est pas un bon moyen de recherche. En revanche, c'est le choix explicite de sauver quelque chose qui pourrait faillir.

[^5]: Ibid.  

[^6]: Je donne au mot  scientifique le sens d'un système qui produit une connaissance. Il ne signifie rien quant à la nature des connaissances produites (sont-elles empiriques, théoriques, abstraites, mystiques) et donc ne concerne pas exclusivement les sciences dures.

[^7]: Je dis cela principalement en raison de son livre très fameux *La vie de laboratoire* (Bruno Latour et Steve Woolgar, *La vie delabratoire: la production des faits scientifiques.*, Paris: La Découverte, 2013). Il y développe sa méthode inspirée de l'éthnologie tout en critiquant la sociologie classique.

[^8]: Même si j'ai utilisé plusieurs fois le terme  lambda-CDM comme si je le saisissais, après deux semaines, je ne savais toujours pas ce que c'était et d'ailleurs je n'aurais pas su le définir en-dehors du laboratoire. Dans le laboratoire, le terme est tout simplement utilisé selon une définition tacite que je n'ai pas reçu. Je me suis alors habitué à le dire sans pour autant pouvoir le définir par des mots. Cependant, je m'étais noté très tôt que je ne comprenais pas le terme et que j'étais surpris que tout le monde l'utilise.

[^9]: Science & Technologie Studies, études sur la science et la technologie.

[^10]: Les transcriptions sont trouvées dans leur entièreté dans l'annexe au mémoire.

[^11]: Ce carnet contient mes notes et observations réalisés sur place et durant les entretiens. Benjamin Ulrich, *Carnet A*, 2022.

[^12]: Dans celui-ci j'ai préparé les questionnaires à mes entretiens.  Benjamin Ulrich, *Carnet C*, 2022.

[^13]: Dans ce dernier j'ai donné quelques notes sur mes ressentis.  Benjamin Ulrich, *Carnet B*, 2022.

[^14]: Catherine Allamel-Raffin, "De la lunette de Galilée au microscope à effet tunnel" in *Les Génies de la Science* 23 (2005), pp. 10 - 15.  

[^15]: Catherine Allamel-Raffin, "The Meaning of a Scientific Image: Case Study in Nanoscience. A Semiotic Approach" in *Nanoethics* 5.2 (2011), pp. 165-173.

[^16]: La prospective désigne l'activité de projection d'un acte dans le futur.

[^17]: Platon, *Phèdre*, Paris: Les Belles Lettres, 1998, §265E, pp. 115.  

[^18]: La mythologie joue chez Platon un rôle important que l'on pourrait qualifier de didactique. Son caractère fictif est assumé en apparence mais Platon y mêle volontairement les dieux des mythes grecs classiques. Les mythes servent à expliquer l'intelligence, la mort et la création du monde, et servent aussi de justification pour d'autres concepts: par exemple, le mythe d'Er sert la justice dans *La République* (Platon, *La République*, Paris: Flammarion, 2016). La nature des mythes chez Platon est un vif sujet de recherche en philosophie grecque, sur lequel je ne gagnerais pas à m'avancer ici.

[^19]: Chez Platon l'intelligence est la partie de l'Homme la plus proche des dieux. Elle seule peut tenter de s'élever vers la connaissance dont ils diposeraient ou qui appartienidrait à leur monde supérieur.

[^20]: Aristote se distingue volontairement de Platon en établissant une scission claire entre la physique et la mathématique. Les Platoniciens ne s'apercevraient pas que les Idées sont plus mathématiques que physiques et ne décrivent pas des attributs du corps. Voir Aristote, *Physique*, Paris: Flammarion, 2010, Livre 2, Chapitre 2, § *Physique et mathématique* , 193b - 194a, pp. 122 - 123.

[^21]: Voir les textes Aristote, *Physique*, Paris: Flammarion, 2010, Livre 2, Chapitre 3, § *Les manières de dire le pourquoi* , 194b - 195a, pp. 128 - 129 et Aristote, Métaphysique, Paris: Flammarion, Livre Delta, 2, p. 180 qui décrivent chacun quatre type de causes identiques.

[^22]: En philosophie l'accident désigne l'ensemble des caractères mal déterminés et apparaissant au hasard affectant un concept.  L'accident de la tulipe, c'est d'être jaune ou rouge.

[^23]: Une forme connue également de Léonard de Vinci qui l'utilise beaucoup dans ses dessins.

[^24]: Christiane Chauviré et Galilée, *L'essayeur de Galilée*, Paris, 1980.  

[^25]: Chauviré et Galilée, *L'essayeur de Galilée*, p. 141].

[^26]: Ce qui ne veut pas dire que chaque mathématisation était d'origine instrumentale et observationelle. Nombre d'explications de la vérité proviennent d'une théorie des harmonies mélangeant herméneutique biblique ou kabbalistique et mathématiques platoniciennes. D'autre part, les formes platoniciennes jouent un rôle important pour les savants astronomes du XVIe siècle.

[^27]: Henri Bergson, *Essai sur les données immédiates de la conscience*, Paris: PUF, 2013.  

[^28]: Par "contrepénétré" et plus tard par "contrepénétration" je veux signifier que le mélange des souvenirs de la mémoire et l'action mutuelle qu'ils ont entre eux sur la constitution de nouvelles connaissances est à l'image d'une pénétration mutuelle de plusieurs éléments. Bergson n'est pas associationniste au sens classique du terme, mais il considère deux choses : d'un, qu'un état de conscience succédant à un autre, il sont liés par la contiguité ; de deux, qu'à la mémoire présente peuvent être ramenés les souvenirs de la durée passée, et ainsi créer une suite de pensées qui est plus logique dans l'ordre des causes auxquelles le sujet réfléchit, que dans l'ordre de la succession des *sense data*, ce qui déforme l'ordre de la perception de l'espace-temps par une reformulation dans le courant de la mémoire. Un très bon exemple de cette logique serait celui de la madeleine de Proust, qui fait appel à la réminiscence par analogie à l'expérience présente. Un autre exemple qui se situe dans le moment vécu chez Proust, qui a suivi les cours de Bergson au Collège de France: "il avait vu tout d'un coup chercher à s'élever en un clapotement liquide, la masse de la partie de piano, *multiforme*, *indivise*, plan et entrechoquée comme la mauve agitation des flots que charme et bémolise le clair de Lune \[\...\] une impression aussi *confuse*, une de ces impressions qui sont peut-être pourtant les seules purement musicales, *inétendues, entièrement originales, irréductibles à tout autre ordre d'impressions*. Une impression de ce genre, pendant un instant, est pour ainsi dire *sine materia*." (nous soulignons) Marcel Proust, *Du côté de chez Swann*, Paris: Gallimard, 1992.  Proust décrit une expérience de la durée sous la direction de la musique, et emploie le vocabulaire bergsonien de la qualité originale de la mémoire et du mélange particulier de l'expérience immédiate de la conscience. En ce sens la mémoire est un processus organisé et inconscient. Elle possède une structure due à la succession des états de conscience et à une faculté de remembrance analogique ramenant un souvenir ressemblant à la mémoire présente.

[^29]: Gunnar Declerck, ["Physique de l'espace et phénoménologie de l'espace"](https://www.cairn.info/revue-philosophia-scientiae-2011-3-page-197.htm) in *Philosophia Scientiae* 153.3 (2011).  

[^30]: Désir de réaliser un but.

[^31]: "La nouvelle physique reste bien une physique réaliste, elle ne se risque pas du côté de ce qu'on pourrait appeler une physique transcendantale, soit une physique qui chercherait à intégrer dans les représentations et lois qu'elle met à jour l'action d'un sujet constituant, qui serait l'architecte de sa réalité propre." Declerck, "Physique de l'espace et phénoménologie de l'espace", p. 201.

[^32]: M. Jane Young, ["Pity the Indians of Outer Space"](https://www.jstor.org/stable/1499889) in *Western Folklore* 46.4 (oct. 1987), pp. 269-279.  

[^33]: On retrouve une description de ce principe chez Aristote: Aristote, *Physique*, Livre 2, Chapitre 2, § *Physique et mathématique* , 193b - 194a, pp. 122 - 123.

[^34]: Isaac Newton, *Principes mathématiques de la philosophie naturelle [traduit du latin] par feue madame la marquise du Chastellet [Avec une préface de Roger Cotes et une préface de Voltaire].*[T.1](https://gallica.bnf.fr/ark:/12148/bpt6k1040149v), 1759.  

[^35]: Voir par exemple la définition V, Ibid. p. 3

[^36]: Je fais ici un résumé volontairement grossier car mon propos n'est pas d'analyser la physique newtonienne. Il est évident que d'autres auteurs avant celui-ci avaient préparé sa position en progressant dans la géométrisation des deux milieux, notamment Galilée en s'intéressant à la gravité et Képler en reformulant le mouvement des planètes par la découverte de la loi des aires.

[^37]: Les télescopes et les lunettes sont cependant très imparfaits et ces moyens ne deviennent vraiment pertinent que vers le XIXe siècle.

[^38]: Cette voie est également propre à la physique. Le XVIe siècle est celui du développement des instruments et des expériences instrumentales. Par exemple, la pompe à air ou le baromètre.

[^39]: Terme désignant ce qui peut être connu.  

[^40]: Ce phénomène fait référence à l'effet produit dans un tube cathodique.

[^41]: Hiebert, E.N., "Electric discharge" in *No truth except in the details* (1995),pp. 95-134, p. 96].  

[^42]: Ibid. p. 97.

[^43]: Ibid. p. 105.  

[^44]: Immanuel Kant, *Critique de la faculté de juger*, Paris: Flammarion, 2015, pp. 243 - 244].

[^45]: Au sens où le trope du romantisme est de faire naître un sentiment de sublime.

[^46]: Pierre Duhem, *La théorie physique: son objet, sa structure*, Paris: J. Vrin, 2007.  

[^47]: "La réduction des lois physiques en théories contribue ainsi à cette  *économie intellectuelle* en laquelle M.E. Mach voit le but, le principe directeur de la science." Ibid. p. 47.

[^48]: On peut dire de Duhem qu'il défend un réalisme structural épistémique, et de Poincaré, un réalisme structural ontique. Cela signifie que pour Duhem, ce qui est connaissable, ce sont les relations naturelles décelées par les instruments et conçues par les différentes étapes d'abstraction jusqu'à la théorie physique prédictive. En revanche, il existe au-delà de cela un inconnaissable, dont il ne faut pas s'occuper en science physique (mais dont on peut se soucier en-dehors de ce domaine). Pour Poincaré, les relations naturelles sont les seules choses connaissables, un argument qu'il traite en 1905. Poincaré est donc un antimétaphysicien en tout point. Je prends pour référence le cours magistral donné en 2020 de David Thomasette: "Histoire et philosophie des théories physiques", 2020.

[^49]: J'entends par "recherches fondamentales" un ensemble de recherches qui semblent se diriger toujours plus avant vers la vérité des éléments présupposément fondamentaux de la réalité, tels les atomes et leurs noyaux, indépendamment du fait que cette recherche soit considérée comme absolue ou asymptotique.

[^50]: A la restriction des limites du système de propositions préexistant et des règles adoptées.

[^51]: Duhem, *La théorie physique*, p. 53.  

[^52]: Gaston Bachelard, *Le nouve esprit scientifique*, 2020.

[^53]: J'appelle situation le contexte particulier dans lequel l'instrument est placé. Le contexte implique un phénomène observé en relation avec l'instrument, des conditions d'observation extérieure à cette observation, et une pratique technique de l'instrument par une personne.

[^54]: Bien que comme nous l'ayons montré avec l'exemple des tubes à vide et du plasma, que cette mathématisation n'est pas systématique et puisse être plutôt une prérogative qui n'est pas nécessairement facile à effectuer.

[^55]: Albert Einstein, *La relativité: théorie de la relativité restreinte et générale, larelitivité et le problème de l'espace.*, Paris: Payot & Rivages, 2001.  

[^56]: Paul Couderc, *La relativité*, Paris, 1962 MANQUE EDITEUR.

[^57]: Je sais qu'il n'est pas commun, en histoire et philosophie des sciences, de faire confiance à un scientifique lorsqu'il parle de lui-même, puisqu'il manque toujours d'une attitude critique et risque de valoriser certains enjeux symboliques propres à son champ.  Mais le livre d'Einstein est clair et informatif du point de vue de la forme de sa théorie, indépendamment de son histoire. Comme j'ai déjà introduit les tropes de la théorie physique avec Duhem, c'est la particularité de cette forme de représentation de l'espace et du temps qui m'intéresse.

[^58]: Einstein, *La relativité*, p. 135.  

[^59]: L'espace et le temps absolu de Newton ont été critiqués à sa propre époque, mais sans succès.

[^60]: Par trois expériences (en 1849, 1850 et 1851).  

[^61]: L'isotropie de la lumière est sa propriété à se propager à la même vitesse dans toutes les directions.

[^62]: Ou dans les distances longues. La lumière possédant une célérité, elle implique nécessairement une relation entre la distance et le temps.

[^63]: 299 796,458 km/s selon le 17e congrès de la Conférence Générale des Poids et des Mesures.

[^64]: Einstein, *La relativité*, p. 137.  

[^65]: La matière noire mettant en défaut la règle prédite pour les galaxies en astronomie, les prémisses de la relativité générale deviennent problématiques.

[^66]: Einstein, *La relativité*, p. 79.  

[^67]: Einstein, *La relativité*, p. 18.

[^68]: Einstein, *La relativité*, p. 125.  

[^69]: On peut définir un continuum comme une suite de points contigus.

[^70]: Kuhn, *La structure des révolutions scientifiques*.  

[^71]: [Comme le propose Einstein à la fin de son livre.  Einstein, *La relativité*, p. 151.

[^72]: Une logique hypothéticodéductive est une logique qui de prémisses données, accepte une conclusion.

[^73]: Couplée à la philosophie d'inspiration platonicienne que j'analysais dans le chapitre précédent.

[^74]: Yaël Nazé, *Histoire du télescope: la contemplation de l'univers des premiers instruments aux actuelles machines célestes*, Paris: Vuibert, 2009.  

[^75]: Je dis bien "indéfini" et non pas infini, car toutes sortes de paramètres limitent l'acquisition des données: qualité optique du télescope, résolution du CCD, temps de pose accordé, conditions météorologiques\...

[^76]: Selon Michael Hoskin, l'efficacité d'une plaque argentique pour la captation d'une image n'est que de 2% des photons transmis sur le plan focal alors que celle des capteurs CCD serait de 70%. Michael A. Hoskin, *The Cambridge concise history of astronomy*, Cambrige; New York: Cambridge University Press, 1999, p. 309].

[^77]: Françoise Combes, *la matière noire dans l'Univers: leçon inaugurale prononcée le jeudi 18 décembre 2014*, Paris: Collège de France Fayard, 2015, pp. 23 - 24.  

[^78]: David Elbaz, *À la recherche de l'univers invisible: matière noire, énergie noire, trous noirs*, Paris: Odile Jacob, 2016, pp. 42 - 43.

[^79]: Il est intéressant de savoir qu'Adams a émit ce doute, alors qu'il a aussi prédit la position de cet astre avant Le Verrier, mais avec plus d'imprécision. Le Verrier en est le découvreur officiel.  Ibid., p. 41.

[^80]: Au sens où cette invisibilité crée une anomalie : c'est selon *La structure des révolutions scientifiques* de Kuhn un problème créé à l'intérieur du paradigme qui jette les chercheurs dans le doute quant à sa solution. Il impose de sauver le paradigme ou de l'abandonner car le problème touche à des principes forts du paradigme. Dans le cas où l'insolubilité du problème est évidente, l'anomalie entame une révolution, puisqu'alors les hypothèses de solution commencent à se défaire des principes du paradigme.

[^81]: "By 1929 he had radial velocities and independent distance determinations for twenty-four galaxies, and he published a graph of the velocities (up to 1,100 km/sec) as a function of distance (up to 2x10^6 parsecs, or a little over 6x10^6 light years)." Hoskin, *The Cambridge concise history of astronomy*, p. 303.

[^82]: L'effet Compton décrit la perte d'énergie d'un photon suite à une interaction électronique.

[^83]: Voir Helge Kragh, ["Is The Universe Expanding ? Fritz Zwicky And Early Tired-Light Hypotheses"](https://engine.scichina.com/doi/10.3724/SP.J.1440-2807.217-01-01) in *Journal of Astronomical History and Heritage*, 20.01 (mars 2017), pp. 2-12.  

[^84]: L'expérience de Kennedy-Barkas est publiée dans *Physical Review*, mais aucun protagoniste n'y répond. Ibid., p. 7.

[^85]: "by 1940 the Static Universe was no longer part of mainstream astronomy. On the other hand, it had not yet been replaced by the Expanding Universe in the sense of relativistic cosmology" Ibid., p. 10].

[^86]: Je prends pour référence le livre d'Étienne Balibar, qui décrit très bien l'histoire de la notion, qui ne fut d'ailleurs pas toujours liée à la lumière. Françoise Balibar, *Einstein 1905: de l'éther aux quanta*, Paris: PUF, 1992.

[^87]: MOND signifie "*MOdified Newtonian Dynamics*". L'hypothèse est proposée à la fin des années 80 par Mordehai Milgrom, mais elle connaît aujourd'hui un grand nombre d'adaptations différentes. MOND explique le problème de masse non-prédite comme un problème de lois du mouvement mal saisies, et modifie ainsi la seconde loi de Newton.  WIMP (*weakly interactive particles*) prend la voie de la physique standard et cherche une particule lourde mais peu lumineuse pour expliquer cet absence de masse dans la théorie, qui est pourtant constatée expérimentalement.

[^88]: Si l'on connaît bien son instrument, puisque le système optique parfait n'existe pas: la présence d'artefacts est toujours de mise.

[^89]: Ainsi une la réalisation d'une carte du ciel grâce à la photographie a été proposée en 1885. Elle s'est arrêtée en 1964 (il faut prendre en compte les difficultés introduites par de deux guerres mondiales). Hoskin, *The Cambridhe concise history of astronomy*, pp. 222 - 223.

[^90]: "Des millions détoiles ont ainsi été observées pendant plus de dix ans, ce qui a permis d 'éliminer l'hypothèse d'objets compacts comme constituant une partie significative de la matière noire." Combes, *La matière noire dans l'Univers*, p. 39.

[^91]: Elbaz, *À la recherche de l'univers invisible*, p. 34.  

[^92]: 8 *inches* est égal à 202.3mm. *f* exprime le rapport d'ouverture entre une lentille et sa focale. Dans ce cas, 1 exprime un rapport de $1:1$, donc la focale est aussi large que le diamètre du miroir employé.

[^93]: Elbaz, *À la recherche de l'univers invisible* p. 40.  

[^94]: Combes, *La matière noire dans l'univers*, p. 24.

[^95]: Combes, *La matière noire dans l'univers*, p. 26.  

[^96]: Elbaz, *À la recherche de l'univers invisible*, note 3 p. 40].

[^97]: Combes, *La matière noire dans l'univers*, p. 28.  

[^98]: Combes, *La matière noire dans l'univers*, p. 29.

[^99]: Il s'agit de Robert H. Dicke qui en 1964, fait la même supposition que Robert Gamow à la fin des années 40: il doit y avoir un résidu de ce qu'on appellera plus tard le Big Bang aux alentours de 5K. Voir Hoskin, *The Cambridge concise history of astronomy*, p. 316.

[^100]: Bernard Pire, [*ROBERT WOODROW WILSON (1936-)*](https://www.universalis.fr/encyclopedie/robert-woodrow-wilson).  

[^101]: Vera Rubin, "Dark Matter in Spiral Galaxies" in *Scientific American* 248.6 (1983), pp. 96 - 109.

[^102]: Ce concept est largement développée par Catherine Allamel-Raffin et Jean-luc Gangloff. Il décrit la manière dont la comparaison de plusieurs phénomènes avec plusieurs instruments permet de donner un caractère de réalité à partir de ressemblances.  C'est un argument courant dans les articles de recherche publiés en astrophysique. Catherine Allamel-Raffin et Jean-Luc Gangloff, "Scientific images and robustness" in *Colloque international: "Caractériser la robustesse des sciences après le 'tournant pratique' en philosophie des sciences"* (26-27 juin 2008).

[^103]: Rubin, "Dark Matter in Spiral Galaxies".  

[^104]: Les représentations par images sont des arguments centraux et habituels dans la littérature de l'astrophysique. Allamel-Raffin et Gangloff, "Scientific images and robustness".

[^105]: "AO 136-0801" , Rubin, "Dark Matter in Spiral Galaxies", p. 106.  

[^106]: Ibid., p. 106.

[^107]: "The theoretical model that least disturbs accepted ideas about aglaxies accounts for the observed data about galactic curves by embedding each spiral galaxy in a spherical \"halo\" of dark matter that extends well beyond the visible limits of the galactic disk." Ibid., p. 102.

[^108]: Immanuel Kant, *Critique de la raison pure*, Paris: PUF, 2012.  

[^109]: L'hypersensibilisation est un procédé qui permet à des plaques photographiques de posséder une plus grande sensibilité à la lumière, réduisant ainsi grandement le temps d'exposition qui pourrait être très long. Une longue exposition requiert un long suivi de l'astre, ce qui peut être difficile en même temps que c'est peu économique en terme d'organisation et utilisation du télescope.  Rubin, "Dark Matter in Spiral Galaxies", pp. 100 - 106.

[^110]: Pour celles et ceux qui sont déjà rodés à la notion, je n'implique pas qu'un modèle soit nécessairement de forme informatique, ni peut-être de forme mathématique stricte. Mais c'est une manière de représenter les choses qui a un caractère hypothétique: cela ressemble alors à un patron. Rubin est ambiguë car elle mentionne l'utilisation d'un ordinateur une seule fois dans son article, mais seulement pour un graph, voir Ibid., p. 101.

[^111]: Ce type de raisonnement se présente dans la forme comme suit: un fait surprenant *I* se produit, et on suppute la cause probable *H*. La validité que H implique I n'est pas de mise, elle doit être testée: l'abduction est un procédé heuristique qui demeure au pur niveau de l'hypothèse. L'exemple le plus fameux de recherche par raisonnement abductif est donné par Hempel, à propos des recherches d'Ignaz Semmelweis (1818 - 1865) sur la fièvre puerpérale.  Carl Gustav Hempel, *Éléments d'épistémologie*, Paris: A. Colin, 2012, ch. II, pp. 25 - 29.

[^112]: J'ai trois carnets de note que je nomme A, B, et C. Dans le carnet A, des notes prises sur le moment de mes entretiens et observations. Dans le carnet B, des questionnaires préparés. Dans le carnet C, mon ressenti personnel et quelques pensées que je n'utilise pas ici. Tous mes enregistrements audio sont retranscrits excepté celui du 27 avril 2022 qui est de mauvaise qualité.

[^113]: Michael Polanyi, *The tacit dimension*, London: Routledge & K. Paul, 1967.  

[^114]: J'avais écrit le questionnaire en anglais, car l'OBAS est un lieu multilingue, où l'on parle principalement l'anglais et le français.

[^115]: Ulrich, *Carnet C*, p. 1.  

[^116]: Ulrich, *Carnet C*, p. 2.

[^117]: Ce n'est pas l'ordre chronologique de mes entretiens.  

[^118]: Tous les chercheurs que j'ai consulté font partie de l'équite GALHECOS (Galaxies, High Energy, Cosmology, Compact Objects & Stars) mais j'ai gardé leur anonymat.

[^119]: Je dispose également d'un enregistrement audio (27 avril 2022) mais par le hasard de la pièce dans laquelle je me trouvais, il y a beaucoup d'écho et les voix sont faibles. Je me réfère donc plutôt à mes notes prises en même temps que l'enregistrement a été effectué, qui sont de toute manière complètes. Ulrich, *Carnet A*, pp. 25 - 34].

[^120]: Il est d'ailleurs possible d'accéder à ces données pour les visualiser en utilisant le logiciel Aladin ou en visitant une page web (Simbad, <http://simbad.u-strasbg.fr/simbad/sim-fid>, VizieR, <https://vizier.cds.unistra.fr/viz-bin/VizieR.>)

[^121]: Par exemple la Carte du Ciel commencée en 1885 et terminée en 1964, perturbée par deux guerres mondiales.Hoskin, *The Cambridge concise history of astronomy*, pp. 222 - 223.

[^122]: Hoskin, *The Cambridge concise history of astronomy*, p. 222.  

[^123]: Hoskin, *The Cambridge concise history of astronomy*, p. 223.

[^124]: Benjamin Ulrich, *Enregistrement du 2 mai à 15h08 (14min56s)*, 2022, 11min36s - 11min42s.  

[^125]: Nazé, *Histoire du télescope*.

[^126]: Ulrich, *Enregistrement du 2 mai à 15h08 (14min56s),* 11min53s - 12min02s.  

[^127]: Ulrich, *Carnet A*, pp. 25 - 34.

[^128]: Ulrich, *Carnet A*, p. 34].  

[^129]: Ulrich, *Carnet A*, pp. 1 - 7.

[^130]: Lambda-CDM est un type d'hypothèse qui utilise une particule de matière noire dite "froide" , donc peu mobile et lourde, qui décrit assez bien les grandes structures de l'univers.

[^131]: Je pense qu'il est utile de garder le terme de computation pour faire référence au calcul informatisé, et ainsi le distinguer du simple calcul cérébral. Si en français computation et calcul sont synonymes, en anglais le premier terme fait explicitement référence au calcul par machine. C'est ce que Varenne propose dans son introduction: Franck Varenne et Marc Silberstein, éd., *Modéliser & Simuler: épistémologies et pratiques de la modélisation et de la simulation. Tome 1.*, Paris: Editions Matériologiques, 2021.

[^132]: Ulrich, *Carnet A*, pp. 7 - 13.  

[^133]: Durant mes derniers jours de stage j'ai eu l'occasion de m'essayer à un outil de simulation astrophysique. Je dis m'essayer à car je disposais de peu d'informations directives quant à quoi faire avec le code de départ, cependant avec assez d'efforts, il m'a été possible d'en comprendre quelques parties permettant de le faire calculer. Ce dont je dus m'occuper principalement fut d'inscrire et de proposer les bonnes valeurs à calculer, pour n'être pas au point des ordres que le programme refuse d'opérer. L'accomodation préalable aux outils informatiques dont je diposais dans ma vie personnelle m'a beaucoup aidé.

[^134]: Un code est un langage de représentation qui permet de ne pas à avoir à écrire dans le langage immédiat de la machine, qui est en général du binaire.

[^135]: Ulrich, *Carnet A*, pp. 13 - 14.  

[^136]: Ulrich, *Carnet A*, pp. 15 - 20.

[^137]: Benjamin Ulrich, *Enregistrement du 26 avril à 10h04 (49min59s)*, 2022.  

[^138]: La "particularité" est un terme que j'introduis de moi-même et qui me permet d'analyser ce que le chercheur a dit. Pour moi, elle met en valeur la manière dont un modèle doit devenir unique selon certaines règles de méthode.

[^139]: Certains modèles possèdent un but abstrait tel que la comparaison aux observations ne peut pas être immédiate, mais ils sont considérés comme des modèles tout de même.

[^140]: Ulrich, *Enregistrement du 26 avril à 10h04 (49min59s)*, 43min34s - 44min30s.  

[^141]: Ces phénomènes ont parfois pu exister et être connus depuis longtemps, mais dépourvus de représentation mathématique, ils apparaissent alors neufs grâce à un degré de connaissance innovant en étant modélisés prédictivement.

[^142]: Ulrich, *Carnet A*, pp. 21 - 25].  

[^143]: Non, elle n'utilise pas un serpent: python est un langage de programmation connu et largement utilisé à des fins scientifiques dans des domaines aussi divers que la biologie, la chimie, la physique, l'astrophysique et la cosmologie.

[^144]: Ulrich, *Carnet A*, p. 23].  

[^145]: Ulrich, *Carnet A*, pp. 40 - 43.

[^146]: Ulrich, *Carnet A*, pp. 44 - 46.  

[^147]: Je veux dire par là qu'il ne tergiverse pas sur l'hypothèse à employer, mais qu'il en utilise une qui marche suffisamment bien pour être représentative dans son travail de recherche qui concerne un autre sujet.

[^148]: Ulrich, *Carnet A*, pp. 52 - 57.  

[^149]: Benjamin Ulrich, *Enregistrement du 26 avril à 17h01 (1h04min17s)*, 2022, 5min36s - 7min52s.

[^150]: Benjamin Ulrich, *Enregistrement du 26 avril à 17h01 (1h04min17s)*, 2022, 29min15s - 29min58s.  

[^151]: Benjamin Ulrich, *Enregistrement du 26 avril à 17h01 (1h04min17s)*, 2022, 40min54s - 40min56s.

[^152]: Benjamin Ulrich, *Enregistrement du 26 avril à 17h01 (1h04min17s)*, 2022, 37min17s - 31min23s.  

[^153]: Benjamin Ulrich, *Enregistrement du 26 avril à 17h01 (1h04min17s)*, 2022, 39min47s - 40min16s.

[^154]: Benjamin Ulrich, *Enregistrement du 3 mai à 14h03 (46min14s)*, 2022.  

[^155]: Benjamin Ulrich, *Enregistrement du 4 mai à 14h23 (29min52s)*, 2022.

[^156]: [RAMSES](http://www.astro.iag.usp.fr/~ruggiero/cosmotutorial/index.html).  

[^157]: Chaque télescope introduit sa propre phénoménologie et ses possibles; un temps de calcul trop long n'est pas envisageable pour organiser le partage des ressources techniques: il faut alors discrétiser ou simplifier le modèle.

[^158]: William James, *La psychologie de la croyance et autres essais pragmatistes*, Nantes: C. Defaut, 2010.  

[^159]: Son auteur est un philosophe, et si j'utilise une traduction de 2015, l'édition originale date de 1987. Les théories biologiques du livre sont plus qu'étranges (vitalistes et freudiennes) en revanche l'effort donné pour interpréter une épistémologie non-humaine par la fiction est passionnant. Vilém Flusser et Louis Bec, *Vampyroteuthis infernalis: un traité, suivi d'un rapport de l'Institut scientifique de la recherche paranaturaliste*, Bruxelles; Le Kremlin-Bicêtre: Zones Sensibles; Diffusion Les Belles Lettres, 2015.

[^160]: Marcel Mauss, "Les techniques du corps" in *Journal de Psychologie* XXXII.3-4 (1934).  

[^161]: Henri Focillon, *Vie des formes, suivi de Éloge sur la main.*, Paris: PUF, 1943, pp. 101 - 128.

[^162]: Ces réflexes pratiques peuvent être étendus à d'autres objets, par exemple le tableau blanc et le feutre noir ou le carnet de note accompagné de son stylo bleu.

[^163]: J'utilise le mot "radical" pour signifier que cette technique est une étape inévitable, structurelle et souvent chronologiquement première dans l'effort de conception d'un modèle: elle est à la *racine* de la modélisation. Certes, avant de concevoir, il y a peut-être une hypothèse, mais les éléments fondamentaux intellectuels de l'hypothèse sont eux-même réflexes.

[^164]: James, *La psychologie de la croyance et autres essais pragmatistes*.  

[^165]: Martin Heidegger, *Être et temps*, Paris: Gallimard, 1995.

[^166]: James, fervent défenseur de la liberté de croire au sens théologique, cherchait à justifier son droit en l'universalisant grâce au concept de croyance qu'il mettait à la racine de chaque connaissance.

[^167]: [Je fais précisément référence au §6 "La tâche d'une désobstruction de l'histoire de l'ontologie" , du deuxième chapitre de l'introduction. Heidegger, *Être et temps*, pp. 45 - 52].

[^168]: Je mets cette expression entre guillemets car il me semble qu'elle appartient elle-même à une certaine culture. Certes l'humain possède une mémoire, mais l'idée d'être et la pratique de l'histoire de l'être, c'est une historiographie située historiquement.

[^169]: Jakob von Uexküll, *Mondes animaux et monde humain: suivi de Théorie de la signification.*, Paris: Pocket, 2004.  

[^170]: Bachelard, *Le nouvel esprit scientifique*.

[^171]: La réification est l'acte de transformer en objet une idée. On emploie parfois le terme de chosifier dans le même sens.

[^172]: Je cite Vincent Bontems dans sa présentation, Bachelard, *Le nouvel esprit scientifique*, p. 17.  

[^173]: Duhem, *La théorie physique*.

[^174]: Là où Bachelard fait surtout l'histoire de théories scientifiques, Duhem propose un appareil conceptuel d'analyse de la théorisation scientifique.

[^175]: "Est-ce qu'on voit à travers un microscope ?" *in* Ian Hacking, *Anthropologie philosophique et raison scientifique.*, Paris: J. Vrin, 2023.  

[^176]: Hacking, *Anthropologie philosophique et raison scientifique*.

[^177]: Hacking, *Anthropologie philosophique et raison scientifique*, p. 58.  

[^178]: Les deux disciplines collaborent de manière si proche qu'il n'est pas très pertinent de les distinguer sur la plupart de leurs pratiques méthodologiques, à ceci près que la cosmologie a pour but d'inclure les objets particuliers de l'univers dans une dynamique universelle, alors que l'astrophysique peut se contenter d'un moindre niveau de généralisation).

[^179]: "Toutefois le sens du *vecteur* épistémologique nous paraît bien net. Il va sûrement du rationnel au réel." Bachelard, *Le nouvel esprit scientifique*, p. 27.

[^180]: On verra peut-être ici un raisonnement cyclique, puisque l'instrument semble n'être conçu que pour affirmer la théorie à son origine. Ce n'est pas tout à fait exact, puisque par exemple dans le cas de la matière noire, l'instrument tiré de la théorie de la lumière a fait émerger une énigme difficile à résoudre. L'instrument est plutôt un pari sur la théorie qui est mise à l'épreuve quand il est opéré.

[^181]: J'ai eu l'occasion d'observer, manipuler et démonter cet objet lors du séminaire "Material Culture in the History of Physics", organisé du 26 février 2023 au 3 mars 2023 au Deutsches Musem de Munich.

[^182]: [*Stabpyrometer mit Quecksilberthermometer*](https://digital.deutsches-museum.de/projekte/gruendungssammlung/detail/849/).  

[^183]: L'avancée est telle que les derniers et prochains télescopes créent tant de données qu'il faut des années pour les traiter, malgré l'aide apportée par l'algorithmisation de la tâche.

[^184]: C'est le même dans sa structure de vérification, mais ce n'est pas le même dans ses techniques: il n'y a pas de microscope électronique en astrophysique, par exemple.

[^185]: "Voit-on à travers un microscope ?" in Hacking, *Anthropologie philosophique et raison scientifique.  [^186]: "Style pour historiens et philosophes" in hacking, *Anthropologie philosophique et raison scientifique*, p. 58

[^187]: Comme le montre Catherine Allamel-Raffin dans les publications d'astrophysique. La méthode employée par Rubin, qui est permise par l'emploi de spectres différents au cours du XXe siècle, est devenue une norme aujourd'hui. Allamel-Raffin et Gangloff, "Scientific images and robustness".

 [^188]: "Style pour historiens et philosophes." in *Anthropologie philosophique et raison scientifique*, p. 57 

 [^189]: Je parle des observations au télescope. Les modèles informatiques sont réalistes si l'on connaît la très longue suite d'étapes qui les constitue, en somme si l'on connaît leur histoire, ce qui ne veut pas dire qu'ils sont proches de la réalité: ils sont réalistes car il est possible d'évaluer leur justesse et probabilité de proposer un élément symbolisant potentiellement une observation.  Souvent, cette probabilité est très basse.

[^190]: La phénoménotechnique et l'intervention introduisent une ontologie plus complexe qu'une simple représentation de niveau sémantique par analogie entre par exemple l'image lumineuse d'une galaxie et la source lumineuse de la galaxie. La correspondance est informée car la transformation de l'onde est connue et parce que l'analyse de l'onde perçue peut être incorporée dans le schéma prédictif de la chimie, ce que l'on appelle une analyse spectrographique. D'ailleurs, l'extériorité de la galaxie ne peut être conçue qu'à partir de la captation et transformation primordiale de l'onde par un instrument. C'est la loi de Hubble qui influence particulièrement l'interprétation de la donnée du télescope pour concevoir une mise à distance de l'objet.

[^191]: Bachelard cite Gustave Juvet, Kuhn ne fait pas mention de Bachelard (ce qui n'indique pas nécessairement qu'il ne l'a pas lu, mais ce qui mène à penser qu'il n'en a pas une connaissance approchée ou qu'il n'en pense que peu dans son ouvrage) et si Hacking a bien lu Kuhn, il se dit d'inspiration foucaldienne (archéologie du savoir). Foucault a lu Bachelard, mais Hacking n'y fait pas référence.

[^192]: Kuhn, *La structure des révolutions scientifiques*, p. 84.  

[^193]: La durée de ces mouvements est indéterminable, mais leur présence est régulière.

[^194]: "Plus la précision et la portée du paradigme sont grandes, plus celui-ci se révèle un *indicateur* sensible pour signaler les anomalies et amener évetuellement un changement de paradigme" Kuhn, *La structure des révolutions scientifiques*, pp. 118 - 119.

[^195]: "L'échec des règles existantes est le prélude de la recherche de nouvelles règles.", Ibid., pp. 123 - 124.

[^196]: Car les particules hypothétisées nécessitent des accélérateurs à très haute énergie pour disposer d'une détection directe, ce qui est paradoxalement considéré comme définitif en comparaison d'une détection par modèle, plus possible mais moins déterminante.

[^197]: Ibid., p. 167].  

[^198]: Comme c'est le cas avec la scission corpuscule/WIMP et mouvement/MOND.

[^199]: Comme le concevrait Feyerabend pour qui \"tout va\".  

[^200]: "C'est tout le problème de la connaissance scientifique du réel qui est engagé par le choix d'une mathématique intiale (\...) on cherche du côté de l'abstrait les preuves de la cohérence du concret. Le tableau des possibilités d'expérience est alors le tableau des axiomatiques." Bachelard, *Le nouvel esprit scientifique*, p. 59.

[^201]: "En effet, dès que l'objet se présente comme un complexe de relations, il faut l'appréhender par des méthodes multiples.  L'objectivité ne peut se détacher des caractères sociaux de la preuve. On ne peut arriver à l'objet qu'en exposant d'une manière discursive et détaillée une méthode d'objectivation." Ibid., p. 34.

[^202]: "Il arrive toujours une heure où l'on n'a plus d'intérêt à chercher le nouveau sur les traces de l'ancien, où l'esprit scientifique ne peut progresser qu'en créant des méthodes nouvelles.  Les concepts scientifiques eux-mêmes peuvent perdre leur universalité." Ibid., p. 145.

[^203]: "C'est après coup, quand on s'est installé d'emblée dans la pensée relativisite, qu'on retrouve dans les calculs astronomiques de la Relativité - par des mutilations et des abandons - les résultats numériques fournis par l'astronomie newtonienne. Il n'y a donc pas de transition entre le système de Newton et le système d'Einstein. (\...) On suit donc une induction transcendante et non pas une induction amplifiante en allant de la pensée classique à la pensée relativiste." Ibid., p. 62.

[^204]: [Ecrits respectivement en 2001, 2008 et 2004. Je les mentionne dans l'ordre que j'ai trouvé compilé, il me semble qu'il est mieux de conserver cet ordre car il va croissant en précision sur la définition du concept. Hacking, *Anthropologie philosophique et raison scientifique*.

[^205]: "Leçon inaugurale du Collège de France", Ibid., p. 33.  

[^206]: Ibid., p. 33

[^207]: Ibid., pp. 33 - 34 

[^208]: "Style pour historiens et philosophes" in Hacking, *Anthropologie philosophique et raison scientifique*, p. 64].

[^209]: "L'ontologie historique" in Hacking, *Anthropologie philosophique et raison scientifique*, p. 85].  

[^210]: "L'ontologie historique" in Hacking, *Anthropologie philosophique et raison scientifique*, p. 84

[^211]: Kuhn considère d'ailleurs que si la révolution d'un paradigme rend l'ancien et le neuf incommensurables, cela ne veut pas pour autant dire que tout a été détruit puis reconstruit sur les décombres. Un paradigme peut ne changer que très peu après une révolution dans le nombre et la nature de ses lois, cependant, ce qui change ce sont les interprétations disponibles après cette transformation. Il apparaît alors des cas où les anciennes interprétations ne sont plus possibles. On ne pense plus à la géométrie riemannienne comme le cas particulier de la géométrie euclidienne, et les lois de Newton ne sont plus valables quant la vitesse de la lumière entre en compte. Il viendra nécessairement un jour où il ne sera plus possible de penser la matière noire comme l'une des deux options les plus fameuses que j'ai mentionné.

[^212]: Il a écrit sur l'éthique, sur la phénoménologie, sur l'histoire et la philosophie des techniques de la photographie, sur l'épistémologie\... Je crois qu'il est peu connu, mais son oeuvre est très large.

[^213]: Flusser et Bec, *Vampyroteuthis infernalis: un traité, suivi d'un rapport de l'Institut scientifique de recherche paranaturaliste*.  

[^214]: J'utilise cependant la traduction française parue chez Zones Sensibles, car je ne lis pas l'allemand.

[^215]: Faute de pouvoir étudier ces animaux car ils survivent peu de temps en aquarium, Flusser se permet de faire un parallèle avec d'autres céphalopodes abyssaux.

[^216]: Je m'en tiens à la considération biologique que le haut niveau de développement nerveux de l'animal implique probablement qu'il dispose d'une forme de mémoire et à tout le moins d'une haute sensibilité. C'est le type de prérequis que l'on demande en général de l'humain pour qu'il dispose d'une culture et d'une pensée. Je considère donc que c'est une définition de l'intelligence, celle de conférer un sens au monde, peu importe la nature de ce sens et le degré d'innéité que l'on veut lui attribuer.

[^217]: Il serait possible de pousser l'analogie plus loin encore, en se débarassant du point de vue d'un cœlomate et en prenant celui d'un acœlomate. On gagnerait également à se désinhiber du corps unifié et à prendre en compte par exemple, le siphonophore géant ou d'autres animaux-colonies. Enfin, tous les êtres dilemmatiques que nous situons à la limite entre le vivant et le non-vivant ont certainement à nous apprendre (tels les coraux et les virus).

[^218]: [D'abord imaginé par Faraday, le champ est différent de la force newtonienne qui est d'habitude représentée par une droite. Il représente un espace où les forces peuvent avoir différentes directions. Aujourd'hui, le champ représente également différentes intensités. Balibar, *Einstein 1905*, chapitre 2.].

[^219]: Le paradigme de la physique standard est en effet très fructueux puisqu'il ouvrait encore la voie expérimentale du boson de Higgs il y a dix ans.

[^220]: Tous les résultats des modélisation informatiques ne sont pas compris en nombre. Certains sont des mots.

[^221]: A l'exception de l'introduction des réseaux neuronaux et du *machine learning* qui sont des opérations appelées boîtes noires au sens où justement, l'histoire des calculs effectués par la machine n'est pas prévisible ni entièrement contrôlable par l'expérimentateur. Si deux doctorants que j'ai rencontré employaient ce moyen (l'un pour étudier l'ère de réionisation après le Big Bang, le 25 avril 2022, l'autre pour prévoir des équations à partir d'une analyse du langage,le 4 mai 2022), en revanche ce n'est pas la méthode la plus courante.

[^222]: Par exemple, certaines galaxies particulières échappent aux WIMP. Mais d'autres phénomènes à grande échelle échappent à MOND.

[^223]: Kuhn, *La structure des révolutions scientifiques*, p. 167.  

[^224]: La manière dont une solution est valorisée étant entièrement relative aux valeurs propres de chaque paradigme.
